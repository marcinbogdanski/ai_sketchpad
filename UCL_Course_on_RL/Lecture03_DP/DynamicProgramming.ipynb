{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<!-- <p style=\"text-align:center;font-weight:bold;font-size:400%\">Dynamic Programming</p> -->\n",
    "<img src=\"assets/title.png\"/>\n",
    "\n",
    "$$ \\Huge{\\underline{\\textbf{ Dynamic Programming }}} $$\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Implementation of algorithms presented in Lecture 3 of UCL RL course by David Silver.\n",
    "\n",
    "Contents:\n",
    "* Intro\n",
    "  * [Introduction](#Introduction) - this section\n",
    "  * [Frozen Lake](#Frozen-Lake) - environment\n",
    "* Algorithms\n",
    "  * [Iterative Policy Evaluation](#Iterative-Policy-Evaluation) - matrix form\n",
    "  * [Policy Iteration](#Policy-Iteration) - matrix form\n",
    "  * [Value Iteration](#Value-Iteration) - loopy form\n",
    "\n",
    "Notes:\n",
    "* As OpenAI gym doesn't have environment corresponding to gridworld used in lectures. We use FrozenLake-v0 instead\n",
    "\n",
    "Sources:\n",
    "* UCL Course on RL: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "  * Lecture 3 pdf: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf\n",
    "  * Lecture 3 vid: https://www.youtube.com/watch?v=Nd1-UUMVfz4\n",
    "\n",
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "np.set_printoptions(linewidth=115)  # nice printing of large arrays\n",
    "\n",
    "# Initialise variables used through script\n",
    "env = gym.make('FrozenLake-v0')\n",
    "nb_states = env.env.nS        # number of possible states\n",
    "nb_actions = env.env.nA       # number of actions from each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for FrozenLake-v0: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "Frozen Lake is 4x4 grid:\n",
    "<img src='assets/frozenlake.png'>\n",
    "Note on actions:\n",
    "* environment is 'slippery' - choosing to go 'North' will result with 1/3 probability of moving West/North/East each.\n",
    "* external walls are 'bouncy' - if action would result in falling off the grid, agent remains in current state instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save number of states and actions for later. These variables are used through the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_states = env.env.nS        # number of possible states\n",
    "nb_actions = env.env.nA       # number of actions from each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment transition probabilities and rewards are stored in array __env.env.P__. For example, if agent is in state 6 and select action 'West' (action 0), then __env.env.P[6][0]__ stores all possible transitions from that state-action pair to next-states along with expected rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 2, 0.0, False),\n",
       " (0.3333333333333333, 5, 0.0, True),\n",
       " (0.3333333333333333, 10, 0.0, False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P[6][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above tells us, that after selectin action 'West', wind can blow us to three possible states:\n",
    "* given starting state __6__ and action __0__ (West), there is __0.33__ chance ending in state __2__, with reward __0.0__, non-terminal\n",
    "* given starting state 6 and action 0 (West), there is 0.33 chance ending in state 5, with reward 0.0, terminal (hole)\n",
    "* given starting state 6 and action 0 (West), there is 0.33 chance ending in state 10, with reward 0.0, non-terminal\n",
    "\n",
    "Perhaps this is better ilustrated with a diagram\n",
    "<img src=\"assets/diag_frozen_lake.png\">\n",
    "\n",
    "Perhaps good time to introduce notation:\n",
    "* Probabilities\n",
    "  * $\\mathbf{P}^\\pi$ - transition probability matrix under our policy $\\pi$ with shape [nb_states, nb_states]\n",
    "  * $P_{s,s'}^\\pi$ - transition probability from state s to s' under current policy $\\pi$, these are elements of matrix $\\mathbf{P}^\\pi$\n",
    "  * $P_{s,s'}^a$ - transition probability from state-action s,a to state s' (this is __33%__ above)\n",
    "  * $\\pi(a|s)$ - policy - probability of choosing action a, given state s - defined by us\n",
    "* Expected Rewards\n",
    "  * $\\mathbf{R}^\\pi$ is vector of expected rewards for each state\n",
    "  * $R_s^\\pi$ - expected reward on transition from state s to any state s' (weighted sum over all actions), these are elements of vector $\\mathbf{R}^\\pi$\n",
    "  * $R_s^a$ - expected reward on transition from state-action s,a to any next state (weighted sum over all next-states)\n",
    "  * $R_{s,s'}^a$ - expected reward on transition from state-action s,a to state s' (this is __0.0__ above)\n",
    "* State Values\n",
    "  * $\\mathbf{v}^{k+1}$ is vector of state-values $v(s)$ at iteration k+1\n",
    "  * $\\mathbf{v}^k$ is vector of state-values $v(s)$ at iteration k\n",
    "* $\\gamma$ is a discount factor, here equals 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Policy Evaluation\n",
    "\n",
    "Policy Evaluation can be implemented in two ways:\n",
    "* Iterate over every state s and perform backup using Bellman Expectation Equation\n",
    "* Calculate matrix $\\mathbf{P}^\\pi$ and vector $\\mathbf{R}^\\pi$ and perform backup in matrix form\n",
    "\n",
    "Because we evaluate fixed policy matrix $\\mathbf{P}^\\pi$ and vector $\\mathbf{R}^\\pi$ are constant. They need to be computed only once, then we can do backup over and over again in matrix form.\n",
    "\n",
    "We will implement following algorithm to evaluate our fixed random policy:\n",
    "* Flatten Markov Decision Process to Markov Reward Process, i.e. assume policy is part of environment\n",
    "* Initialize all state values to zero: $\\mathbf{v \\leftarrow 0}$\n",
    "* Iterate k=100 times:\n",
    "  * update all state values using Bellman Expectation Equation:\n",
    "  \n",
    "$$ \\Large{ \\mathbf{v}^{k+1} = \\mathbf{R}^\\pi + \\gamma \\mathbf{P}^\\pi \\mathbf{v}^k } $$\n",
    "\n",
    "First we will calculate transition probability from state s to s' as follows\n",
    "\n",
    "$$ \\Large{ P^\\pi_{s,s'}=\\sum_{a \\in A} \\pi(a|s) P^a_{s,s'} } $$\n",
    "\n",
    "Then we use $P_{s,s'}^\\pi$ to form transition probability matrix $\\mathbf{P}^\\pi$\n",
    "<img src=\"assets/eq_P_matrix.png\">\n",
    "\n",
    "Second we calculate expected reward on transition from state-action pair\n",
    "\n",
    "$$ \\Large{ R^a_s = \\sum_{s' \\in S} P^a_{s,s'} R^a_{s,s'} } $$\n",
    "\n",
    "Now we can calculate expected reward from state s (not state-action!) to any next state\n",
    "\n",
    "$$ \\Large{ R^\\pi_s = \\sum_{a \\in A} \\pi(a|s) R^a_s } $$\n",
    "\n",
    "Helper function to calculate both $\\mathbf{P}^\\pi$ and $\\mathbf{R}^\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_mdp(policy, model):\n",
    "    P_pi = np.zeros([nb_states, nb_states])  # transition probability matrix (s) to (s')\n",
    "    R_pi = np.zeros([nb_states])             # exp. reward from state (s) to any next state\n",
    "    for s in range(nb_states):\n",
    "        for a in range(nb_actions):\n",
    "            for p_, s_, r_, _ in model[s][a]:\n",
    "                # p_ - transition probability from (s,a) to (s')\n",
    "                # s_ - next state (s')\n",
    "                # r_ - reward on transition from (s,a) to (s')\n",
    "                P_pi[s, s_] += policy[s,a] * p_   # transition probability (s) -> (s')\n",
    "                Rsa = p_ * r_                     # exp. reward from (s,a) to any next state\n",
    "                R_pi[s] += policy[s,a] * Rsa      # exp. reward from (s) to any next state\n",
    "    assert np.alltrue(np.sum(P_pi, axis=-1)==np.ones([nb_states]))  # rows should sum to 1\n",
    "    return P_pi, R_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do policy evaluation for random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.ones([nb_states, nb_actions]) / nb_actions  # 0.25 probability for each action\n",
    "P_pi, R_pi = flatten_mdp(policy, model=env.env.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5  0.25 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.25 0.25 0.25 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.25 0.5  0.   0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.25 0.   0.   0.25 0.   0.25 0.   0.   0.25 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.25 0.   0.   0.   0.25 0.25 0.   0.   0.25 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.   0.25 0.   0.   0.25 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.   0.25 0.   0.   0.25 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.25 0.25 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.25 0.25]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(P_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(R_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 100 steps of iterative policy evaluation according to equation\n",
    "\n",
    "$$ \\Large{ \\mathbf{v}^{k+1} = \\mathbf{R}^\\pi + \\gamma \\mathbf{P}^\\pi \\mathbf{v}^k } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.014 0.012 0.021 0.01 ]\n",
      " [0.016 0.    0.041 0.   ]\n",
      " [0.035 0.088 0.142 0.   ]\n",
      " [0.    0.176 0.439 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "lmbda = 1.0  # discount\n",
    "\n",
    "V_pi = np.zeros([nb_states])\n",
    "for k in range(100):\n",
    "    V_pi = R_pi + lmbda * P_pi @ V_pi\n",
    "print(V_pi.reshape([4, -1]).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct output:\n",
    "```\n",
    "[[0.014 0.012 0.021 0.01 ]\n",
    " [0.016 0.    0.041 0.   ]\n",
    " [0.035 0.088 0.142 0.   ]\n",
    " [0.    0.176 0.439 0.   ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement following algorithm:\n",
    "* repeat n=10 times:\n",
    "  * flatten current policy into MRP\n",
    "  * perform k=100 steps of Policy Evaluation\n",
    "  * perform greedy policy improvement (requires q values)\n",
    "\n",
    "To do policy improvement we need to know best action, which means we need to calculate action-values for all state-action pairs. Diagram below shows how to calculate action-values using Bellman Expectation Equation\n",
    "<img src=\"assets/diag_Q_pi.png\"/>\n",
    "<img src=\"assets/eq_Q_pi.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to calculate vector $\\mathbf{Q^\\pi}$ of action-values for all actions, each elements is corresponding $q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Q_pi(V_pi, model, lmbda):\n",
    "    Q_pi=np.zeros([nb_states, nb_actions])\n",
    "    for s in range(nb_states):\n",
    "        for a in range(nb_actions):\n",
    "            for p_, s_, r_, _ in model[s][a]:\n",
    "                # p_ - transition probability from (s,a) to (s')\n",
    "                # s_ - next state (s')\n",
    "                # r_ - reward on transition from (s,a) to (s')\n",
    "                Rsa = p_ * r_   # expected reward for transition s,a -> s_\n",
    "                Vs_ = V_pi[s_]  # state-value of s_\n",
    "                Q_pi[s, a] += Rsa + lmbda * p_ * Vs_\n",
    "    return Q_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.015 0.014 0.014 0.013]\n",
      " [0.009 0.012 0.011 0.016]\n",
      " [0.024 0.021 0.024 0.014]\n",
      " [0.01  0.01  0.007 0.014]\n",
      " [0.022 0.017 0.016 0.01 ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.054 0.047 0.054 0.007]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.017 0.041 0.035 0.046]\n",
      " [0.07  0.118 0.106 0.059]\n",
      " [0.189 0.176 0.16  0.043]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.088 0.205 0.234 0.176]\n",
      " [0.252 0.538 0.527 0.439]\n",
      " [0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "Q_pi = calc_Q_pi(V_pi, env.env.P, lmbda)   # calc Q_pi for V_pi from previous section\n",
    "print(Q_pi.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct output:\n",
    "```\n",
    "[[0.015 0.014 0.014 0.013]\n",
    " [0.009 0.012 0.011 0.016]\n",
    " [0.024 0.021 0.024 0.014]\n",
    " [0.01  0.01  0.007 0.014]\n",
    " [0.022 0.017 0.016 0.01 ]\n",
    " [0.    0.    0.    0.   ]\n",
    " [0.054 0.047 0.054 0.007]\n",
    " [0.    0.    0.    0.   ]\n",
    " [0.017 0.041 0.035 0.046]\n",
    " [0.07  0.118 0.106 0.059]\n",
    " [0.189 0.176 0.16  0.043]\n",
    " [0.    0.    0.    0.   ]\n",
    " [0.    0.    0.    0.   ]\n",
    " [0.088 0.205 0.234 0.176]\n",
    " [0.252 0.538 0.527 0.439]\n",
    " [0.    0.    0.    0.   ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Policy Iteration algorithm. Start with random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_pi = np.zeros([nb_states])\n",
    "policy = np.ones([nb_states, nb_actions]) / nb_actions  # random policy, 25% each action\n",
    "lmbda = 1.0  # discount\n",
    "\n",
    "for n in range(10):\n",
    "    # flatten MDP\n",
    "    P_pi, R_pi = flatten_mdp(policy, env.env.P)\n",
    "    \n",
    "    # evaluate policy\n",
    "    for k in range(100):\n",
    "        V_pi = R_pi + lmbda * P_pi @ V_pi\n",
    "        \n",
    "    # iterate policy\n",
    "    Q_pi = calc_Q_pi(V_pi, env.env.P, lmbda)\n",
    "    a_max = np.argmax(Q_pi, axis=-1)     # could distribute actions between all max(q) values\n",
    "    policy *= 0  # clear\n",
    "    policy[range(nb_states), a_max] = 1  # pick greedy action    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<' '^' '^' '^']\n",
      " ['<' '<' '<' '<']\n",
      " ['^' 'v' '<' '<']\n",
      " ['<' '>' 'v' '<']]\n"
     ]
    }
   ],
   "source": [
    "a2w = {0:'<', 1:'v', 2:'>', 3:'^'}\n",
    "policy_arrows = [a2w[x] for x in np.argmax(policy, axis=-1)]\n",
    "print(np.array(policy_arrows).reshape([-1, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct optimal policy:\n",
    "```\n",
    "[['<' '^' '^' '^']\n",
    " ['<' '<' '<' '<']\n",
    " ['^' 'v' '<' '<']\n",
    " ['<' '>' 'v' '<']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "Similarly to Policy Evaluation, backup can be performed in matrix form, or by iterating over states. Because state-values change every step, I think it is more intuitive to perform Value Iteration by iterating over states (this is what calc_Q_pi does).\n",
    "\n",
    "Algorithm:\n",
    "* Iterate n=50 times:\n",
    "  * perform Bellman Optimality Equation backup\n",
    "\n",
    "For reference\n",
    "\n",
    "$$ \\Large{ v_{k+1}(s) = \\max\\limits_{a \\in A} q_k(s,a) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "V_pi = np.zeros([nb_states])\n",
    "lmbda = 1.0  # discount\n",
    "\n",
    "for n in range(50):\n",
    "    Q_pi = calc_Q_pi(V_pi, env.env.P, lmbda)\n",
    "    a_max = np.argmax(Q_pi, axis=-1)\n",
    "    V_pi = Q_pi[range(nb_states), a_max]\n",
    "\n",
    "# construct policy\n",
    "policy = np.zeros([nb_states, nb_actions])\n",
    "policy[range(nb_states), a_max] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<' '^' '^' '^']\n",
      " ['<' '<' '<' '<']\n",
      " ['^' 'v' '<' '<']\n",
      " ['<' '>' 'v' '<']]\n"
     ]
    }
   ],
   "source": [
    "a2w = {0:'<', 1:'v', 2:'>', 3:'^'}\n",
    "policy_arrows = [a2w[x] for x in np.argmax(policy, axis=-1)]\n",
    "print(np.array(policy_arrows).reshape([-1, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct optimal policy:\n",
    "```\n",
    "[['<' '^' '^' '^']\n",
    " ['<' '<' '<' '<']\n",
    " ['^' 'v' '<' '<']\n",
    " ['<' '>' 'v' '<']]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
