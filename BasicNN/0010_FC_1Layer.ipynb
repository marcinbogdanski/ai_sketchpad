{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "$$ \\huge{\\underline{\\textbf{ 1-Layer Neural Network }}} $$\n",
    "\n",
    "$$ \\large{\\textbf{ (Binary Logistic Regression) }} $$\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "TODO:\n",
    "    \n",
    "* redo Contents\n",
    "\n",
    "Contents:\n",
    "* [Introduction](#Introduction)\n",
    "* [Load and Explore Data](#Load-and-Explore-Data)\n",
    "* [Preprocess](#Preprocess)\n",
    "* [Neural Network](#Neural-Network)\n",
    "* [Train Classifier](#Train-Classifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents simplest possible **1-layer neural network** trained with backpropagation. I say \"neural network\", but most people would call it binary logistic regression.\n",
    "\n",
    "**Model**\n",
    "\n",
    "* one layer: fully connected with sigmoid activation\n",
    "* loss: binary cross-entropy\n",
    "* optimizer: vanilla SGD\n",
    "\n",
    "**Dependencies**\n",
    "* numpy, matplotlib - neural net and backprop\n",
    "* optional:\n",
    "  * pandas - load college_admission dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(x, y, nb_epochs, W, b):\n",
    "    \"\"\"Params:\n",
    "        x - inputs  - shape: (nb_examples, nb_inputs)\n",
    "        y - targets - shape: (nb_examples, nb_outputs)\n",
    "        W - weights, modified in place - shape: (nb_inputs, nb_outputs)\n",
    "        b - biases, modified in place  - shape: (1, nb_outputs)\n",
    "    \"\"\"\n",
    "    losses = []     # for plotting\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        \n",
    "        # Forward\n",
    "        z = x @ W + b                                      # (eq 1a)    z.shape: (batch_size, nb_neurons)\n",
    "        y_hat = sigmoid(z)                                 # (eq 1b)    y_hat.shape: (batch_size, nb_neurons)\n",
    "        \n",
    "        # Backward\n",
    "        ro = y_hat - y                                     # (eq 3)   binary CE derivative\n",
    "        dW = (x.T @ ro) / len(x)\n",
    "        db = np.sum(ro, axis=0, keepdims=True) / len(x)    # (eq 5)\n",
    "        \n",
    "        # Gradient Check (slows things hugely)\n",
    "        ngW, ngb = numerical_gradient(x, y, W, b)\n",
    "        assert np.allclose(ngW, dW) and np.allclose(ngb, db)\n",
    "\n",
    "        W += -lr * dW\n",
    "        b += -lr * db\n",
    "\n",
    "        # Train loss\n",
    "        loss_train = loss(y_hat, y)                            # binary cross-entropy\n",
    "        losses.append(loss_train)                              # save for plotting\n",
    "\n",
    "        if e % (nb_epochs / 10) == 0:\n",
    "            print('loss {0}'.format(loss_train.round(4)))\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, W, b):                      # x.shape (batch_size, nb_inputs)\n",
    "    return sigmoid( x @ W + b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat, y):                                   #                          y_hat, y shapes: (batch_size, nb_outputs)\n",
    "    result = -y*np.log(y_hat) -(1-y)*np.log(1-y_hat)  # binary cross-entropy     result.shape: (batch_size, 1)\n",
    "    return np.mean( result )                          # average over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x)*(1-sigmoid(x))              # (eq 3)\n",
    "    return 1/(1+np.exp(-x))                           # (eq 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass is pretty simple\n",
    "$$ z = xW \\quad\\quad \\hat{y} = S(z) \\tag{1a, 1b} $$\n",
    "\n",
    "* $x$ is matrix of input features, where rows are separate training examples in mini-batch and columns are features\n",
    "* $W$ is weight matrix, once column corresponds to weights of one neuron\n",
    "* $z$ is matrix of preactivations, where rows correspond to $x$ and single column is our neuron\n",
    "* $\\hat{y}$ is model estimates in a matrix, rows correspond to $x$, one column is our output probability [0..1]\n",
    "* $S$ is a sigmoid function (defined below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid transfer function and its derivative ([proof](https://en.wikipedia.org/wiki/Logistic_function#Derivative))\n",
    "$$ S(z) = \\frac{1}{1+\\epsilon^{-z}} \\quad\\quad \\frac{\\partial S}{\\partial z} = S(z)(1-S(z)) \\tag{2a, 2b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Cross-Entropy loss function\n",
    "$$ J(y,\\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m} -y \\log(\\hat{y}) - (1-y)\\log(1-\\hat{y}) \\tag{3}$$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial z} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward pass\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{W}} = \\frac{1}{m}x^T \\big[ -(y-\\hat{y}) \\odot S'(x) \\big] \\quad\\quad\\quad \\text{ where $\\odot$ is element-wise product} $$\n",
    "\n",
    "If you are wondering how above came about, then good resources are [here](http://cs231n.stanford.edu/handouts/linear-backprop.pdf) and [here](http://cs231n.stanford.edu/handouts/derivatives.pdf), both taken from famous cs231n course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve AND-Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training examples   A    B\n",
    "x_train = np.array([[0.0, 0.0],\n",
    "                    [0.0, 1.0],\n",
    "                    [1.0, 0.0],\n",
    "                    [1.0, 1.0]])\n",
    "\n",
    "# desired outputs     Z\n",
    "y_train = np.array([[0.0],\n",
    "                    [0.0],\n",
    "                    [0.0],\n",
    "                    [1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "nb_epochs = 2000\n",
    "lr = 1\n",
    "\n",
    "# Initialize\n",
    "np.random.seed(0)  # for reproducibility\n",
    "n_inputs, n_outputs = x_train.shape[1], y_train.shape[1]  # get dataset shape\n",
    "W = np.random.normal(scale=n_inputs**-.5, size=[n_inputs, n_outputs])  # Xavier init\n",
    "b = np.zeros(shape=[1, n_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forward(x_train, W, b).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_classifier(x_train, y_train, nb_epochs, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(x_train, W, b).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, W, b):\n",
    "    assert x.ndim == 2; assert W.ndim == 2\n",
    "\n",
    "    z = x @ W + b                               # linear combination,     z.shape: (batch_size, nb_neurons)\n",
    "    y_hat = sigmoid(z)                          # transfer function,      y_hat.shape: (batch_size, nb_neurons)\n",
    "    \n",
    "    assert z.ndim == 2; assert y_hat.ndim == 2\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "TODO: change to BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions\n",
    "\n",
    "\n",
    "$$ L_\\text{MSE}(y,\\hat{y}) = \\frac{1}{2m} \\sum_{i=1}^{m} (y-\\hat{y})^2 $$\n",
    "\n",
    "$m$ is length of mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    assert y_hat.ndim == 2                 # y_hat.shape: (batch_size, 1)\n",
    "    assert y.ndim == 2                     # y.shape: (batch_size, 1)\n",
    "    assert y_hat.shape[1] == 1\n",
    "    \n",
    "    # Option #1: binary cross entropy loss (better)\n",
    "    #result = -1 * ( y*np.log(y_hat) + (1-y)*np.log(1-y_hat) )      # result.shape: [batch_size, 1]\n",
    "    #result = np.mean(result)  # average over batch                 # result: scalar\n",
    "    \n",
    "    # Option #2: MSE loss (simpler)\n",
    "    result = .5 * np.mean((y-y_hat)**2)                          # result: scalar\n",
    "    \n",
    "    assert y_hat.shape[1] == 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(x, y, W, b):\n",
    "    assert x.ndim == 2; assert y.ndim == 2; assert W.ndim == 2\n",
    "    \n",
    "    # Forward pass\n",
    "    z = x @ W + b\n",
    "    y_hat = sigmoid(z)\n",
    "    \n",
    "    # Backward pass\n",
    "    # ro = -(y-y_hat)                       # Option #1: binary CE\n",
    "    ro = -(y-y_hat) * sigmoid_deriv(z)    # Option #2: MSE\n",
    "    del_W = (x.T @ ro) / len(x)\n",
    "    del_b = np.sum(ro, axis=0, keepdims=True) / len(x)\n",
    "    \n",
    "    assert del_W.ndim == 2\n",
    "    assert del_b.ndim == 2\n",
    "    return del_W, del_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(x, y, W, b):\n",
    "    \"\"\"Check gradient numerically\"\"\"\n",
    "    assert W.ndim == 2\n",
    "    assert b.ndim == 2\n",
    "    assert b.shape[0] == 1\n",
    "    \n",
    "    eps = 1e-4\n",
    "    \n",
    "    # Weights\n",
    "    del_W = np.zeros_like(W)    \n",
    "    for r in range(W.shape[0]):\n",
    "        for c in range(W.shape[1]):\n",
    "            W_min = W.copy()\n",
    "            W_pls = W.copy()\n",
    "            \n",
    "            W_min[r, c] -= eps\n",
    "            W_pls[r, c] += eps\n",
    "            \n",
    "            y_hat_pls = forward(x, W_pls, b)\n",
    "            y_hat_min = forward(x, W_min, b)\n",
    "            \n",
    "            l_pls = loss(y, y_hat_pls)\n",
    "            l_min = loss(y, y_hat_min)\n",
    "\n",
    "            del_W[r, c] = (l_pls - l_min) / (eps * 2)\n",
    "            \n",
    "    # Biases\n",
    "    del_b = np.zeros_like(b)\n",
    "    for c in range(b.shape[1]):\n",
    "        b_min = b.copy()\n",
    "        b_pls = b.copy()\n",
    "            \n",
    "        b_min[0, c] -= eps\n",
    "        b_pls[0, c] += eps\n",
    "            \n",
    "        y_hat_pls = forward(x, W, b_pls)\n",
    "        y_hat_min = forward(x, W, b_min)\n",
    "            \n",
    "        l_pls = loss(y, y_hat_pls)\n",
    "        l_min = loss(y, y_hat_min)\n",
    "\n",
    "        del_b[0, c] = (l_pls - l_min) / (eps * 2)\n",
    "    \n",
    "    return del_W, del_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_in = 10\n",
    "N_out = 1\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(10):\n",
    "    x = np.random.rand(batch_size, N_in)\n",
    "    W = np.random.randn(N_in, N_out)\n",
    "    b = np.random.randn(1, N_out)\n",
    "    y = np.random.randint(low=0, high=2, size=[batch_size, N_out])\n",
    "\n",
    "    dW, db = backward(x, y, W, b)\n",
    "    ngW, ngb = numerical_gradient(x, y, W, b)\n",
    "\n",
    "    # print(np.max(np.abs(ngW-dW)))\n",
    "    assert np.allclose(ngW, dW)\n",
    "    assert np.allclose(ngb, db)\n",
    "\n",
    "print('Gradient tests: OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve AND-Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest possible problem, let's learn AND function. Symbol below denotes AND gate in electronics.\n",
    "\n",
    "<img src=\"assets/and_gate.png\">\n",
    "\n",
    "Function we are trying to learn:\n",
    "\n",
    "| A | B | Z (output) |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 |\n",
    "| 1 | 1 | 1 |\n",
    "\n",
    "* A - second input\n",
    "* B - second input\n",
    "* Z - desired ouput\n",
    "\n",
    "Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training examples   A    B\n",
    "x_train = np.array([[0.0, 0.0],\n",
    "                    [0.0, 1.0],\n",
    "                    [1.0, 0.0],\n",
    "                    [1.0, 1.0]])\n",
    "\n",
    "# desired output      Z\n",
    "y_train = np.array([[0.0],\n",
    "                    [0.0],\n",
    "                    [0.0],\n",
    "                    [1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "nb_epochs = 2000\n",
    "lr = 1\n",
    "\n",
    "# Initialize\n",
    "np.random.seed(0)  # for reproducibility\n",
    "n_inputs = x_train.shape[1]\n",
    "n_outputs = y_train.shape[1]\n",
    "W = np.random.normal(scale=n_inputs**-.5, size=[n_inputs, n_outputs])  # Xavier init\n",
    "b = np.zeros(shape=[1, n_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = forward(x_train, W, b)\n",
    "print(outputs.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accumulate statistics during training (for plotting)\n",
    "trace_loss = []                                   # for plotting\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    \n",
    "    # Forward\n",
    "    y_hat = forward(x_train, W, b)\n",
    "    \n",
    "    # Backprop\n",
    "    dW, db = backward(x_train, y_train, W, b)    # with 4x training examples don't bother with mini-batches\n",
    "    W += -lr * dW\n",
    "    b += -lr * db\n",
    "    \n",
    "    # Train loss\n",
    "    loss_train = loss(y_train, y_hat)                   # calculate loss\n",
    "    trace_loss.append(loss_train)                 # save for plotting\n",
    "    \n",
    "    if e % (nb_epochs / 10) == 0:\n",
    "        print('loss {0}'.format(loss_train.round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = forward(x_train, W, b)\n",
    "print(outputs.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trace_loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve College Admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "We will use graduate school admissions data ([https://stats.idre.ucla.edu/stat/data/binary.csv]()). Each row is one student. Columns are as follows:\n",
    "* admit - was student admitted or not? This is our target we will try to predict\n",
    "* gre - student GRE score\n",
    "* gpa - student GPA\n",
    "* rank - prestige of undergrad school, 1 is highest, 4 is lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loda data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('college_admissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show first couple rows. First column is index, added automatically by pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some more information about dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot data, each rank separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=[8,6])\n",
    "axes = axes.flatten()\n",
    "for i, rank in enumerate([1,2,3,4]):\n",
    "    # pick not-admitted students with given rank\n",
    "    tmp = df.loc[(df['rank']==rank) & (df['admit']==0)]\n",
    "    axes[i].scatter(tmp['gpa'], tmp['gre'], color='red', marker='.', label='rejected')\n",
    "    # pick admitted students with given rank\n",
    "    tmp = df.loc[(df['rank']==rank) & (df['admit']==1)]\n",
    "    axes[i].scatter(tmp['gpa'], tmp['gre'], color='green', marker='.', label='admitted')\n",
    "    axes[i].set_title('Rank '+str(rank))\n",
    "    axes[i].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot scatter matrix, just for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = {1: 'red', 2:'green', 3:'blue', 4:'black'}\n",
    "colors = df['rank'].apply(lambda cc:cmap[cc])\n",
    "pd.plotting.scatter_matrix(df[['gre', 'gpa']], c=colors, figsize=[8,6]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below does following things:\n",
    "* convert _rank_ column into one-hot encoded features\n",
    "* normalize _gre_ and _gpa_ columns to zero mean and unit standard deviation\n",
    "* splits of 20% of data as test set\n",
    "* splits into input features (gre, gpa, one-hot-rank) and targets (admit)\n",
    "* convert into numpy\n",
    "* assert shapes are ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies\n",
    "temp = pd.get_dummies(df['rank'], prefix='rank')\n",
    "data = pd.concat([df, temp], axis=1)\n",
    "data.drop(columns='rank', inplace=True)\n",
    "\n",
    "# Normalize\n",
    "for col in ['gre', 'gpa']:\n",
    "    mean, std = data[col].mean(), data[col].std()\n",
    "    # data.loc[:, col] = (data[col]-mean) / std\n",
    "    data[col] = (data[col]-mean) / std\n",
    "\n",
    "# Split off random 20% of the data for testing\n",
    "np.random.seed(0)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.iloc[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features_train = data.drop('admit', axis=1)\n",
    "targets_train =  data['admit']\n",
    "features_test = test_data.drop('admit', axis=1)\n",
    "targets_test = test_data['admit']\n",
    "\n",
    "# Convert to numpy\n",
    "x_train = features_train.values            # features train set (numpy)\n",
    "y_train = targets_train.values[:,None]     # targets train set (numpy)\n",
    "x_test = features_test.values              # features validation set (numpy)\n",
    "y_test = targets_test.values[:,None]       # targets valudation set (numpy)\n",
    "\n",
    "# Assert shapes came right way around\n",
    "assert x_train.shape == (360, 6)\n",
    "assert y_train.shape == (360, 1)\n",
    "assert x_test.shape == (40, 6)\n",
    "assert y_test.shape == (40, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # for reproducibility\n",
    "\n",
    "n_inputs = x_train.shape[1]\n",
    "n_outputs = y_train.shape[1]\n",
    "\n",
    "W = np.random.normal(scale=n_inputs**-.5, size=[n_inputs, n_outputs])  # Xavier init\n",
    "b = np.zeros(shape=[1, n_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 2000\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accumulate statistics during training (for plotting)\n",
    "trace_loss_train = []\n",
    "trace_loss_test = []\n",
    "trace_acc_test = []\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    \n",
    "    # Forward\n",
    "    y_hat = forward(x_train, W, b)\n",
    "    \n",
    "    # Backprop (this re-computes forward pass unneceserily, we do it properly later)\n",
    "    dW, db = backward(x_train, y_train, W, b)\n",
    "    W += -lr * dW\n",
    "    b += -lr * db\n",
    "    \n",
    "    # Train loss\n",
    "    loss_train = loss(y_train, y_hat)\n",
    "    trace_loss_train.append(loss_train)        \n",
    "    \n",
    "    # if e % (nb_epochs / 10) == 0:\n",
    "    y_hat_test = forward(x_test, W, b)\n",
    "    loss_test = loss(y_hat_test, y_test)\n",
    "    trace_loss_test.append(loss_test)\n",
    "    \n",
    "    # Predictions and Accuracy\n",
    "    predictions = fwd(x_test, W)\n",
    "    predictions = predictions > 0.5\n",
    "    acc_test = np.mean(predictions == y_test)\n",
    "    trace_acc_test.append(acc_test)\n",
    "\n",
    "    if e % (nb_epochs / 10) == 0:\n",
    "        print('loss {0}, tacc {1:.3f}'.format(loss_train, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, figsize=[12,6])\n",
    "ax1.plot(trace_loss_train, label='train loss')\n",
    "ax1.plot(trace_loss_test, label='test loss')\n",
    "ax1.legend(loc='right')\n",
    "ax1.grid()\n",
    "ax2.plot(trace_acc_test, color='darkred', label='test accuracy')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quick Regression Test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_result = np.array([0.15224777536275844,\n",
    "                           0.13015955315177377,\n",
    "                           0.11435294270610373,\n",
    "                           0.10585677810621827,\n",
    "                           0.10191394554520483,\n",
    "                           0.1000000143239566,\n",
    "                           0.09898677097344712,\n",
    "                           0.0984065319217976,\n",
    "                           0.0980521765593448,\n",
    "                           0.09782432184510809])\n",
    "assert np.alltrue(trace_loss_train[::200] == correct_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
