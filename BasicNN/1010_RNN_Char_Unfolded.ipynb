{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Udacity_DL_Nanodegree/031%20RNN%20Super%20Basics/SimpleRNN01.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding: 'a'=[0,0,1] 'b'=[0,1,0] 'c'=[1,0,0]\n",
    "\n",
    "#                            < ----- 4x time steps ----- >\n",
    "x_train = np.array([    \n",
    "                    [ [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0] ],  #  'bbcb'\n",
    "                    [ [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0] ],  #  'cbcb'   ^\n",
    "                    [ [0, 1, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0] ],  #  'bcbc'   ^\n",
    "                    [ [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0] ],  #  'cbbc'   ^\n",
    "                    [ [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0] ],  #  'ccbc'   ^\n",
    "    \n",
    "    \n",
    "                    [ [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0] ],  #  'bacb'   | 9x batch size\n",
    "                    [ [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1] ],  #  'ccba'   v\n",
    "                    [ [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0] ],  #  'acbc'   ^\n",
    "                    [ [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0] ],  #  'cbac'   ^\n",
    "                    \n",
    "                    [ [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0] ],  #  'baab'\n",
    "                    [ [0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0] ],  #  'aabc'\n",
    "    \n",
    "                    [ [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1] ],  #  'acaa'\n",
    "                   ])\n",
    "y_train = np.array([ [0],   # <->  no timesteps\n",
    "                     [0],   #\n",
    "                     [0],   #\n",
    "                     [0],   #\n",
    "                     [0],   #\n",
    "                    \n",
    "                     [1],   #  ^\n",
    "                     [1],   #  |  9x batch size\n",
    "                     [1],   #  ^\n",
    "                     [1],   #  |  9x batch size\n",
    "                    \n",
    "                     [0],   #  v\n",
    "                     [0],   #\n",
    "                    \n",
    "                     [1] ]) #\n",
    "x_test = np.array([\n",
    "                   [ [0,1,0], [1,0,0], [1,0,0], [0,1,0] ],  #  'bccb' -> 0\n",
    "                   [ [1,0,0], [1,0,0], [0,1,0], [1,0,0] ],  #  'ccbb' -> 0\n",
    "                   [ [0,1,0], [1,0,0], [0,0,1], [1,0,0] ],  #  'bcac' -> 1\n",
    "                   [ [0,1,0], [0,0,1], [1,0,0], [0,1,0] ],  #  'bacb' -> 1\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int = np.random.choice(3, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_der(x):\n",
    "    return 1.0 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd(x, Wxh, Whh, Who):\n",
    "    assert x.ndim==3 and x.shape[1:]==(4, 3)\n",
    "    \n",
    "    x_t1 = x[:,0,:]\n",
    "    x_t2 = x[:,1,:]\n",
    "    x_t3 = x[:,2,:]\n",
    "    x_t4 = x[:,3,:]\n",
    "        \n",
    "    s_t0 = np.zeros([len(x), len(Whh)])   # [n_batch, n_hid]\n",
    "    z_t1 = s_t0 @ Whh + x_t1 @ Wxh\n",
    "    s_t1 = tanh(z_t1)\n",
    "    z_t2 = s_t1 @ Whh + x_t2 @ Wxh\n",
    "    s_t2 = tanh(z_t2)\n",
    "    z_t3 = s_t2 @ Whh + x_t3 @ Wxh\n",
    "    s_t3 = tanh(z_t3)\n",
    "    z_t4 = s_t3 @ Whh + x_t4 @ Wxh\n",
    "    s_t4 = tanh(z_t4)\n",
    "    z_out = s_t4 @ Who\n",
    "    y_hat = sigmoid( z_out )\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd(x, Wxh, Whh, Who):\n",
    "    assert x.ndim==3 and x.shape[1:]==(4, 3)\n",
    "    \n",
    "    x_t = {}\n",
    "    \n",
    "    for t in range(1, x.shape[1]+1):\n",
    "        print(t)\n",
    "        \n",
    "        x_t[t] = x[:,]\n",
    "    \n",
    "    x_t0 = x[:,0,:]\n",
    "    x_t1 = x[:,1,:]\n",
    "    x_t2 = x[:,2,:]\n",
    "    x_t3 = x[:,3,:]\n",
    "        \n",
    "    s_init = np.zeros([len(x), len(Whh)])   # [n_batch, n_hid]\n",
    "    z_t0 = s_init @ Whh + x_t0 @ Wxh\n",
    "    s_t0 = tanh(z_t0)\n",
    "    z_t1 = s_t0 @ Whh + x_t1 @ Wxh\n",
    "    s_t1 = tanh(z_t1)\n",
    "    z_t2 = s_t1 @ Whh + x_t2 @ Wxh\n",
    "    s_t2 = tanh(z_t2)\n",
    "    z_t3 = s_t2 @ Whh + x_t3 @ Wxh\n",
    "    s_t3 = tanh(z_t3)\n",
    "    z_out = s_t3 @ Who\n",
    "    y_hat = sigmoid( z_out )\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back(x, y, Wxh, Whh, Who):\n",
    "    assert x.ndim==3 and x.shape[1:]==(4, 3)\n",
    "    assert y.ndim==2 and y.shape[1:]==(1,)\n",
    "    assert len(x) == len(y)\n",
    "    \n",
    "    # Forward\n",
    "    x_t1 = x[:,0,:]\n",
    "    x_t2 = x[:,1,:]\n",
    "    x_t3 = x[:,2,:]\n",
    "    x_t4 = x[:,3,:]\n",
    "        \n",
    "    s_t0 = np.zeros([len(x), len(Whh)])   # [n_batch, n_hid]\n",
    "    z_t1 = s_t0 @ Whh + x_t1 @ Wxh\n",
    "    s_t1 = tanh(z_t1)\n",
    "    z_t2 = s_t1 @ Whh + x_t2 @ Wxh\n",
    "    s_t2 = tanh(z_t2)\n",
    "    z_t3 = s_t2 @ Whh + x_t3 @ Wxh\n",
    "    s_t3 = tanh(z_t3)\n",
    "    z_t4 = s_t3 @ Whh + x_t4 @ Wxh\n",
    "    s_t4 = tanh(z_t4)\n",
    "    z_out = s_t4 @ Who\n",
    "    y_hat = sigmoid( z_out )\n",
    "    \n",
    "    # Backward\n",
    "    dWxh = np.zeros_like(Wxh)\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    dWho = np.zeros_like(Who)\n",
    "    \n",
    "    err = -(y-y_hat)/len(x) * sigmoid_der( z_out )\n",
    "    #err /= len(x)\n",
    "    dWho = s_t4.T @ err\n",
    "\n",
    "    ro_t4 = err @ Who.T * tanh_der(z_t4)\n",
    "    dWxh += x_t4.T @ ro_t4\n",
    "    dWhh += s_t3.T @ ro_t4\n",
    "    \n",
    "    ro_t3 = ro_t4 @ Whh.T * tanh_der(z_t3)\n",
    "    dWxh += x_t3.T @ ro_t3\n",
    "    dWhh += s_t2.T @ ro_t3\n",
    "    \n",
    "    ro_t2 = ro_t3 @ Whh.T * tanh_der(z_t2)\n",
    "    dWxh += x_t2.T @ ro_t2\n",
    "    dWhh += s_t1.T @ ro_t2\n",
    "    \n",
    "    ro_t1 = ro_t2 @ Whh.T * tanh_der(z_t1)\n",
    "    dWxh += x_t1.T @ ro_t1\n",
    "    dWhh += s_t0.T @ ro_t1\n",
    "#     ro_t1 = ro_t2 @ Whh.T              # [n_batch, n_hid]\n",
    "#     dWxh += x_t1.T @ ro_t1\n",
    "#     dWhh += s_t0.T @ ro_t1           # [n_hid, n_hid]\n",
    "    \n",
    "    return y_hat, dWxh, dWhh, dWho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(x, y, Wxh, Whh, Who):\n",
    "    y_hat = fwd(x, Wxh, Whh, Who)\n",
    "    return 0.5 * np.mean((y-y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "W_xh = 0.1 * np.random.randn(3, 2)  # Wxh.shape: [n_in, n_hid]\n",
    "W_hh = 0.1 * np.random.randn(2, 2)  # Whh.shape: [n_hid, n_hid]\n",
    "W_ho = 0.1 * np.random.randn(2, 1)  # Who.shape: [n_hid, n_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(x, y, nb_epochs, learning_rate, Wxh, Whh, Who):\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        \n",
    "        y_hat, dWxh, dWhh, dWho = back(x, y, Wxh, Whh, Who)\n",
    "        \n",
    "        Wxh += -learning_rate * dWxh\n",
    "        Whh += -learning_rate * dWhh\n",
    "        Who += -learning_rate * dWho\n",
    "        \n",
    "        # Log and print\n",
    "        loss_train = mse(x, y, Wxh, Whh, Who)\n",
    "        losses.append(loss_train)\n",
    "        if e % (nb_epochs / 10) == 0:\n",
    "            print('loss ', loss_train.round(4))\n",
    "        \n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  0.0545\n",
      "loss  0.0469\n",
      "loss  0.0381\n",
      "loss  0.0293\n",
      "loss  0.0217\n",
      "loss  0.0159\n",
      "loss  0.0119\n",
      "loss  0.0092\n",
      "loss  0.0074\n",
      "loss  0.0061\n"
     ]
    }
   ],
   "source": [
    "losses = train_rnn(x_train, y_train, 1000, 0.1, W_xh, W_hh, W_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd(x_train, W_xh, W_hh, W_ho).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd(x_test, W_xh, W_hh, W_ho).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f906e1516d8>]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEa1JREFUeJzt23+IZeV9x/H3Jzt1JW3w55jaXe0a1vyh+dVksPSPQqpdY0J1Q1eSlRA1UQy0S6AhkBW70JhCapoSKRXKplpESNy4iTDiH5LEpgQJ1pmsjVnNxnHT4GRDnVWR/KDKkm//uGfx7uTuM3d+7XXW9wsu95znfM8zz3cH9nPPOXdSVUiSdDxvGPUCJEmvbQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU1jo17ASjj77LNr06ZNo16GJK0p09PTh6tqfKG6kyIoNm3axNTU1KiXIUlrSpKfDlPnrSdJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktQ0VFAkuSLJgSQzSXYOOL4+yZ7u+KNJNnXjW5JMJ3mie7+0G39Tksf7XoeT3N4duz7JXN+xG1euXUnSYo0tVJBkHXAHsAWYBR5LMllVT/aV3QC8WFWbk2wHbgM+DBwGrqyqQ0neBjwEbKiqXwDv6vsZ08A3+ubbU1U7ltmbJGkFDHNFcQkwU1UHq+oV4F5g67yarcDd3fZe4LIkqap9VXWoG98PnJpkff+JSS4EzgG+u9QmJEmrZ5ig2AA827c/240NrKmqI8BLwFnzarYB+6rq5Xnj19C7gqj+2iQ/SLI3yXmDFpXkpiRTSabm5uaGaEOStBTDBEUGjNViapJcTO921CcG1G0Hvtq3/wCwqareAXyLV69Ujp28andVTVTVxPj4eGP5kqTlGCYoZoH+T/UbgUPHq0kyBpwGvNDtbwTuB66tqmf6T0ryTmCsqqaPjlXV831XHV8G3jN0N5KkFTdMUDwGXJjkgiSn0LsCmJxXMwlc121fDTxcVZXkdOBB4OaqemTA3Ndw7NUESc7t270KeGqINUqSVsmC33qqqiNJdtD7xtI64K6q2p/kVmCqqiaBO4F7kszQu5LY3p2+A9gM7Eqyqxu7vKqe67Y/BHxg3o/8ZJKrgCPdXNcvuTtJ0rLl2GfIa9PExERNTU2NehmStKYkma6qiYXq/MtsSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqWmooEhyRZIDSWaS7BxwfH2SPd3xR5Ns6sa3JJlO8kT3fmk3/qYkj/e9Die5vTWXJGk0FgyKJOuAO4D3AxcB1yS5aF7ZDcCLVbUZ+BJwWzd+GLiyqt4OXAfcA1BVv6iqdx19AT8FvrHAXJKkERjmiuISYKaqDlbVK8C9wNZ5NVuBu7vtvcBlSVJV+6rqUDe+Hzg1yfr+E5NcCJwDfLc112KakiStnGGCYgPwbN/+bDc2sKaqjgAvAWfNq9kG7Kuql+eNXwPsqapaxFySpBNkbIiaQZ/mazE1SS6mdwvp8gF124GPLvLnkeQm4CaA888/f8ApkqSVMMwVxSxwXt/+RuDQ8WqSjAGnAS90+xuB+4Frq+qZ/pOSvBMYq6rpYebqV1W7q2qiqibGx8eHaEOStBTDBMVjwIVJLkhyCr0rgMl5NZP0HlYDXA08XFWV5HTgQeDmqnpkwNzXAF8dZq4h1ilJWgULBkX3nGAH8BDwFPC1qtqf5NYkV3VldwJnJZkBPgUc/QrtDmAzsKvvq7Dn9E3/IX47KI43lyRpBHIyfFifmJioqampUS9DktaUJNNVNbFQnX+ZLUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUNFRRJrkhyIMlMkp0Djq9Psqc7/miSTd34liTTSZ7o3i/tO+eUJLuT/DjJj5Js68avTzKX5PHudePKtCpJWoqxhQqSrAPuALYAs8BjSSar6sm+shuAF6tqc5LtwG3Ah4HDwJVVdSjJ24CHgA3dObcAz1XVW5O8ATizb749VbVjuc1JkpZvmCuKS4CZqjpYVa8A9wJb59VsBe7utvcClyVJVe2rqkPd+H7g1CTru/2PA58HqKrfVNXh5TQiSVodwwTFBuDZvv1ZXr0q+K2aqjoCvAScNa9mG7Cvql5Ocno39rkk309yX5I399cm+UGSvUnOG7YZSdLKGyYoMmCsFlOT5GJ6t6M+0Q2NARuBR6rq3cD3gC92xx4ANlXVO4Bv8eqVyrE/MLkpyVSSqbm5uSHakCQtxTBBMQv0f6rfCBw6Xk2SMeA04IVufyNwP3BtVT3T1T8P/LobB7gPeDdAVT1fVS93418G3jNoUVW1u6omqmpifHx8iDYkSUsxTFA8BlyY5IIkpwDbgcl5NZPAdd321cDDVVXdLaYHgZur6pGjxVVV9K4c3tsNXQY8CZDk3L55rwKeWlRHkqQVteC3nqrqSJId9L6xtA64q6r2J7kVmKqqSeBO4J4kM/SuJLZ3p+8ANgO7kuzqxi6vqueAz3Tn3A7MAR/rjn8yyVXAkW6u61egT0nSEqX34X5tm5iYqKmpqVEvQ5LWlCTTVTWxUJ1/mS1JajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1DRUUSa5IciDJTJKdA46vT7KnO/5okk3d+JYk00me6N4v7TvnlCS7k/w4yY+SbGvNJUkajQWDIsk64A7g/cBFwDVJLppXdgPwYlVtBr4E3NaNHwaurKq3A9cB9/SdcwvwXFW9tZv3PxeYS5I0AsNcUVwCzFTVwap6BbgX2DqvZitwd7e9F7gsSapqX1Ud6sb3A6cmWd/tfxz4PEBV/aaqDrfmWmxjkqSVMUxQbACe7duf7cYG1lTVEeAl4Kx5NduAfVX1cpLTu7HPJfl+kvuSvHkRc0mSTpBhgmLQp/laTE2Si+ndQvpENzQGbAQeqap3A98DvriIn0eSm5JMJZmam5trdyBJWrJhgmIWOK9vfyNw6Hg1ScaA04AXuv2NwP3AtVX1TFf/PPDrbhzgPuDdC83Vr6p2V9VEVU2Mj48P0YYkaSmGCYrHgAuTXJDkFGA7MDmvZpLew2qAq4GHq6q6W0wPAjdX1SNHi6uqgAeA93ZDlwFPtuZaVFeSpBUztlBBVR1JsgN4CFgH3FVV+5PcCkxV1SRwJ3BPkhl6n/63d6fvADYDu5Ls6sYur6rngM9059wOzAEf644fby5J0gjkZPiwPjExUVNTU6NehiStKUmmq2pioTr/MluS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU1DBUWSK5IcSDKTZOeA4+uT7OmOP5pkUze+Jcl0kie690v7zvlON+fj3eucbvz6JHN94zeuTKuSpKUYW6ggyTrgDmALMAs8lmSyqp7sK7sBeLGqNifZDtwGfBg4DFxZVYeSvA14CNjQd95HqmpqwI/dU1U7ltaSJGklDXNFcQkwU1UHq+oV4F5g67yarcDd3fZe4LIkqap9VXWoG98PnJpk/UosXJJ0YgwTFBuAZ/v2Zzn2quCYmqo6ArwEnDWvZhuwr6pe7hv79+720q4k6a9N8oMke5OcN2hRSW5KMpVkam5ubog2JElLMUxQZMBYLaYmycX0bkd9ou/4R6rq7cCfdq+PduMPAJuq6h3At3j1SuXYyat2V9VEVU2Mj48P0YYkaSmGCYpZoP9T/Ubg0PFqkowBpwEvdPsbgfuBa6vqmaMnVNXPuvdfAF+hd4uLqnq+76rjy8B7FteSJGklDRMUjwEXJrkgySnAdmByXs0kcF23fTXwcFVVktOBB4Gbq+qRo8VJxpKc3W3/DvAXwA+7/XP75r0KeGrxbUmSVsqC33qqqiNJdtD7xtI64K6q2p/kVmCqqiaBO4F7kszQu5LY3p2+A9gM7Eqyqxu7HPgV8FAXEuvo3WL6cnf8k0muAo50c12//DYlSUuVqvmPG9aeiYmJmpoa9C1bSdLxJJmuqomF6vzLbElSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTamqUa9h2ZLMAT8d9TqW4Gzg8KgXcYK93np+vfUL9ryW/GFVjS9UdFIExVqVZKqqJka9jhPp9dbz661fsOeTkbeeJElNBoUkqcmgGK3do17ACLzeen699Qv2fNLxGYUkqckrCklSk0GxypKcmeSbSZ7u3s84Tt11Xc3TSa4bcHwyyQ9Xf8XLs5x+k7wxyYNJfpRkf5J/OLGrX5wkVyQ5kGQmyc4Bx9cn2dMdfzTJpr5jN3fjB5K870SuezmW2nOSLUmmkzzRvV96ote+VMv5PXfHz0/yyySfPlFrXnFV5WsVX8AXgJ3d9k7gtgE1ZwIHu/czuu0z+o7/JfAV4Iej7mc1+wXeCPxZV3MK8F3g/aPu6Th9rgOeAd7SrfW/gYvm1fwV8K/d9nZgT7d9UVe/Hrigm2fdqHta5Z7/CPiDbvttwM9G3c9q99x3/OvAfcCnR93PUl9eUay+rcDd3fbdwAcH1LwP+GZVvVBVLwLfBK4ASPJ7wKeAvz8Ba10JS+63qn5dVf8BUFWvAN8HNp6ANS/FJcBMVR3s1novvd779f9b7AUuS5Ju/N6qermqfgLMdPO91i2556raV1WHuvH9wKlJ1p+QVS/Pcn7PJPkgvQ9C+0/QeleFQbH63lxVPwfo3s8ZULMBeLZvf7YbA/gc8E/Ar1dzkStouf0CkOR04Erg26u0zuVasIf+mqo6ArwEnDXkua9Fy+m53zZgX1W9vErrXElL7jnJ7wKfAT57Ata5qsZGvYCTQZJvAb8/4NAtw04xYKySvAvYXFV/M/++5yitVr99848BXwX+uaoOLn6FJ0SzhwVqhjn3tWg5PfcOJhcDtwGXr+C6VtNyev4s8KWq+mV3gbFmGRQroKr+/HjHkvxvknOr6udJzgWeG1A2C7y3b38j8B3gT4D3JPkfer+rc5J8p6reywitYr9H7QaerqrbV2C5q2UWOK9vfyNw6Dg1s134nQa8MOS5r0XL6ZkkG4H7gWur6pnVX+6KWE7PfwxcneQLwOnAb5L8X1X9y+ove4WN+iHJyf4C/pFjH+5+YUDNmcBP6D3QPaPbPnNezSbWxsPsZfVL71nM14E3jLqXBfoco3fv+QJefch58byav+bYh5xf67Yv5tiH2QdZGw+zl9Pz6V39tlH3caJ6nlfzd6zhh9kjX8DJ/qJ3f/bbwNPd+9H/ECeAf+ur+zi9h5ozwMcGzLNWgmLJ/dL7tFbAU8Dj3evGUffU6PUDwI/pfSvmlm7sVuCqbvtUet92mQH+C3hL37m3dOcd4DX6za6V7Bn4W+BXfb/Xx4FzRt3Pav+e++ZY00HhX2ZLkpr81pMkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTf8P39iws5pAHN4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW_xh [[-0.00141006 -0.00181138]\n",
      " [-0.00222526 -0.00256484]\n",
      " [-0.0006515   0.00162391]]\n",
      "dW_hh [[ 0.00050641  0.00406317]\n",
      " [-0.00127919 -0.00321661]]\n",
      "dW_ho [[0.00022729]\n",
      " [0.00575002]]\n"
     ]
    }
   ],
   "source": [
    "# test backward\n",
    "_, dW_xh, dW_hh, dW_ho = back(x_train, y_train, W_xh, W_hh, W_ho)\n",
    "print('dW_xh', dW_xh)\n",
    "print('dW_hh', dW_hh)\n",
    "print('dW_ho', dW_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngW_xh [[-0.00141006 -0.0018114 ]\n",
      " [-0.00222526 -0.00256484]\n",
      " [-0.0006515   0.00162391]]\n",
      "ngW_hh [[ 0.00050641  0.00406316]\n",
      " [-0.00127919 -0.00321661]]\n",
      "ngW_ho [[0.00022729]\n",
      " [0.00575002]]\n"
     ]
    }
   ],
   "source": [
    "# test numerical gradient\n",
    "ngW_xh, ngW_hh, ngW_ho = ngrad(x_train, y_train, W_xh, W_hh, W_ho)\n",
    "print('ngW_xh', ngW_xh)\n",
    "print('ngW_hh', ngW_hh)\n",
    "print('ngW_ho', ngW_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(dW_xh, ngW_xh)\n",
    "assert np.allclose(dW_hh, ngW_hh)\n",
    "assert np.allclose(dW_ho, ngW_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ngrad(x, y, Wxh, Whh, Who):\n",
    "    dWxh = np.zeros_like(Wxh)\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    dWho = np.zeros_like(Who)\n",
    "    eps = 1e-4\n",
    "    \n",
    "    for r in range(len(Wxh)):\n",
    "        for c in range(Wxh.shape[1]):\n",
    "            Wxh_pls = Wxh.copy()\n",
    "            Wxh_min = Wxh.copy()\n",
    "            \n",
    "            Wxh_pls[r, c] += eps\n",
    "            Wxh_min[r, c] -= eps\n",
    "            \n",
    "            l_pls = mse(x, y, Wxh_pls, Whh, Who)\n",
    "            l_min = mse(x, y, Wxh_min, Whh, Who)\n",
    "            \n",
    "            dWxh[r, c] = (l_pls - l_min) / (2*eps)\n",
    "    \n",
    "    for r in range(len(Whh)):\n",
    "        for c in range(Whh.shape[1]):\n",
    "            Whh_pls = Whh.copy()\n",
    "            Whh_min = Whh.copy()\n",
    "            \n",
    "            Whh_pls[r, c] += eps\n",
    "            Whh_min[r, c] -= eps\n",
    "            \n",
    "            l_pls = mse(x, y, Wxh, Whh_pls, Who)\n",
    "            l_min = mse(x, y, Wxh, Whh_min, Who)\n",
    "            \n",
    "            dWhh[r, c] = (l_pls - l_min) / (2*eps)\n",
    "    \n",
    "    for r in range(len(Who)):\n",
    "        for c in range(Who.shape[1]):\n",
    "            Who_pls = Who.copy()\n",
    "            Who_min = Who.copy()\n",
    "            \n",
    "            Who_pls[r, c] += eps\n",
    "            Who_min[r, c] -= eps\n",
    "            \n",
    "            l_pls = mse(x, y, Wxh, Whh, Who_pls)\n",
    "            l_min = mse(x, y, Wxh, Whh, Who_min)\n",
    "            \n",
    "            dWho[r, c] = (l_pls - l_min) / (2*eps)\n",
    "    \n",
    "    \n",
    "    return dWxh, dWhh, dWho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [n_batch, n_time, n_in] - this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array([[[1], [0], [0]],    # dim-0 is training examples in batch - here T=5\n",
    "                    [[0], [1], [0]],    # dim-1 is across time steps - here 3\n",
    "                    [[1], [1], [0]],    # dim-2 is width of features vector - here 1\n",
    "                    [[1], [0], [1]],\n",
    "                    [[1], [1], [1]]], dtype=np.float64)\n",
    "train_y = np.array([[1],                # we ignore time steps, thus only 2 dimensions\n",
    "                    [1],\n",
    "                    [2],\n",
    "                    [2],\n",
    "                    [3]], dtype=np.float64)\n",
    "\n",
    "W_hh = np.array([[1]], dtype=np.float64)  # correct answer is W_hh == [[1]]\n",
    "W_xh = np.array([[1]], dtype=np.float64)  # correct answer is W_xh == [[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd(x, Wxh, Whh):\n",
    "    assert x.ndim==3 and x.shape[1:]==(3, 1)\n",
    "    \n",
    "    x_t1 = x[:,0,:]\n",
    "    x_t2 = x[:,1,:]\n",
    "    x_t3 = x[:,2,:]\n",
    "        \n",
    "    s0 = np.zeros([len(x), 1])   # [batch_size, nb_state_neurons]\n",
    "    s1 = s0 @ Whh + x_t1 @ Wxh\n",
    "    s2 = s1 @ Whh + x_t2 @ Wxh\n",
    "    s3 = s2 @ Whh + x_t3 @ Wxh\n",
    "    y_hat = s3\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back(x, y, Wxh, Whh):\n",
    "    assert x.ndim==3 and x.shape[1:]==(3, 1)\n",
    "    assert y.ndim==2 and y.shape[1:]==(1,)\n",
    "    \n",
    "    # Forward\n",
    "    s0 = np.zeros([len(x), 1])   # [n_batch, n_hid]\n",
    "    x_t1 = x[:,0,:]\n",
    "    x_t2 = x[:,1,:]\n",
    "    x_t3 = x[:,2,:]\n",
    "    s1 = s0 @ Whh + x_t1 @ Wxh\n",
    "    s2 = s1 @ Whh + x_t2 @ Wxh\n",
    "    s3 = s2 @ Whh + x_t3 @ Wxh\n",
    "    y_hat = s3\n",
    "    \n",
    "    # Backward Whh\n",
    "    err = (y-y_hat)\n",
    "    der_t1 = s0\n",
    "    der_t2 = der_t1 @ Whh + s1\n",
    "    der_t3 = der_t2 @ Whh + s2\n",
    "    dWhh = -np.sum(err * der_t3, keepdims=True) / len(x)\n",
    "    \n",
    "    # Backward Wxh\n",
    "    der_t1 = x_t1\n",
    "    der_t2 = der_t1 @ Whh + x_t2\n",
    "    der_t3 = der_t2 @ Whh + x_t3\n",
    "    dWxh = -np.sum(err * der_t3, keepdims=True) / len(x)\n",
    "    \n",
    "    return dWxh, dWhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_2(x, y, Wxh, Whh):\n",
    "    \n",
    "    # Forward\n",
    "    s0 = np.zeros([len(x), 1])   # [n_batch, n_hid]\n",
    "    x_t1 = x[:,0,:]\n",
    "    x_t2 = x[:,1,:]\n",
    "    x_t3 = x[:,2,:]\n",
    "    s1 = s0 @ Whh + x_t1 @ Wxh\n",
    "    s2 = s1 @ Whh + x_t2 @ Wxh\n",
    "    s3 = s2 @ Whh + x_t3 @ Wxh\n",
    "    y_hat = s3\n",
    "    \n",
    "    \n",
    "    # Backward\n",
    "    dWxh = np.zeros_like(Wxh)\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    \n",
    "    ro3 = -(y-y_hat) / len(x)    # [n_batch, n_hid]\n",
    "    dWxh += x_t3.T @ ro3\n",
    "    dWhh += s2.T @ ro3           # [n_hid, n_hid]\n",
    "    \n",
    "    ro2 = ro3 @ Whh.T              # [n_batch, n_hid]\n",
    "    dWxh += x_t2.T @ ro2\n",
    "    dWhh += s1.T @ ro2           # [n_hid, n_hid]\n",
    "    \n",
    "    ro1 = ro2 @ Whh.T              # [n_batch, n_hid]\n",
    "    dWxh += x_t1.T @ ro1\n",
    "    dWhh += s0.T @ ro1           # [n_hid, n_hid]\n",
    "    \n",
    "    return dWxh, dWhh\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(x, y, Wxh, Whh):\n",
    "    y_hat = fwd(x, Wxh, Whh)\n",
    "    return 0.5 * np.mean((y-y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hh = np.array([[0.95]], dtype=np.float64)  # correct answer is W_hh == [[1]]\n",
    "W_xh = np.array([[0.95]], dtype=np.float64)  # correct answer is W_xh == [[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.857375],\n",
       "       [0.9025  ],\n",
       "       [1.759875],\n",
       "       [1.807375],\n",
       "       [2.709875]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd(train_x, W_xh, W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**back check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW_xh [[-0.37204525]]\n",
      "dW_hh [[-0.431718]]\n"
     ]
    }
   ],
   "source": [
    "dW_xh, dW_hh = back(train_x, train_y, W_xh, W_hh)\n",
    "print('dW_xh', dW_xh)\n",
    "print('dW_hh', dW_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**back_2 check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW_xh [[-0.37204525]]\n",
      "dW_hh [[-0.431718]]\n"
     ]
    }
   ],
   "source": [
    "dW_xh, dW_hh = back_2(train_x, train_y, W_xh, W_hh)\n",
    "print('dW_xh', dW_xh)\n",
    "print('dW_hh', dW_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ngrad check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW_xh [[-0.37204525]]\n",
      "dW_hh [[-0.43171798]]\n"
     ]
    }
   ],
   "source": [
    "dW_xh, dW_hh = ngrad(train_x, train_y, W_xh, W_hh)\n",
    "print('dW_xh', dW_xh)\n",
    "print('dW_hh', dW_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrad(x, y, Wxh, Whh):\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    dWxh = np.zeros_like(Wxh)\n",
    "    eps = 1e-4\n",
    "    \n",
    "    for r in range(len(Whh)):\n",
    "        for c in range(Whh.shape[1]):\n",
    "            Whh_pls = Whh.copy()\n",
    "            Whh_min = Whh.copy()\n",
    "            \n",
    "            Whh_pls[r, c] += eps\n",
    "            Whh_min[r, c] -= eps\n",
    "            \n",
    "            l_pls = mse(x, y, Wxh, Whh_pls)\n",
    "            l_min = mse(x, y, Wxh, Whh_min)\n",
    "            \n",
    "            dWhh[r, c] = (l_pls - l_min) / (2*eps)\n",
    "            \n",
    "    for r in range(len(Wxh)):\n",
    "        for c in range(Wxh.shape[1]):\n",
    "            Wxh_pls = Wxh.copy()\n",
    "            Wxh_min = Wxh.copy()\n",
    "            \n",
    "            Wxh_pls[r, c] += eps\n",
    "            Wxh_min[r, c] -= eps\n",
    "            \n",
    "            l_pls = mse(x, y, Wxh_pls, Whh)\n",
    "            l_min = mse(x, y, Wxh_min, Whh)\n",
    "            \n",
    "            dWxh[r, c] = (l_pls - l_min) / (2*eps)\n",
    "    \n",
    "    return dWxh, dWhh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_in = 2\n",
    "n_out = 1\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_in])\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, n_out])\n",
    "\n",
    "W = tf.get_variable(name='W', shape=[n_in, n_out])\n",
    "b = tf.get_variable(name='b', shape=[1, n_out])\n",
    "\n",
    "z = tf.matmul(X, W) + b\n",
    "y_hat = tf.sigmoid(z)\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=Y, logits=z)\n",
    "\n",
    "writer = tf.summary.FileWriter('tf_log', graph=tf.get_default_graph())\n",
    "writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64       # 64 hidden units\n",
    "n_values = 78  # nb notes\n",
    "Tx = 30        # audio chunk size (nb time steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapor = tf.keras.layers.Reshape([1, 78])                        # one hot vector\n",
    "LSTM_cell = tf.keras.layers.LSTM(n_a, return_state = True)         # Used in Step 2.C\n",
    "densor = tf.keras.layers.Dense(n_values, activation='softmax')     # Used in Step 2.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.layers.Input(shape=[Tx, n_values])   # shape: [n_batch, n_time, n_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(?, 30, 78) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Lambda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
