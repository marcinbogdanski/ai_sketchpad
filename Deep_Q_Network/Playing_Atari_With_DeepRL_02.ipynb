{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "$$ \\huge{\\underline{\\textbf{ Playing Atari Games with Deep RL }}} $$\n",
    "\n",
    "$$ \\large{\\textbf{MountainCar + DQN + Memory Reply}} $$\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT TESTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, frames, gamma, eps_decay_steps, eps_target, batch_size, model, mem, callback=None, trace=None):\n",
    "    \"\"\"Episodic Semi-Gradient Sarsa\n",
    "    \n",
    "    Params:\n",
    "        env - environment\n",
    "        ep - number of episodes to run\n",
    "        gamma - discount factor [0..1]\n",
    "        eps - epsilon-greedy param\n",
    "        model      - function approximator, already initialised, with methods:\n",
    "                     eval(state, action) -> float\n",
    "                     train(state, target) -> None\n",
    "    \"\"\"\n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            q_values = model.eval(np.array([st]))\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "    \n",
    "    if eps_decay_steps is not None:\n",
    "        eps_delta = (1-eps_target) / eps_decay_steps\n",
    "        eps = 1\n",
    "    else:\n",
    "        eps = eps_target\n",
    "        \n",
    "    # Fill memory buffer using random policy\n",
    "    while len(mem) < mem.max_len:\n",
    "        S = env.reset();\n",
    "        for t_ in itertools.count():\n",
    "            A = env.action_space.sample()    # random policy\n",
    "            S_, R, done, _ = env.step(A)\n",
    "            mem.append(S, A, R, S_, done)\n",
    "            if done:\n",
    "                break\n",
    "            S = S_\n",
    "\n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(e_, t_, S, A, R, done, eps, model, mem, trace)\n",
    "            \n",
    "            states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "            targets = model.eval(n_states)\n",
    "            targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "            targets[dones] = rewards[dones]                # return of next-to-terminal state is just R\n",
    "            model.train(states, actions, targets)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if tts_ >= frames:\n",
    "                return\n",
    "                \n",
    "            S = S_\n",
    "            \n",
    "            if eps > eps_target:\n",
    "                eps = max(eps - eps_delta, eps_target)\n",
    "                \n",
    "            tts_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enjoy_env(env, frames, episodes, eps, model, callback=None, trace=None, render=True, sleep=0):\n",
    "\n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            q_values = model.eval(np.array([st]))\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(sleep)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(e_, t_, S, A, R, done, eps, model, None, trace)\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if tts_ >= frames:\n",
    "                return\n",
    "                \n",
    "            S = S_\n",
    "                \n",
    "            tts_ += 1\n",
    "            \n",
    "        if e_ >= episodes-1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT TESTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports (source file: [tiles3.py](tiles3.py), [helpers_1001.py](helpers_1001.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Debug_NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tables_logger\n",
    "importlib.reload(tables_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need callback to capture q-value array for whole state-action space at specified episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trace():\n",
    "    def __init__(self, eval_every, render=False, test_states=None, test_labels=None, state_labels=None):\n",
    "        if test_states is not None:\n",
    "            assert test_states.ndim == 2\n",
    "            assert len(test_states) == len(test_labels)\n",
    "            \n",
    "        self.eval_every = eval_every\n",
    "        self.render = render\n",
    "        self.test_states = test_states\n",
    "        self.test_labels = test_labels\n",
    "        self.state_labels = state_labels\n",
    "        \n",
    "        self.total_tstep = 0\n",
    "        \n",
    "        self.q_values = collections.OrderedDict()\n",
    "        self.ep_end_idx = collections.OrderedDict()\n",
    "        self.ep_rewards = collections.defaultdict(float)\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []  # t+1\n",
    "        self.dones = []    # t+1\n",
    "        self.epsilons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    \"\"\"Called from gradient_MC after every episode.\n",
    "    \n",
    "    Params:\n",
    "        episode [int] - episode number\n",
    "        tstep [int]   - timestep within episode\n",
    "        model [obj]   - function approximator\n",
    "        trace [list]  - list to write results to\"\"\"\n",
    "    \n",
    "    if trace.total_tstep == 0 and memory is not None:\n",
    "        print('num exits in memory:', np.count_nonzero(memory._hist_done_1))\n",
    "        \n",
    "    if done_:\n",
    "        trace.ep_end_idx[episode] = trace.total_tstep\n",
    "    \n",
    "    trace.states.append(st)\n",
    "    trace.actions.append(act)\n",
    "    trace.rewards.append(rew_)\n",
    "    trace.dones.append(done_)\n",
    "    trace.epsilons.append(eps)\n",
    "    \n",
    "    trace.ep_rewards[episode] += rew_\n",
    "    \n",
    "    if trace.render:\n",
    "        env.render()\n",
    "            \n",
    "    if trace.eval_every is not None:\n",
    "        if trace.total_tstep % trace.eval_every == 0:\n",
    "            print()\n",
    "            print('â– '*80)\n",
    "            print('episode:', episode, '\\t time step:', tstep,\n",
    "                  '\\t total time step:', trace.total_tstep, '\\t eps:', round(eps,3), \n",
    "                  '\\t wall time:', datetime.datetime.now())\n",
    "\n",
    "            \n",
    "        if trace.total_tstep % trace.eval_every == 0:\n",
    "            \n",
    "            if len(st) == 2:\n",
    "                # We are working with 2D environment,\n",
    "                # plot whole Q-Value functions across whole state space\n",
    "            \n",
    "                q_arr = helpers.eval_state_action_space(model, env, split=[128,128])\n",
    "                trace.q_values[trace.total_tstep] = q_arr\n",
    "\n",
    "                helpers.plot_mountain_car(env, episode, trace.total_tstep, 1000, trace, memory,\n",
    "                                          axis_labels=['state[0]', 'state[1]'],\n",
    "                                          action_labels=['Act 0', 'Act 1', 'Act 2'],\n",
    "                                          action_colors=['red', 'blue', 'green'])\n",
    "                \n",
    "            else:\n",
    "                # Environment is not 2D, so we can't plot whole Q-Value function\n",
    "                # Instead we plot state on standard graph, which is still better than nothing\n",
    "                \n",
    "                if trace.test_states is not None:\n",
    "                    y_hat = model.eval(trace.test_states)\n",
    "                    trace.q_values[trace.total_tstep] = y_hat\n",
    "                \n",
    "                helpers.plot_generic_environment(env, trace.total_tstep, 1000, trace, memory)\n",
    "\n",
    "    trace.total_tstep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Debug_NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tables_logger\n",
    "importlib.reload(tables_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in gym.envs.registry.all():\n",
    "    if env.id.startswith('Pong'):\n",
    "        print(env.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "    obs_rgb = rgb2gray(obs)\n",
    "    obs_110x84 = resize(obs_rgb, output_shape=(110, 84), mode='reflect', anti_aliasing=True)\n",
    "    obs_84x84 = obs_110x84[13:-13,:]\n",
    "    obs_uint8 = (obs_84x84*255).astype(np.uint8)\n",
    "    return obs_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames(frames):\n",
    "    stack = np.array(frames)  # convert LazyFrame to np.ndarray\n",
    "    assert stack.shape == (84, 84, 4)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=stack.shape[-1], figsize=[16,4])\n",
    "    for i in range(stack.shape[-1]):\n",
    "        axes[i].imshow(stack[:,:,i], cmap='gray', vmin=0, vmax=255)\n",
    "        axes[i].set_title('frame '+str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyFrames:\n",
    "    def __init__(self, frames):\n",
    "        assert isinstance(frames, list)\n",
    "        assert isinstance(frames[0], np.ndarray)\n",
    "        self._frames = frames   # list of np.ndarray\n",
    "        \n",
    "    def __array__(self, dtype=None):\n",
    "        # print('__ARRAY__ called')\n",
    "        merged = np.stack(self._frames, axis=-1)\n",
    "        if dtype is not None:\n",
    "            merged = merged.astype(dtype)\n",
    "        return merged\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(np.round(np.stack(self._frames, axis=-1), decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapAtari:\n",
    "    def __init__(self, env):\n",
    "        assert env.observation_space == gym.spaces.Box(low=0, high=255, shape=[210,160,3], dtype=np.uint8)\n",
    "        assert env.action_space == gym.spaces.Discrete(6)\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=[84, 84, 4], dtype=np.uint8)\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "        self._env = env\n",
    "        self._frames = collections.deque(maxlen=4)\n",
    "    \n",
    "    def reset(self):\n",
    "        raw_obs = self._env.reset()           # 160x120 RGB\n",
    "        obs = preprocess(raw_obs)             # 84x84 grayscale\n",
    "        for _ in range(self._frames.maxlen):\n",
    "            self._frames.append(obs)          # replace all\n",
    "        return LazyFrames(list(self._frames))\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        raw_obs, rew, done, info = self._env.step(action)\n",
    "        obs = preprocess(raw_obs)             # 84x84 grayscale\n",
    "        self._frames.append(obs)\n",
    "        return LazyFrames(list(self._frames)), np.sign(rew), done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self._env.render()\n",
    "    \n",
    "    def close(self):\n",
    "        self._env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enjoy Random Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, don't train\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_disp(episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    global axes\n",
    "    if rew_ != 0:\n",
    "        print('rew:', rew_)\n",
    "    if done_:\n",
    "        print('done:', done_)\n",
    "    # plot_frames(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enjoy agent\n",
    "try:\n",
    "    enjoy_env(env, frames=float('inf'), episodes=1, eps=1.0, model=None, callback=callback_disp)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT TESTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pong(frames):\n",
    "    \n",
    "    tf_model = TFNeuralNet(nb_in=8, nb_hid_1=64, nb_hid_2=64, nb_out=6, lr=0.00025)\n",
    "    model = TFFunctApprox(tf_model,\n",
    "                          np.array([-1., -1., -1., -1., -1., -1., -1., -1.]),  # state space low\n",
    "                          np.array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]),\n",
    "                          rew_mean=0,\n",
    "                          rew_std=1,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(8,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000,\n",
    "                 test_states=np.array([[0, 1.4, 0, 0, 0, 0, 0, 0],     # init\n",
    "                                       [0, 0.7, 0, 0, 0, 0, 0, 0],     # half way, no tilt\n",
    "                                       [0, 0.0, 0, 0, 0, 0, 0, 0],]),  # landing pad\n",
    "                 test_labels=['start', 'half-way', 'landing-pad'],\n",
    "                 state_labels=['Pos.x', 'Pos.y', 'Vel.x', 'Vel.y', 'Angle', 'Ang. Vel', 'Left Leg', 'Right Leg'])\n",
    "    \n",
    "    if frames != 0:\n",
    "        with tables.open_file('outarray.h5', mode='w') as f_:\n",
    "            model._model.setup_logdb(f_)\n",
    "            q_learning(env, frames=frames, gamma=.99, eps_decay_steps=50000, eps_target=0.1,\n",
    "                       batch_size=4096, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, don't train\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "trace_rl, model, mem = experiment_pong(frames=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from scratch\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "trace_rl, model, mem = experiment_lunarlander_tf(frames=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "model._model.save('./tf_models/PongNoFrameskip-v4.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "model._model.load('./tf_models/PongNoFrameskip-v4.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_disp(episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    if done_:\n",
    "        print(rew_, done_)\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Enjoy agent\n",
    "try:\n",
    "    enjoy_env(env, frames=float('inf'), eps=1.0, model=model, callback=callback_disp)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximators and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFNeuralNet():\n",
    "    def __init__(self, nb_out):\n",
    "        \n",
    "        self.nb_out = nb_out\n",
    "        \n",
    "        try:    sess.close()\n",
    "        except: pass\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self._log_filename = None\n",
    "        self._dict_layers = {}\n",
    "\n",
    "        self._x = tf.placeholder(name='x', shape=[None, 84, 84, 4], dtype=tf.float32)\n",
    "        self._y = tf.placeholder(name='y', shape=[None, nb_out], dtype=tf.float32)\n",
    "\n",
    "        model = tf.layers.conv2d(self._x, filters=16, kernel_size=[8, 8], strides=[4, 4],\n",
    "                                 padding='valid', activation=tf.nn.relu, name='Conv_1')\n",
    "            \n",
    "        model = tf.layers.conv2d(model, filters=32, kernel_size=[4, 4], strides=[2, 2],\n",
    "                                 padding='valid', activation=tf.nn.relu, name='Conv_2')\n",
    "        \n",
    "        model = tf.layers.flatten(model)\n",
    "        model = tf.layers.dense(model, 256, activation=tf.nn.relu, name='Dense')\n",
    "        self._y_hat = tf.layers.dense(model, nb_out, activation=None, name='Output')\n",
    "        \n",
    "        # self._mse = tf.reduce_mean( tf.pow(self._y - self._y_hat, 2) )\n",
    "        self._loss = tf.losses.mean_squared_error(self._y, self._y_hat)\n",
    "\n",
    "        # No gradient clipping\n",
    "        #self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "        #self._grads_and_vars = self._optimizer.compute_gradients(self._loss)\n",
    "        #self._train_op = self._optimizer.apply_gradients(self._grads_and_vars)\n",
    "        \n",
    "        # Global gradient clipping\n",
    "#         self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "#         gradients, variables = zip(*self._optimizer.compute_gradients(self._loss))\n",
    "#         gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
    "#         self._train_op = self._optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        # Per matrix\n",
    "        self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "        gradients, variables = zip(*self._optimizer.compute_gradients(self._loss))\n",
    "        gradients = [ None if gradient is None else tf.clip_by_norm(gradient, 1.0) for gradient in gradients ]\n",
    "        self._train_op = self._optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        \n",
    "\n",
    "        self._sess = tf.Session()\n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "    def backward(self, x, y):\n",
    "        assert x.ndim == 4\n",
    "        assert y.ndim == 2\n",
    "        assert x.shape == (32, 84, 84, 4)\n",
    "        assert y.shape == (32, 6)\n",
    "        \n",
    "        summary, _, loss = self._sess.run([self._dict_layers, self._train_op, self._loss],\n",
    "                                   feed_dict={self._x: x, self._y: y})\n",
    "        \n",
    "        if self._log_filename is not None:\n",
    "            tables_logger.append_log(self._log_filename, summary)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._sess.run(self._y_hat, feed_dict={self._x: x})\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.saver(self._sess, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self._sess, filepath)\n",
    "        \n",
    "    def setup_logdb(self, filename, batch_save):\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        dict_inout = {\n",
    "            #'batch_x' : cnn._x[0:batch_save,:,:,:],\n",
    "            'batch_y' : cnn._y[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_conv_1 = {\n",
    "            'W': graph.get_tensor_by_name('Conv_1/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('Conv_1/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('gradients/Conv_1/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('gradients/Conv_1/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('Conv_1/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "        }\n",
    "\n",
    "        dict_conv_2 = {\n",
    "            'W': graph.get_tensor_by_name('Conv_2/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('Conv_2/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('gradients/Conv_2/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('gradients/Conv_2/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('Conv_2/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "        }\n",
    "\n",
    "        dict_dense = {\n",
    "            'W': graph.get_tensor_by_name('Dense/kernel:0')[:100,:50],\n",
    "            'b': graph.get_tensor_by_name('Dense/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('gradients/Dense/MatMul_grad/tuple/control_dependency_1:0')[:100,:50],\n",
    "            'db': graph.get_tensor_by_name('gradients/Dense/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('Dense/BiasAdd:0')[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_output = {\n",
    "            'W': graph.get_tensor_by_name('Output/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('Output/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('gradients/Output/MatMul_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('gradients/Output/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('Output/BiasAdd:0')[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_metrics = {\n",
    "            'loss': cnn._loss,\n",
    "        }\n",
    "\n",
    "        self._log_filename = filename\n",
    "        self._dict_layers = {\n",
    "            'inout': dict_inout,\n",
    "            'conv_1': dict_conv_1,\n",
    "            'conv_2': dict_conv_2,\n",
    "            'dense': dict_dense,\n",
    "            'output': dict_output,\n",
    "            'metrics': dict_metrics,\n",
    "        }\n",
    "\n",
    "        tables_logger.create_log(filename, self._dict_layers, batch_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFFunctApprox():\n",
    "\n",
    "    def __init__(self, model, st_low, st_high, rew_mean, rew_std, nb_actions):\n",
    "        \"\"\"Q-function approximator using Keras model\n",
    "\n",
    "        Args:\n",
    "            model: Keras compiled model\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        \n",
    "        assert np.isscalar(st_low) and np.isscalar(st_high)\n",
    "        \n",
    "        if nb_actions != model.nb_out:\n",
    "            raise ValueError('Output shape does not match action_space shape')\n",
    "\n",
    "        # normalise inputs\n",
    "        self._offsets = st_low + (st_high - st_low) / 2\n",
    "        self._scales = 1 / ((st_high - st_low) / 2)\n",
    "        \n",
    "        self._rew_mean = rew_mean\n",
    "        self._rew_std = rew_std\n",
    "\n",
    "    def eval(self, states):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert states.ndim == 4\n",
    "        assert states.shape == (32, 84, 84, 4)\n",
    "        \n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "\n",
    "        y_hat = self._model.forward(inputs)\n",
    "        \n",
    "        return y_hat*self._rew_std + self._rew_mean\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert isinstance(actions, np.ndarray)\n",
    "        assert isinstance(targets, np.ndarray)\n",
    "        assert states.ndim == 4\n",
    "        assert actions.ndim == 1\n",
    "        assert targets.ndim == 1\n",
    "        assert len(states) == len(actions) == len(targets)\n",
    "        \n",
    "        targets = (targets-self._rew_mean) / self._rew_std    # decreases range (std>1) to approx -1..1\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "        all_targets = self._model.forward(inputs)             # this range should be small already\n",
    "        all_targets[np.arange(len(all_targets)), actions] = targets\n",
    "        return self._model.backward(inputs, all_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Circular buffer for DQN memory reply. Fairly fast.\"\"\"\n",
    "\n",
    "    def __init__(self, max_len, state_shape, state_dtype):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_len: maximum capacity\n",
    "        \"\"\"\n",
    "        assert isinstance(max_len, int)\n",
    "        assert max_len > 0\n",
    "\n",
    "        self.max_len = max_len                            # maximum length        \n",
    "        self._curr_insert_ptr = 0                          # index to insert next data sample\n",
    "        self._curr_len = 0                                 # number of currently stored elements\n",
    "\n",
    "        state_arr_shape = [max_len] + list(state_shape)\n",
    "\n",
    "        self._hist_St = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_At = np.zeros(max_len, dtype=int)\n",
    "        self._hist_Rt_1 = np.zeros(max_len, dtype=float)\n",
    "        self._hist_St_1 = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_done_1 = np.zeros(max_len, dtype=bool)\n",
    "\n",
    "    def append(self, St, At, Rt_1, St_1, done_1):\n",
    "        \"\"\"Add one sample to memory, override oldest if max_len reached.\n",
    "\n",
    "        Args:\n",
    "            St [np.ndarray]   - state\n",
    "            At [int]          - action\n",
    "            Rt_1 [float]      - reward\n",
    "            St_1 [np.ndarray] - next state\n",
    "            done_1 [bool]       - next state terminal?\n",
    "        \"\"\"\n",
    "        self._hist_St[self._curr_insert_ptr] = St\n",
    "        self._hist_At[self._curr_insert_ptr] = At\n",
    "        self._hist_Rt_1[self._curr_insert_ptr] = Rt_1\n",
    "        self._hist_St_1[self._curr_insert_ptr] = St_1\n",
    "        self._hist_done_1[self._curr_insert_ptr] = done_1\n",
    "        \n",
    "        if self._curr_len < self.max_len:                 # keep track of current length\n",
    "            self._curr_len += 1\n",
    "            \n",
    "        self._curr_insert_ptr += 1                         # increment insertion pointer\n",
    "        if self._curr_insert_ptr >= self.max_len:         # roll to zero if needed\n",
    "            self._curr_insert_ptr = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples in memory, 0 <= length <= max_len\"\"\"\n",
    "        return self._curr_len\n",
    "\n",
    "    def get_batch(self, batch_len):\n",
    "        \"\"\"Sample batch of data, with repetition\n",
    "\n",
    "        Args:\n",
    "            batch_len: nb of samples to pick\n",
    "\n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, next_done, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert self._curr_len > 0\n",
    "        assert batch_len > 0\n",
    "\n",
    "        \n",
    "        indices = np.random.randint(                   # randint much faster than np.random.sample\n",
    "            low=0, high=self._curr_len, size=batch_len, dtype=int)\n",
    "\n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        if states.dtype == object and isinstance(mem._hist_St[0], LazyFrames): \n",
    "            states = np.stack(states)       # convert to single np.ndarray shape [batch_size, 4, 84, 84]\n",
    "            states_1 = np.stack(states_1)   # where '4' is number of history frames presented to agent\n",
    "\n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "\n",
    "\n",
    "    \n",
    "    def pick_last(self, nb):\n",
    "        \"\"\"Pick last nb elements from memory\n",
    "        \n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, done_1, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert nb <= self._curr_len\n",
    "        \n",
    "        start = self._curr_insert_ptr - nb                # inclusive\n",
    "        end = self._curr_insert_ptr                       # not inclusive\n",
    "        indices = np.array(range(start,end), dtype=int)   # indices to pick, can be negative\n",
    "        indices[indices < 0] += self._curr_len            # loop negative to positive\n",
    "        \n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        if states.dtype == object and isinstance(mem._hist_St[0], LazyFrames): \n",
    "            states = np.stack(states)       # convert to single np.ndarray shape [batch_size, 4, 84, 84]\n",
    "            states_1 = np.stack(states_1)   # where '4' is number of history frames presented to agent\n",
    "        \n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong NN Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_fill(env, mem, one_episode=False):\n",
    "        \n",
    "    # Fill memory buffer using random policy\n",
    "    while len(mem) < mem.max_len:\n",
    "        S = env.reset();\n",
    "        for t_ in itertools.count():\n",
    "            A = env.action_space.sample()    # random policy\n",
    "            S_, R, done, _ = env.step(A)\n",
    "            mem.append(S, A, R, S_, done)\n",
    "            if done:\n",
    "                if one_episode:\n",
    "                    return\n",
    "                break\n",
    "            S = S_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(max_len=10000, state_shape=(), state_dtype=object)\n",
    "mem_fill(env, mem, one_episode=False)\n",
    "print(len(mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, n_states, dones, _ = mem.pick_last(len(mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rewards==-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rewards==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rewards==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del states\n",
    "del n_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)\n",
    "#cnn.setup_logdb('outarray.h5', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS SHOULD CONVERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(50000):\n",
    "    states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "    targets = model.eval(n_states)\n",
    "    targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "    targets[dones] = rewards[dones]                # return of next-to-terminal state is just R\n",
    "    loss = model.train(states, actions, targets)\n",
    "    \n",
    "    losses.append(loss)\n",
    "    if i % 25 == 0:\n",
    "        print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CNN Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(max_len=1000, state_shape=(), state_dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_fill(env, mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.get_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('----')\n",
    "    print(rewards_1[i])\n",
    "    plot_frames(states[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.get_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_nn = states / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.forward(states_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(logdir='tf_log', graph=cnn._sess.graph)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CNN Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'outarray.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.setup_logdb(filename, batch_save=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_logger.print_log(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Lazy Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1, 1, 1])\n",
    "B = np.array([2, 2, 2])\n",
    "C = np.array([3, 3, 3])\n",
    "\n",
    "lf1 = LazyFrame([A, B])\n",
    "lf2 = LazyFrame([B, C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = np.zeros(shape=[10], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem[0] = lf1\n",
    "mem[1] = lf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf1._frames[0][0] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf1._frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(mem[[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(mem[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Mem Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lframes = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lframes_, rew_, done_, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(10, (), object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem.append(lframes, 0, rew_, lframes_, done_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lframes = lframes_\n",
    "lframes_, rew_, done_, _ = env.step(0)\n",
    "mem.append(lframes, 0, rew_, lframes_, done_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mem._hist_St)\n",
    "print(mem._hist_At)\n",
    "print(mem._hist_Rt_1)\n",
    "print(mem._hist_St_1)\n",
    "print(mem._hist_done_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.take(mem._hist_St, np.array([0, 1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames(np.stack(arr)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames(np.stack(arr)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.get_batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states.shape)\n",
    "print(actions.shape)\n",
    "print(rewards_1.shape)\n",
    "print(states_1.shape)\n",
    "print(dones_1.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
