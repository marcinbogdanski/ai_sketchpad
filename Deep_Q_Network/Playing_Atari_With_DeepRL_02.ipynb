{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "$$ \\huge{\\underline{\\textbf{ Playing Atari Games with Deep RL }}} $$\n",
    "\n",
    "$$ \\large{\\textbf{MountainCar + DQN + Memory Reply}} $$\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(start_step, env, frames, gamma, eps_decay_steps, eps_target,\n",
    "               batch_size, model, mem, callback=None, trace=None, render=False, rng=None):\n",
    "    \"\"\"Episodic Semi-Gradient Sarsa\n",
    "    \n",
    "    Params:\n",
    "        env - environment\n",
    "        ep - number of episodes to run\n",
    "        gamma - discount factor [0..1]\n",
    "        eps - epsilon-greedy param\n",
    "        model      - function approximator, already initialised, with methods:\n",
    "                     eval(state, action) -> float\n",
    "                     train(state, target) -> None\n",
    "    \"\"\"\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = np.random\n",
    "    \n",
    "    def policy(st, model, eps):\n",
    "        if rng.rand() > eps:\n",
    "            stack = np.stack([st])  # convert lazyframe to nn input shape [1, 84, 84, 4]\n",
    "            q_values = model.eval(stack)\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "    \n",
    "    if eps_decay_steps is not None:\n",
    "        eps_delta = (1-eps_target) / eps_decay_steps\n",
    "        eps = 1 - start_step*eps_delta\n",
    "        eps = max(eps, eps_target)\n",
    "    else:\n",
    "        eps = eps_target\n",
    "        \n",
    "    assert len(mem) >= batch_size\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        episode_full_reward = 0\n",
    "        if render: env.render()\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, info = env.step(A)\n",
    "            episode_full_reward += info['full-reward']  # unclipped reward\n",
    "            if render: env.render()\n",
    "            \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(tts_+start_step, t_, S, A, R, done, info, eps, episode_full_reward, model, mem, trace)\n",
    "            \n",
    "            states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "            targets = model.eval(n_states)\n",
    "            targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "            targets[dones] = rewards[dones]                # return of next-to-terminal state is just R\n",
    "            model.train(states, actions, targets)\n",
    "\n",
    "            tts_ += 1\n",
    "            if tts_ >= frames:\n",
    "                return\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            S = S_\n",
    "            \n",
    "            if eps > eps_target:\n",
    "                eps = max(eps - eps_delta, eps_target)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, frames, episodes, eps, model, callback=None, trace=None, render=False, sleep=0, rng=None):\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random\n",
    "    \n",
    "    def policy(st, model, eps):\n",
    "        if rng.rand() > eps:\n",
    "            stack = np.stack([st])  # convert lazyframe to nn input shape [1, 84, 84, 4]\n",
    "            q_values = model.eval(stack)\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "        \n",
    "    per_episode_full_rewards = []\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        episode_full_reward = 0\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, info = env.step(A)\n",
    "            episode_full_reward += info['full-reward']   # unclipped reward\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(sleep)\n",
    "            \n",
    "            if callback is not None:\n",
    "                raise  # todo remove callback\n",
    "                callback(tts_, e_, t_, S, A, R, done, eps, model, None, trace)\n",
    "    \n",
    "            if done:\n",
    "                per_episode_full_rewards.append(episode_full_reward)\n",
    "                break\n",
    "                \n",
    "            if frames is not None and tts_ >= frames:\n",
    "                return per_episode_full_rewards\n",
    "                \n",
    "            S = S_\n",
    "                \n",
    "            tts_ += 1\n",
    "            \n",
    "        if episodes is not None and e_ >= episodes-1:\n",
    "            return per_episode_full_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefill_memory(env, mem, steps=None, episodes=None, render=False, rng=None):\n",
    "        \n",
    "    if rng is None:\n",
    "        rng = np.random\n",
    "        \n",
    "    # Fill memory buffer using random policy\n",
    "    tts_ = 0\n",
    "    for e_ in itertools.count():\n",
    "        if episodes is not None and e_ >= episodes:\n",
    "            return\n",
    "        \n",
    "        S = env.reset();\n",
    "        if render: env.render()\n",
    "        \n",
    "        for t_ in itertools.count():\n",
    "            \n",
    "            A = rng.randint(0, env.action_space.n)    # random policy\n",
    "            S_, R, done, _ = env.step(A)\n",
    "            if render: env.render()\n",
    "                \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            tts_ += 1\n",
    "            \n",
    "            if steps is not None and tts_ >= steps:\n",
    "                return\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            S = S_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports (source file: [tiles3.py](tiles3.py), [helpers_1001.py](helpers_1001.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import PIL\n",
    "import gym\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess:\n",
    "    devs = sess.list_devices()\n",
    "    print('\\n'.join([x.name for x in devs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Debug_NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tables_logger\n",
    "importlib.reload(tables_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need callback to capture q-value array for whole state-action space at specified episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trace():\n",
    "    def __init__(self, tf_summary_writer, eval_every, test_states=None):\n",
    "        \n",
    "        self.tf_summary_writer = tf_summary_writer\n",
    "        \n",
    "        self.eval_every = eval_every\n",
    "        self.test_states = test_states\n",
    "        \n",
    "        self.total_step = 0\n",
    "        self.ep_rewards = collections.defaultdict(float)\n",
    "        \n",
    "        self.ep_start_time = None\n",
    "        \n",
    "    def push_summary(self, tag, simple_value, flush=False):\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=tag, simple_value=simple_value)\n",
    "        self.tf_summary_writer.add_summary(summary, self.total_step)\n",
    "        self.tf_summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(total_time_step, tstep, st, act, rew_, done_, info, eps, ep_full_reward, model, memory, trace):\n",
    "    \"\"\"Called from gradient_MC after every episode.\n",
    "    \n",
    "    Params:\n",
    "        episode [int] - episode number\n",
    "        tstep [int]   - timestep within episode\n",
    "        model [obj]   - function approximator\n",
    "        trace [list]  - list to write results to\"\"\"\n",
    "    \n",
    "    assert total_time_step == trace.total_step    \n",
    "    \n",
    "    if tstep == 0:    # Episode just started\n",
    "        trace.ep_start_time = time.time()\n",
    "    \n",
    "    if done_:\n",
    "        trace.ep_rewards[total_time_step] = ep_full_reward\n",
    "        \n",
    "    #\n",
    "    #   Summaries\n",
    "    #\n",
    "    if trace.tf_summary_writer is not None:\n",
    "            \n",
    "        # Epsilon\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag='Metrics/Epsilon', simple_value=eps)\n",
    "        trace.tf_summary_writer.add_summary(summary, trace.total_step)\n",
    "\n",
    "        # Average_Q\n",
    "        if trace.eval_every is not None:\n",
    "            if total_time_step % trace.eval_every == 0:\n",
    "                q_test_values = model.eval(trace.test_states)\n",
    "                q_test_average = np.mean(np.max(q_test_values, axis=-1))  # max over actions\n",
    "                summary = tf.Summary()\n",
    "                summary.value.add(tag='Metrics/Average_Q', simple_value=q_test_average)\n",
    "                trace.tf_summary_writer.add_summary(summary, trace.total_step)\n",
    "\n",
    "        # Ep_Reward\n",
    "        if done_:\n",
    "            episode_wall_time = time.time() - trace.ep_start_time\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag='Metrics/Ep_Reward', simple_value=ep_full_reward)\n",
    "            summary.value.add(tag='Other/StepsPerSec', simple_value=tstep/episode_wall_time)\n",
    "            trace.tf_summary_writer.add_summary(summary, trace.total_step)\n",
    "    \n",
    "    trace.total_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atari Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in gym.envs.registry.all():\n",
    "    if env.id.startswith('Q'):\n",
    "        print(env.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.transform import resize\n",
    "# from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise  # Sentinel\n",
    "# def preprocess(obs):\n",
    "#     obs_rgb = rgb2gray(obs)\n",
    "#     obs_110x84 = resize(obs_rgb, output_shape=(110, 84), mode='reflect', anti_aliasing=True)\n",
    "#     obs_84x84 = obs_110x84[13:-13,:]\n",
    "#     obs_uint8 = (obs_84x84*255).astype(np.uint8)\n",
    "#     return obs_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise  # Sentinel\n",
    "# def preprocess(obs):\n",
    "#     img = PIL.Image.fromarray(obs)\n",
    "#     img = img.convert('L')\n",
    "#     img = img.resize([84, 84], resample=PIL.Image.NEAREST, box=[0,34,160,160+34])\n",
    "#     return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "    img = PIL.Image.fromarray(obs)\n",
    "    img = img.convert('L')\n",
    "    # (left, upper, right, lower)\n",
    "    # OFFSET = 8   # Breakout\n",
    "    OFFSET = 16  # Pong, BeamRider, SpaceInvaders, Breakout?\n",
    "    img = img.resize([84, 84], resample=PIL.Image.BILINEAR, box=[0,210-160-OFFSET,160,210-OFFSET])\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    env = gym.make('SpaceInvaders-v4')\n",
    "    obs = env.reset()\n",
    "    for i in range(50):\n",
    "        obs, _, _, _ = env.step(0)\n",
    "        \n",
    "    plt.imshow(obs); plt.show()\n",
    "    \n",
    "    imgp2 = preprocess(obs)\n",
    "    plt.imshow(imgp2, cmap='gray', vmin=0, vmax=255); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames(frames):\n",
    "    stack = np.array(frames)  # convert LazyFrame to np.ndarray\n",
    "    assert stack.shape == (84, 84, 4)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=stack.shape[-1], figsize=[16,4])\n",
    "    for i in range(stack.shape[-1]):\n",
    "        axes[i].imshow(stack[:,:,i], cmap='gray', vmin=0, vmax=255)\n",
    "        axes[i].set_title('frame '+str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyFrames:\n",
    "    def __init__(self, frames):\n",
    "        assert isinstance(frames, list)\n",
    "        assert isinstance(frames[0], np.ndarray)\n",
    "        self._frames = frames   # list of np.ndarray\n",
    "        \n",
    "    def __array__(self, dtype=None):\n",
    "        # print('__ARRAY__ called')\n",
    "        merged = np.stack(self._frames, axis=-1)\n",
    "        if dtype is not None:\n",
    "            merged = merged.astype(dtype)\n",
    "        return merged\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(np.round(np.stack(self._frames, axis=-1), decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapAtari:\n",
    "    def __init__(self, env):\n",
    "        assert env.observation_space == gym.spaces.Box(low=0, high=255, shape=[210,160,3], dtype=np.uint8)\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=[84, 84, 4], dtype=np.uint8)\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "        self._env = env\n",
    "        self._frames = collections.deque(maxlen=4)\n",
    "    \n",
    "    def reset(self):\n",
    "        raw_obs = self._env.reset()           # 160x120 RGB\n",
    "        obs = preprocess(raw_obs)             # 84x84 grayscale\n",
    "        for _ in range(self._frames.maxlen):\n",
    "            self._frames.append(obs)          # replace all\n",
    "        return LazyFrames(list(self._frames))\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        raw_obs, rew, done, info = self._env.step(action)\n",
    "        obs = preprocess(raw_obs)             # 84x84 grayscale\n",
    "        self._frames.append(obs)\n",
    "        assert 'full-reward' not in info\n",
    "        info['full-reward'] = rew\n",
    "        return LazyFrames(list(self._frames)), np.sign(rew), done, info\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        self._env.seed(seed)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self._env.render(mode=mode)\n",
    "    \n",
    "    def close(self):\n",
    "        self._env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_experiment(env_name, mem_size, mem_fill, tf_logdir=None, seed=None):\n",
    "    global env\n",
    "    if tf_logdir is not None:\n",
    "        assert not os.path.exists(tf_logdir)\n",
    "    try: env.close()\n",
    "    except: pass\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    if env_name == 'MovingDot-v0': env.max_steps = 100\n",
    "    env = WrapAtari(env)\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    \n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    if seed is not None:\n",
    "        tf.set_random_seed(seed)\n",
    "    session = tf.Session()\n",
    "    summary_writer = None\n",
    "    if tf_logdir is not None:\n",
    "        summary_writer = tf.summary.FileWriter(tf_logdir)\n",
    "\n",
    "    neural_net = TFNeuralNet(tf_session=session, tf_summary_writer=summary_writer,\n",
    "                             nb_out=env.action_space.n, lr=0.0002, extended_debug=False)\n",
    "    \n",
    "    model = TFFunctApprox(neural_net, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=env.action_space.n)\n",
    "    \n",
    "    if seed is None:\n",
    "        mem = Memory(max_len=mem_size, state_shape=(), state_dtype=object)\n",
    "        prefill_memory(env, mem, steps=mem_fill, render=False)\n",
    "    else:\n",
    "        mem = Memory(max_len=mem_size, state_shape=(), state_dtype=object, rng=np.random.RandomState(seed))\n",
    "        prefill_memory(env, mem, steps=mem_fill, render=False, rng=np.random.RandomState(seed))\n",
    "    test_states, _, _, _, _, _ = mem.get_batch(32)\n",
    "    \n",
    "    trace = Trace(tf_summary_writer=summary_writer,\n",
    "                  eval_every=1000,\n",
    "                  test_states=test_states)\n",
    "    \n",
    "    if summary_writer is not None:\n",
    "        summary_writer.add_graph(session.graph)\n",
    "        summary_writer.flush()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # neural_net.setup_logdb('outarray.h5', 20)\n",
    "    \n",
    "    return env, trace, model, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, trace, model, mem, epoch_size, nb_total_steps, eps_decay_steps,\n",
    "                   test_frames, test_episodes, stop_filename, render, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random\n",
    "    while trace.total_step < nb_total_steps:\n",
    "        q_learning(trace.total_step, env, frames=epoch_size, gamma=.95, eps_decay_steps=eps_decay_steps, eps_target=0.1,\n",
    "               batch_size=32, model=model, mem=mem, callback=callback, trace=trace, render=render, rng=rng)\n",
    "        # model._model.save('./tf_models/Pong-v0_'+ str(trace.total_step) + '.ckpt')\n",
    "        ep_rewards = evaluate(env, test_frames, test_episodes, eps=0.05, model=model, render=render, rng=rng)\n",
    "        trace.push_summary(tag='Metrics/Reward_Avg', simple_value=np.mean(ep_rewards))\n",
    "        trace.push_summary(tag='Metrics/Reward_Max', simple_value=np.max(ep_rewards))\n",
    "        trace.push_summary(tag='Metrics/Test_Reward', simple_value=np.sum(ep_rewards))   # backward compatibility only\n",
    "        \n",
    "        print('Epoch:', trace.total_step // epoch_size,\n",
    "              '\\tTotal Step:', trace.total_step, \n",
    "              '\\tNum Episodes:', len(ep_rewards),\n",
    "              '\\tTotal Reward:', np.sum(ep_rewards),\n",
    "              '\\tAvg Reward:', np.mean(ep_rewards),\n",
    "              '\\tMax Reward:', np.max(ep_rewards))\n",
    "\n",
    "        if os.path.exists(stop_filename):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movning Dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moving_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, trace, model, mem = setup_experiment(env_name='MovingDot-v0',\n",
    "                                          mem_size=10000, mem_fill=1000,\n",
    "                                          tf_logdir='tf_log_2/movingdot/44_bilin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(env, trace, model, mem, epoch_size=1000, nb_total_steps=10000, eps_decay_steps=10000,\n",
    "               test_frames=None, test_episodes=10, stop_filename='STOP_MOVINGDOT', render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f q_learning -f model._model.backward \\\n",
    "    q_learning(trace.total_step, env, frames=1000, gamma=.95, eps_decay_steps=mem.max_len, eps_target=0.1, \\\n",
    "               batch_size=32, model=model, mem=mem, callback=callback, trace=trace, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_full_rewards = evaluate(env, frames=1050, episodes=None, eps=0.05, model=model, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, trace, model, mem = setup_experiment(env_name='BeamRiderDeterministic-v4',\n",
    "                                          mem_size=200000, mem_fill=10000,\n",
    "                                          tf_logdir='tf_log_2/beam_rider/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(env, trace, model, mem, epoch_size=25000, nb_total_steps=200000, eps_decay_steps=50000,\n",
    "               test_frames=10000, test_episodes=None, stop_filename='STOP_BEAM', render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, trace, model, mem = setup_experiment(env_name='PongDeterministic-v4',\n",
    "                                          mem_size=200000, mem_fill=10000,\n",
    "                                          tf_logdir='tf_log_2/pong/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_experiment(env, trace, model, mem, epoch_size=25000, nb_total_steps=200000, eps_decay_steps=50000,\n",
    "               test_frames=10000, test_episodes=None, stop_filename='STOP_PONG', render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT TESTED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pong(tf_logdir=None):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    session = tf.Session()\n",
    "    summary_writer = None\n",
    "    if tf_logdir is not None:\n",
    "        assert not os.path.exists(tf_logdir)\n",
    "        summary_writer = tf.summary.FileWriter(tf_logdir)\n",
    "\n",
    "    neural_net = TFNeuralNet(tf_session=session, tf_summary_writer=summary_writer,\n",
    "                             nb_out=6, lr=0.00025)\n",
    "    \n",
    "    model = TFFunctApprox(neural_net, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=6)\n",
    "    \n",
    "    mem = Memory(max_len=200000, state_shape=(), state_dtype=object)\n",
    "    prefill_memory(env, mem, steps=10000, render=False)\n",
    "    test_states, _, _, _, _, _ = mem.get_batch(32)\n",
    "    \n",
    "    trace = Trace(tf_summary_writer=summary_writer,\n",
    "                  eval_every=1000,\n",
    "                  test_states=test_states)\n",
    "    \n",
    "    if summary_writer is not None:\n",
    "        summary_writer.add_graph(session.graph)\n",
    "        summary_writer.flush()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # neural_net.setup_logdb('outarray.h5', 20)\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Experiment\n",
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)\n",
    "trace, model, mem = experiment_pong(tf_logdir='tf_log_2/pong/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment\n",
    "while trace.total_step < 200000:\n",
    "    q_learning(trace.total_step, env, frames=25000, gamma=.95, eps_decay_steps=50000, eps_target=0.1,\n",
    "           batch_size=32, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    # model._model.save('./tf_models/Pong-v0_'+ str(trace.total_step) + '.ckpt')\n",
    "    test_reward = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "    trace.push_summary(tag='Metrics/Test_Reward', simple_value=test_reward)\n",
    "    \n",
    "    if os.path.exists('STOP_PONG_10'):\n",
    "        break\n",
    "    \n",
    "    if psutil.swap_memory().percent > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6, logdir='tf_log_2/pong/5')\n",
    "# cnn.setup_logdb('outarray.h5', 5)\n",
    "model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(max_len=200000, state_shape=(), state_dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f preprocess prefill_memory(env, mem, steps=10000)\n",
    "prefill_memory(env, mem, steps=10000, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = Trace()\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "tr = evaluate(env, None, episodes=3, eps=0.0, model=model, render=True)\n",
    "print('tr', tr)\n",
    "rewards.append(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while trace.total_step < 300000:\n",
    "    q_learning(trace.total_step, env, frames=50000, gamma=.95, eps_decay_steps=50000, eps_target=0.1,\n",
    "           batch_size=32, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    # tr = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "    tr = evaluate(env, None, episodes=3, eps=0.0, model=model, render=True)\n",
    "    # cnn.save('./tf_models/PongDeterministic-v4_'+ str(trace.total_step) + '.ckpt')\n",
    "    print('iter', trace.total_step, 'tr', tr)\n",
    "    rewards.append(tr)\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = evaluate(env, 10000, None, eps=0.0, model=model, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout - test test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=env.action_space.n, logdir='tf_log_2/breakout/1')\n",
    "# cnn.setup_logdb('outarray.h5', 5)\n",
    "model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(max_len=500000, state_shape=(), state_dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefill_memory(env, mem, steps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    idx = np.random.randint(len(mem))\n",
    "    plot_frames(mem._hist_St[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = Trace()\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "cnn.save('./tf_models/BreakoutDeterministic-v4_'+ str(trace.total_step) + '.ckpt')\n",
    "print('iter', trace.total_step, 'tr', tr)\n",
    "rewards.append(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while trace.total_step < 2000000:\n",
    "    q_learning(trace.total_step, env, frames=50000, gamma=.95, eps_decay_steps=500000, eps_target=0.1,\n",
    "           batch_size=32, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    tr = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "    cnn.save('./tf_models/BreakoutDeterministic-v4_'+ str(trace.total_step) + '.ckpt')\n",
    "    print('iter', trace.total_step, 'tr', tr)\n",
    "    rewards.append(tr)\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, trace, model, mem = setup_experiment(env_name='SpaceInvadersDeterministic-v4',\n",
    "                                          mem_size=1000000, mem_fill=50000,\n",
    "                                          tf_logdir='tf_log_2/space_invaders/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_experiment(env, trace, model, mem, epoch_size=50000, nb_total_steps=10000000, eps_decay_steps=1000000,\n",
    "               test_frames=10000, test_episodes=None, stop_filename='STOP_PONG', render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT TESTED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, trace, model, mem = setup_experiment(env_name='SpaceInvadersDeterministic-v4',\n",
    "                                          mem_fill=50000, mem_size=500000,\n",
    "                                          tf_logdir='tf_log_2/space_invaders/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(env, trace, model, mem,\n",
    "               nb_total_steps=2000000, test_every=25000, test_steps=10000, stop_filename='STOP_INVADERS_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    idx = np.random.randint(len(mem))\n",
    "    plot_frames(mem._hist_St[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_space_invaders(tf_logdir=None):\n",
    "    \n",
    "    try: env.close()\n",
    "    except: pass\n",
    "    env = gym.make('SpaceInvadersDeterministic-v4')\n",
    "    env = WrapAtari(env)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    session = tf.Session()\n",
    "    summary_writer = None\n",
    "    if tf_logdir is not None:\n",
    "        assert not os.path.exists(tf_logdir)\n",
    "        summary_writer = tf.summary.FileWriter(tf_logdir)\n",
    "\n",
    "    neural_net = TFNeuralNet(tf_session=session, tf_summary_writer=summary_writer,\n",
    "                             nb_out=env.action_space.n, lr=0.00025)\n",
    "    \n",
    "    model = TFFunctApprox(neural_net, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=500000, state_shape=(), state_dtype=object)\n",
    "    prefill_memory(env, mem, steps=50000, render=False)\n",
    "    test_states, _, _, _, _, _ = mem.get_batch(32)\n",
    "    \n",
    "    trace = Trace(tf_summary_writer=summary_writer,\n",
    "                  eval_every=1000,\n",
    "                  test_states=test_states)\n",
    "    \n",
    "    if summary_writer is not None:\n",
    "        summary_writer.add_graph(session.graph)\n",
    "        summary_writer.flush()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # neural_net.setup_logdb('outarray.h5', 20)\n",
    "    \n",
    "    return env, trace, model, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Experiment\n",
    "env, trace, model, mem = experiment_space_invaders(tf_logdir='tf_log_2/space_invaders/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment\n",
    "while trace.total_step < 2000000:\n",
    "    q_learning(trace.total_step, env, frames=25000, gamma=.95, eps_decay_steps=500000, eps_target=0.1,\n",
    "           batch_size=32, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    # model._model.save('./tf_models/Pong-v0_'+ str(trace.total_step) + '.ckpt')\n",
    "    test_reward = evaluate(env, 10000, None, eps=0.05, model=model, render=True)\n",
    "    trace.push_summary(tag='Metrics/Test_Reward', simple_value=test_reward)\n",
    "    \n",
    "    if os.path.exists('STOP_INVADERS_1'):\n",
    "        break\n",
    "    \n",
    "    if psutil.swap_memory().percent > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximators and Memory - Faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFNeuralNet():\n",
    "    def __init__(self, tf_session, tf_summary_writer, nb_out, lr, extended_debug=False):\n",
    "        \n",
    "        self._sess = tf_session\n",
    "        self._summary_writer = tf_summary_writer\n",
    "        self._extended_debug = extended_debug\n",
    "        \n",
    "        self.nb_out = nb_out\n",
    "        \n",
    "        self._log_filename = None\n",
    "        self._dict_layers = {}\n",
    "        self._timestep = 0\n",
    "        \n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "        with tf.variable_scope('NeuralNet'):\n",
    "            \n",
    "            with tf.variable_scope('ZZ_Inputs'):\n",
    "                self._x = tf.placeholder(name='xx', shape=[None, 84, 84, 4], dtype=tf.uint8)\n",
    "                self._y = tf.placeholder(name='yy', shape=[None], dtype=tf.float32)\n",
    "                self._a = tf.placeholder(name='aa', shape=[None], dtype=tf.int32)\n",
    "                self._x_scaled = tf.cast(self._x, tf.float32) / 255.0\n",
    "                tf.summary.histogram('DataIn', self._x)\n",
    "                tf.summary.histogram('Targets', self._y)\n",
    "\n",
    "            with tf.variable_scope('Conv_1'):\n",
    "                model = tf.layers.conv2d(self._x_scaled,\n",
    "                                         filters=16,\n",
    "                                         kernel_size=[8, 8],\n",
    "                                         strides=[4, 4],\n",
    "                                         padding='valid',\n",
    "                                         activation=tf.nn.relu,\n",
    "                                         kernel_initializer=tf.random_normal_initializer(stddev=.01),\n",
    "                                         bias_initializer=tf.constant_initializer(value=.1))\n",
    "                \n",
    "                tf.summary.histogram('Weights', graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/kernel:0'))\n",
    "                tf.summary.histogram('Biases', graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/bias:0'))\n",
    "                tf.summary.histogram('PreActivations', graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/BiasAdd:0'))\n",
    "                \n",
    "                # Norms Ratio\n",
    "                WC1, _ = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='NeuralNet/Conv_1/conv2d')\n",
    "                WC1_bu = tf.get_variable('WC1_bu', trainable=False, initializer=WC1.initialized_value())\n",
    "                tf.summary.scalar( 'Update_Norm_Ratio', tf.norm(WC1 - WC1_bu) / tf.norm(WC1_bu) )\n",
    "                \n",
    "\n",
    "            with tf.variable_scope('Conv_2'):\n",
    "                model = tf.layers.conv2d(model,\n",
    "                                         filters=32,\n",
    "                                         kernel_size=[4, 4],\n",
    "                                         strides=[2, 2],\n",
    "                                         padding='valid',\n",
    "                                         activation=tf.nn.relu,\n",
    "                                         kernel_initializer=tf.random_normal_initializer(stddev=.01),\n",
    "                                         bias_initializer=tf.constant_initializer(value=.1))\n",
    "                \n",
    "                tf.summary.histogram('Weights', graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/kernel:0'))\n",
    "                tf.summary.histogram('Biases', graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/bias:0'))\n",
    "                tf.summary.histogram('PreActivations', graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/BiasAdd:0'))\n",
    "                \n",
    "                # Norms Ratio\n",
    "                WC2, _ = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='NeuralNet/Conv_2/conv2d')\n",
    "                WC2_bu = tf.get_variable('WC2_bu', trainable=False, initializer=WC2.initialized_value())\n",
    "                tf.summary.scalar( 'Update_Norm_Ratio', tf.norm(WC2 - WC2_bu) / tf.norm(WC2_bu) )\n",
    "                \n",
    "            model = tf.layers.flatten(model)\n",
    "            \n",
    "            with tf.variable_scope('Dense'):\n",
    "                model = tf.layers.dense(model,\n",
    "                                        units=256,\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        kernel_initializer=tf.random_normal_initializer(stddev=.01),\n",
    "                                        bias_initializer=tf.constant_initializer(value=.1))\n",
    "                \n",
    "                tf.summary.histogram('Weights', graph.get_tensor_by_name('NeuralNet/Dense/dense/kernel:0'))\n",
    "                tf.summary.histogram('Biases', graph.get_tensor_by_name('NeuralNet/Dense/dense/bias:0'))\n",
    "                tf.summary.histogram('PreActivations', graph.get_tensor_by_name('NeuralNet/Dense/dense/BiasAdd:0'))\n",
    "                \n",
    "                # Norms Ratio\n",
    "                WD, _ = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='NeuralNet/Dense/dense')\n",
    "                WD_bu = tf.get_variable('WD_bu', trainable=False, initializer=WD.initialized_value())\n",
    "                tf.summary.scalar( 'Update_Norm_Ratio', tf.norm(WD - WD_bu) / tf.norm(WD_bu) )\n",
    "            \n",
    "            with tf.variable_scope('Output'):\n",
    "                self._y_hat = tf.layers.dense(model,\n",
    "                                              units=nb_out,\n",
    "                                              activation=None,\n",
    "                                              kernel_initializer=tf.random_normal_initializer(stddev=.01),\n",
    "                                              bias_initializer=tf.constant_initializer(value=.1))\n",
    "                \n",
    "                tf.summary.histogram('Weights', graph.get_tensor_by_name('NeuralNet/Output/dense/kernel:0'))\n",
    "                tf.summary.histogram('Biases', graph.get_tensor_by_name('NeuralNet/Output/dense/bias:0'))\n",
    "                tf.summary.histogram('PreActivations', graph.get_tensor_by_name('NeuralNet/Output/dense/BiasAdd:0'))\n",
    "                \n",
    "                # Norms Ratio\n",
    "                WO, _ = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='NeuralNet/Output/dense')\n",
    "                WO_bu = tf.get_variable('WO_bu', trainable=False, initializer=WO.initialized_value())\n",
    "                tf.summary.scalar( 'Update_Norm_Ratio', tf.norm(WO - WO_bu) / tf.norm(WO_bu) )\n",
    "\n",
    "                \n",
    "            self._one_hot = tf.one_hot(self._a, nb_out, dtype=tf.int32, name='onehot')\n",
    "            self._y_hat_actions = tf.dynamic_partition(self._y_hat, self._one_hot, 2)[1]\n",
    "            self._loss = .5 * tf.losses.mean_squared_error(self._y, self._y_hat_actions)\n",
    "        \n",
    "            \n",
    "            tf.summary.scalar('ZZ_Loss', self._loss)\n",
    "            with tf.name_scope('Metrics/'):\n",
    "                self._loss_summary = tf.summary.scalar('Loss', self._loss)\n",
    "            \n",
    "\n",
    "            \n",
    "            if not extended_debug:\n",
    "                #self._optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "                assert lr == .0002\n",
    "                self._optimizer = tf.train.RMSPropOptimizer(lr, 0.99, 0.0, 1e-6)\n",
    "                self._train_op = self._optimizer.minimize(self._loss)\n",
    "            \n",
    "            else:\n",
    "                with tf.control_dependencies([tf.assign(WC1_bu, WC1), tf.assign(WC2_bu, WC2),\n",
    "                                              tf.assign(WD_bu, WD), tf.assign(WO_bu, WO)]):\n",
    "                    \n",
    "                    # Option 1: no gradient clipping\n",
    "                    self._optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "                    self._grads_and_vars = self._optimizer.compute_gradients(self._loss)\n",
    "                    self._train_op = self._optimizer.apply_gradients(self._grads_and_vars)\n",
    "            \n",
    "                    # Option 2: Global gradient clipping\n",
    "                    #self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "                    #gradients, variables = zip(*self._optimizer.compute_gradients(self._loss))\n",
    "                    #gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
    "                    #self._train_op = self._optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "                    # Option 3: Per matrix\n",
    "                    #self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "                    #gradients, variables = zip(*self._optimizer.compute_gradients(self._loss))\n",
    "                    #gradients = [ None if gradient is None else tf.clip_by_norm(gradient, 1.0) for gradient in gradients ]\n",
    "                    #self._train_op = self._optimizer.apply_gradients(zip(gradients, variables))\n",
    "            \n",
    "                \n",
    "                with tf.variable_scope('NeuralNet/Conv_1/'):\n",
    "                    tf.summary.scalar('GradNorm', tf.norm(graph.get_tensor_by_name(\n",
    "                        'NeuralNet/gradients/NeuralNet/Conv_1/conv2d/Conv2D_grad/tuple/control_dependency_1:0')))\n",
    "                with tf.variable_scope('NeuralNet/Conv_2/'):\n",
    "                    tf.summary.scalar('GradNorm', tf.norm(graph.get_tensor_by_name(\n",
    "                        'NeuralNet/gradients/NeuralNet/Conv_2/conv2d/Conv2D_grad/tuple/control_dependency_1:0')))\n",
    "                with tf.variable_scope('NeuralNet/Dense/'):\n",
    "                    tf.summary.scalar('GradNorm', tf.norm(graph.get_tensor_by_name(\n",
    "                        'NeuralNet/gradients/NeuralNet/Dense/dense/MatMul_grad/tuple/control_dependency_1:0')))\n",
    "                with tf.variable_scope('NeuralNet/Output/'):\n",
    "                    tf.summary.scalar('GradNorm', tf.norm(graph.get_tensor_by_name(\n",
    "                        'NeuralNet/gradients/NeuralNet/Output/dense/MatMul_grad/tuple/control_dependency_1:0')))\n",
    "\n",
    "        self._merged_summaries = tf.summary.merge_all()\n",
    "            \n",
    "        \n",
    "                \n",
    "    def backward(self, x, y, a):\n",
    "        assert x.ndim == 4\n",
    "        assert y.ndim == 1\n",
    "        assert a.ndim == 1\n",
    "        assert x.shape == (32, 84, 84, 4)\n",
    "        \n",
    "        if not self._extended_debug:\n",
    "            loss_summary, _ = \\\n",
    "                self._sess.run([self._loss_summary, self._train_op],\n",
    "                                feed_dict={self._x: x, self._y: y, self._a: a})\n",
    "            self._summary_writer.add_summary(loss_summary, self._timestep)\n",
    "            \n",
    "        else:\n",
    "            dict_layers, merged_summaries, loss_summary, _ = \\\n",
    "                self._sess.run([self._dict_layers, self._merged_summaries, self._loss_summary, self._train_op],\n",
    "                                feed_dict={self._x: x, self._y: y, self._a: a})\n",
    "        \n",
    "            self._summary_writer.add_summary(merged_summaries, self._timestep)\n",
    "            self._summary_writer.add_summary(loss_summary, self._timestep)\n",
    "        \n",
    "            if self._log_filename is not None:\n",
    "                tables_logger.append_log(self._log_filename, dict_layers)\n",
    "        \n",
    "        self._timestep += 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._sess.run(self._y_hat, feed_dict={self._x: x})\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self._sess, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self._sess, filepath)\n",
    "        \n",
    "    def setup_logdb(self, filename, batch_save):\n",
    "        if not self._extended_debug:\n",
    "            raise ValueError('Please enable extended_debug=True in constructor.')\n",
    "        \n",
    "        self._log_filename = filename\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        dict_inout = {\n",
    "            #'batch_x' : cnn._x[0:batch_save,:,:,:],\n",
    "            'batch_y' : cnn._y[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_conv_1 = {\n",
    "            'W': graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Conv_1/conv2d/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Conv_1/conv2d/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "        }\n",
    "\n",
    "        dict_conv_2 = {\n",
    "            'W': graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Conv_2/conv2d/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Conv_2/conv2d/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "        }\n",
    "\n",
    "        dict_dense = {\n",
    "            'W': graph.get_tensor_by_name('NeuralNet/Dense/dense/kernel:0')[:100,:50],\n",
    "            'b': graph.get_tensor_by_name('NeuralNet/Dense/dense/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Dense/dense/MatMul_grad/tuple/control_dependency_1:0')[:100,:50],\n",
    "            'db': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Dense/dense/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('NeuralNet/Dense/dense/BiasAdd:0')[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_output = {\n",
    "            'W': graph.get_tensor_by_name('NeuralNet/Output/dense/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('NeuralNet/Output/dense/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Output/dense/MatMul_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Output/dense/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('NeuralNet/Output/dense/BiasAdd:0')[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_metrics = {\n",
    "            'loss': cnn._loss,\n",
    "        }\n",
    "\n",
    "        self._dict_layers = {\n",
    "            'inout': dict_inout,\n",
    "            'conv_1': dict_conv_1,\n",
    "            'conv_2': dict_conv_2,\n",
    "            'dense': dict_dense,\n",
    "            'output': dict_output,\n",
    "            'metrics': dict_metrics,\n",
    "        }\n",
    "\n",
    "        tables_logger.create_log(filename, self._dict_layers, batch_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFFunctApprox():\n",
    "\n",
    "    def __init__(self, model, st_low, st_high, rew_mean, rew_std, nb_actions):\n",
    "        \"\"\"Q-function approximator using Keras model\n",
    "\n",
    "        Args:\n",
    "            model: Keras compiled model\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        \n",
    "        assert np.isscalar(st_low) and np.isscalar(st_high)\n",
    "        \n",
    "        if nb_actions != model.nb_out:\n",
    "            raise ValueError('Output shape does not match action_space shape')\n",
    "\n",
    "        # normalise inputs\n",
    "        self._offsets = st_low + (st_high - st_low) / 2\n",
    "        self._scales = 1 / ((st_high - st_low) / 2)\n",
    "        \n",
    "        self._rew_mean = rew_mean\n",
    "        self._rew_std = rew_std\n",
    "\n",
    "    def eval(self, states):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert states.ndim == 4\n",
    "        assert states.shape == (32, 84, 84, 4) or states.shape == (1, 84, 84, 4) or states.shape == (10, 84, 84, 4)\n",
    "        \n",
    "        inputs = states # (states - self._offsets) * self._scales\n",
    "\n",
    "        y_hat = self._model.forward(inputs)\n",
    "        \n",
    "        return y_hat*self._rew_std + self._rew_mean\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert isinstance(actions, np.ndarray)\n",
    "        assert isinstance(targets, np.ndarray)\n",
    "        assert states.ndim == 4\n",
    "        assert actions.ndim == 1\n",
    "        assert targets.ndim == 1\n",
    "        assert len(states) == len(actions) == len(targets)\n",
    "        \n",
    "        targets = (targets-self._rew_mean) / self._rew_std    # decreases range (std>1) to approx -1..1\n",
    "\n",
    "        inputs = states # (states - self._offsets) * self._scales\n",
    "#         all_targets = self._model.forward(inputs)             # this range should be small already\n",
    "#         all_targets[np.arange(len(all_targets)), actions] = targets\n",
    "#         return self._model.backward(inputs, all_targets)\n",
    "        return self._model.backward(inputs, targets, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximators and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise # sentinel\n",
    "class TFNeuralNet():\n",
    "    def __init__(self, tf_session, tf_summary_writer, nb_out, lr, extended_debug=False):\n",
    "        \n",
    "        self._sess = tf_session\n",
    "        self._summary_writer = tf_summary_writer\n",
    "        self._extended_debug = extended_debug\n",
    "        \n",
    "        self.nb_out = nb_out\n",
    "        \n",
    "        self._log_filename = None\n",
    "        self._dict_layers = {}\n",
    "        self._timestep = 0\n",
    "        \n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "        with tf.variable_scope('NeuralNet'):\n",
    "            \n",
    "            with tf.variable_scope('ZZ_Inputs'):\n",
    "                self._x = tf.placeholder(name='xx', shape=[None, 84, 84, 4], dtype=tf.float32)\n",
    "                self._y = tf.placeholder(name='yy', shape=[None, nb_out], dtype=tf.float32)\n",
    "                tf.summary.histogram('DataIn', self._x)\n",
    "                tf.summary.histogram('Targets', self._y)\n",
    "\n",
    "            with tf.variable_scope('Conv_1'):\n",
    "                model = tf.layers.conv2d(self._x, filters=16, kernel_size=[8, 8], strides=[4, 4],\n",
    "                                         padding='valid', activation=tf.nn.relu)\n",
    "                tf.summary.histogram('Weights', graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/kernel:0'))\n",
    "                tf.summary.histogram('Biases', graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/bias:0'))\n",
    "                tf.summary.histogram('PreActivations', graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/BiasAdd:0'))\n",
    "                \n",
    "                # Norms Ratio\n",
    "                WC1, _ = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='NeuralNet/Conv_1/conv2d')\n",
    "                WC1_bu = tf.get_variable('WC1_bu', trainable=False, initializer=WC1.initialized_value())\n",
    "                tf.summary.scalar( 'Update_Norm_Ratio', tf.norm(WC1 - WC1_bu) / tf.norm(WC1_bu) )\n",
    "                \n",
    "\n",
    "            with tf.variable_scope('Conv_2'):\n",
    "                model = tf.layers.conv2d(model, filters=32, kernel_size=[4, 4], strides=[2, 2],\n",
    "                                         padding='valid', activation=tf.nn.relu)\n",
    "                tf.summary.histogram('Weights', graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/kernel:0'))\n",
    "                tf.summary.histogram('Biases', graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/bias:0'))\n",
    "                tf.summary.histogram('PreActivations', graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/BiasAdd:0'))\n",
    "                \n",
    "                # Norms Ratio\n",
    "                WC2, _ = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='NeuralNet/Conv_2/conv2d')\n",
    "                WC2_bu = tf.get_variable('WC2_bu', trainable=False, initializer=WC2.initialized_value())\n",
    "                tf.summary.scalar( 'Update_Norm_Ratio', tf.norm(WC2 - WC2_bu) / tf.norm(WC2_bu) )\n",
    "                \n",
    "            model = tf.layers.flatten(model)\n",
    "            \n",
    "            with tf.variable_scope('Dense'):\n",
    "                model = tf.layers.dense(model, 256, activation=tf.nn.relu)\n",
    "                tf.summary.histogram('Weights', graph.get_tensor_by_name('NeuralNet/Dense/dense/kernel:0'))\n",
    "                tf.summary.histogram('Biases', graph.get_tensor_by_name('NeuralNet/Dense/dense/bias:0'))\n",
    "                tf.summary.histogram('PreActivations', graph.get_tensor_by_name('NeuralNet/Dense/dense/BiasAdd:0'))\n",
    "                \n",
    "                # Norms Ratio\n",
    "                WD, _ = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='NeuralNet/Dense/dense')\n",
    "                WD_bu = tf.get_variable('WD_bu', trainable=False, initializer=WD.initialized_value())\n",
    "                tf.summary.scalar( 'Update_Norm_Ratio', tf.norm(WD - WD_bu) / tf.norm(WD_bu) )\n",
    "            \n",
    "            with tf.variable_scope('Output'):\n",
    "                self._y_hat = tf.layers.dense(model, nb_out, activation=None)\n",
    "                tf.summary.histogram('Weights', graph.get_tensor_by_name('NeuralNet/Output/dense/kernel:0'))\n",
    "                tf.summary.histogram('Biases', graph.get_tensor_by_name('NeuralNet/Output/dense/bias:0'))\n",
    "                tf.summary.histogram('PreActivations', graph.get_tensor_by_name('NeuralNet/Output/dense/BiasAdd:0'))\n",
    "                \n",
    "                # Norms Ratio\n",
    "                WO, _ = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='NeuralNet/Output/dense')\n",
    "                WO_bu = tf.get_variable('WO_bu', trainable=False, initializer=WO.initialized_value())\n",
    "                tf.summary.scalar( 'Update_Norm_Ratio', tf.norm(WO - WO_bu) / tf.norm(WO_bu) )\n",
    "\n",
    "            self._loss = tf.losses.mean_squared_error(self._y, self._y_hat)\n",
    "            \n",
    "            tf.summary.scalar('ZZ_Loss', self._loss)\n",
    "            with tf.name_scope('Metrics/'):\n",
    "                self._loss_summary = tf.summary.scalar('Loss', self._loss)\n",
    "            \n",
    "            # No gradient clipping\n",
    "            self._optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            # self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "            self._grads_and_vars = self._optimizer.compute_gradients(self._loss)\n",
    "            \n",
    "            if extended_debug:\n",
    "                with tf.control_dependencies([tf.assign(WC1_bu, WC1), tf.assign(WC2_bu, WC2),\n",
    "                                              tf.assign(WD_bu, WD), tf.assign(WO_bu, WO)]):\n",
    "                    self._train_op = self._optimizer.apply_gradients(self._grads_and_vars)\n",
    "            else:\n",
    "                self._train_op = self._optimizer.apply_gradients(self._grads_and_vars)\n",
    "\n",
    "            # Global gradient clipping\n",
    "    #         self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "    #         gradients, variables = zip(*self._optimizer.compute_gradients(self._loss))\n",
    "    #         gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
    "    #         self._train_op = self._optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "            # Per matrix\n",
    "    #         self._optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.0, momentum=0.95, epsilon=0.01)\n",
    "    #         gradients, variables = zip(*self._optimizer.compute_gradients(self._loss))\n",
    "    #         gradients = [ None if gradient is None else tf.clip_by_norm(gradient, 1.0) for gradient in gradients ]\n",
    "    #         self._train_op = self._optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "            with tf.variable_scope('NeuralNet/Conv_1/'):\n",
    "                tf.summary.scalar('GradNorm', tf.norm(graph.get_tensor_by_name(\n",
    "                    'NeuralNet/gradients/NeuralNet/Conv_1/conv2d/Conv2D_grad/tuple/control_dependency_1:0')))\n",
    "            with tf.variable_scope('NeuralNet/Conv_2/'):\n",
    "                tf.summary.scalar('GradNorm', tf.norm(graph.get_tensor_by_name(\n",
    "                    'NeuralNet/gradients/NeuralNet/Conv_2/conv2d/Conv2D_grad/tuple/control_dependency_1:0')))\n",
    "            with tf.variable_scope('NeuralNet/Dense/'):\n",
    "                tf.summary.scalar('GradNorm', tf.norm(graph.get_tensor_by_name(\n",
    "                    'NeuralNet/gradients/NeuralNet/Dense/dense/MatMul_grad/tuple/control_dependency_1:0')))\n",
    "            with tf.variable_scope('NeuralNet/Output/'):\n",
    "                tf.summary.scalar('GradNorm', tf.norm(graph.get_tensor_by_name(\n",
    "                    'NeuralNet/gradients/NeuralNet/Output/dense/MatMul_grad/tuple/control_dependency_1:0')))\n",
    "        \n",
    "        self._merged_summaries = tf.summary.merge_all()\n",
    "            \n",
    "        \n",
    "                \n",
    "    def backward(self, x, y):\n",
    "        assert x.ndim == 4\n",
    "        assert y.ndim == 2\n",
    "        assert x.shape == (32, 84, 84, 4)\n",
    "        \n",
    "        if not self._extended_debug:\n",
    "            loss_summary, _ = \\\n",
    "                self._sess.run([self._loss_summary, self._train_op],\n",
    "                                feed_dict={self._x: x, self._y: y})\n",
    "            self._summary_writer.add_summary(loss_summary, self._timestep)\n",
    "            \n",
    "        else:\n",
    "            dict_layers, merged_summaries, loss_summary, _ = \\\n",
    "                self._sess.run([self._dict_layers, self._merged_summaries, self._loss_summary, self._train_op],\n",
    "                                feed_dict={self._x: x, self._y: y})\n",
    "        \n",
    "            self._summary_writer.add_summary(merged_summaries, self._timestep)\n",
    "            self._summary_writer.add_summary(loss_summary, self._timestep)\n",
    "        \n",
    "            if self._log_filename is not None:\n",
    "                tables_logger.append_log(self._log_filename, dict_layers)\n",
    "        \n",
    "        self._timestep += 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._sess.run(self._y_hat, feed_dict={self._x: x})\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self._sess, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self._sess, filepath)\n",
    "        \n",
    "    def setup_logdb(self, filename, batch_save):\n",
    "        if not self._extended_debug:\n",
    "            raise ValueError('Please enable extended_debug=True in constructor.')\n",
    "        \n",
    "        self._log_filename = filename\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        dict_inout = {\n",
    "            #'batch_x' : cnn._x[0:batch_save,:,:,:],\n",
    "            'batch_y' : cnn._y[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_conv_1 = {\n",
    "            'W': graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Conv_1/conv2d/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Conv_1/conv2d/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('NeuralNet/Conv_1/conv2d/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "        }\n",
    "\n",
    "        dict_conv_2 = {\n",
    "            'W': graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Conv_2/conv2d/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Conv_2/conv2d/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('NeuralNet/Conv_2/conv2d/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "        }\n",
    "\n",
    "        dict_dense = {\n",
    "            'W': graph.get_tensor_by_name('NeuralNet/Dense/dense/kernel:0')[:100,:50],\n",
    "            'b': graph.get_tensor_by_name('NeuralNet/Dense/dense/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Dense/dense/MatMul_grad/tuple/control_dependency_1:0')[:100,:50],\n",
    "            'db': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Dense/dense/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('NeuralNet/Dense/dense/BiasAdd:0')[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_output = {\n",
    "            'W': graph.get_tensor_by_name('NeuralNet/Output/dense/kernel:0'),\n",
    "            'b': graph.get_tensor_by_name('NeuralNet/Output/dense/bias:0'),\n",
    "            'dW': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Output/dense/MatMul_grad/tuple/control_dependency_1:0'),\n",
    "            'db': graph.get_tensor_by_name('NeuralNet/gradients/NeuralNet/Output/dense/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "            'z': graph.get_tensor_by_name('NeuralNet/Output/dense/BiasAdd:0')[0:batch_save,:],\n",
    "        }\n",
    "\n",
    "        dict_metrics = {\n",
    "            'loss': cnn._loss,\n",
    "        }\n",
    "\n",
    "        self._dict_layers = {\n",
    "            'inout': dict_inout,\n",
    "            'conv_1': dict_conv_1,\n",
    "            'conv_2': dict_conv_2,\n",
    "            'dense': dict_dense,\n",
    "            'output': dict_output,\n",
    "            'metrics': dict_metrics,\n",
    "        }\n",
    "\n",
    "        tables_logger.create_log(filename, self._dict_layers, batch_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise # sentinel\n",
    "class TFFunctApprox():\n",
    "\n",
    "    def __init__(self, model, st_low, st_high, rew_mean, rew_std, nb_actions):\n",
    "        \"\"\"Q-function approximator using Keras model\n",
    "\n",
    "        Args:\n",
    "            model: Keras compiled model\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        \n",
    "        assert np.isscalar(st_low) and np.isscalar(st_high)\n",
    "        \n",
    "        if nb_actions != model.nb_out:\n",
    "            raise ValueError('Output shape does not match action_space shape')\n",
    "\n",
    "        # normalise inputs\n",
    "        self._offsets = st_low + (st_high - st_low) / 2\n",
    "        self._scales = 1 / ((st_high - st_low) / 2)\n",
    "        \n",
    "        self._rew_mean = rew_mean\n",
    "        self._rew_std = rew_std\n",
    "\n",
    "    def eval(self, states):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert states.ndim == 4\n",
    "        assert states.shape == (32, 84, 84, 4) or states.shape == (1, 84, 84, 4) or states.shape == (10, 84, 84, 4)\n",
    "        \n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "\n",
    "        y_hat = self._model.forward(inputs)\n",
    "        \n",
    "        return y_hat*self._rew_std + self._rew_mean\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert isinstance(actions, np.ndarray)\n",
    "        assert isinstance(targets, np.ndarray)\n",
    "        assert states.ndim == 4\n",
    "        assert actions.ndim == 1\n",
    "        assert targets.ndim == 1\n",
    "        assert len(states) == len(actions) == len(targets)\n",
    "        \n",
    "        targets = (targets-self._rew_mean) / self._rew_std    # decreases range (std>1) to approx -1..1\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "        all_targets = self._model.forward(inputs)             # this range should be small already\n",
    "        all_targets[np.arange(len(all_targets)), actions] = targets\n",
    "        return self._model.backward(inputs, all_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Circular buffer for DQN memory reply. Fairly fast.\"\"\"\n",
    "\n",
    "    def __init__(self, max_len, state_shape, state_dtype, rng=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_len: maximum capacity\n",
    "        \"\"\"\n",
    "        assert isinstance(max_len, int)\n",
    "        assert max_len > 0\n",
    "        \n",
    "        if rng is None:\n",
    "            self._random = np.random  # reuse numpy\n",
    "        else:\n",
    "            self._random = rng        # use provided random number generator\n",
    "\n",
    "        self.max_len = max_len                            # maximum length        \n",
    "        self._curr_insert_ptr = 0                          # index to insert next data sample\n",
    "        self._curr_len = 0                                 # number of currently stored elements\n",
    "\n",
    "        state_arr_shape = [max_len] + list(state_shape)\n",
    "\n",
    "        self._hist_St = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_At = np.zeros(max_len, dtype=int)\n",
    "        self._hist_Rt_1 = np.zeros(max_len, dtype=float)\n",
    "        self._hist_St_1 = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_done_1 = np.zeros(max_len, dtype=bool)\n",
    "\n",
    "    def append(self, St, At, Rt_1, St_1, done_1):\n",
    "        \"\"\"Add one sample to memory, override oldest if max_len reached.\n",
    "\n",
    "        Args:\n",
    "            St [np.ndarray]   - state\n",
    "            At [int]          - action\n",
    "            Rt_1 [float]      - reward\n",
    "            St_1 [np.ndarray] - next state\n",
    "            done_1 [bool]       - next state terminal?\n",
    "        \"\"\"\n",
    "        self._hist_St[self._curr_insert_ptr] = St\n",
    "        self._hist_At[self._curr_insert_ptr] = At\n",
    "        self._hist_Rt_1[self._curr_insert_ptr] = Rt_1\n",
    "        self._hist_St_1[self._curr_insert_ptr] = St_1\n",
    "        self._hist_done_1[self._curr_insert_ptr] = done_1\n",
    "        \n",
    "        if self._curr_len < self.max_len:                 # keep track of current length\n",
    "            self._curr_len += 1\n",
    "            \n",
    "        self._curr_insert_ptr += 1                         # increment insertion pointer\n",
    "        if self._curr_insert_ptr >= self.max_len:         # roll to zero if needed\n",
    "            self._curr_insert_ptr = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples in memory, 0 <= length <= max_len\"\"\"\n",
    "        return self._curr_len\n",
    "\n",
    "    def get_batch(self, batch_len):\n",
    "        \"\"\"Sample batch of data, with repetition\n",
    "\n",
    "        Args:\n",
    "            batch_len: nb of samples to pick\n",
    "\n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, next_done, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert self._curr_len > 0\n",
    "        assert batch_len > 0\n",
    "\n",
    "        \n",
    "        indices = self._random.randint(                   # randint much faster than np.random.sample\n",
    "            low=0, high=self._curr_len, size=batch_len, dtype=int)\n",
    "\n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        if states.dtype == object and isinstance(self._hist_St[0], LazyFrames): \n",
    "            states = np.stack(states)       # convert to single np.ndarray shape [batch_size, 4, 84, 84]\n",
    "            states_1 = np.stack(states_1)   # where '4' is number of history frames presented to agent\n",
    "\n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "\n",
    "\n",
    "    \n",
    "    def pick_last(self, nb):\n",
    "        \"\"\"Pick last nb elements from memory\n",
    "        \n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, done_1, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert nb <= self._curr_len\n",
    "        \n",
    "        start = self._curr_insert_ptr - nb                # inclusive\n",
    "        end = self._curr_insert_ptr                       # not inclusive\n",
    "        indices = np.array(range(start,end), dtype=int)   # indices to pick, can be negative\n",
    "        indices[indices < 0] += self._curr_len            # loop negative to positive\n",
    "        \n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        if states.dtype == object and isinstance(self._hist_St[0], LazyFrames): \n",
    "            states = np.stack(states)       # convert to single np.ndarray shape [batch_size, 4, 84, 84]\n",
    "            states_1 = np.stack(states_1)   # where '4' is number of history frames presented to agent\n",
    "        \n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "below is just testing\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do regression test:\n",
    "* restart kernel (optional?)\n",
    "* run all cells up to and including Experiment Setup\n",
    "  * but __DO NOT__ run cell with gpu_options.allow_growth = True\n",
    "* run try_freeze_random_seeds\n",
    "* run cells\n",
    "* compare tensorflow graph with tf_log_2/movingdot/regression_seedXXXX_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym.spaces\n",
    "def try_freeze_random_seeds(seed, reproducible):\n",
    "    \"\"\"Will attempt to make execution fully reproducible\n",
    "\n",
    "    Params:\n",
    "        seed (int): Set random seeds for following modules:\n",
    "            random, numpy.random, tensorflow, gym.spaces\n",
    "        reproducible (bool): if True, then:\n",
    "            Disbale GPU by setting env. var. CUDA_VISIBLE_DEVICES to '-1'\n",
    "            Disable randomised hashing by setting PYTHONHASHSEED to '0'\n",
    "            Force single-threadeed execution in tensorflow\n",
    "    \"\"\"\n",
    "    #\n",
    "    #   Environment variables\n",
    "    #\n",
    "    if reproducible:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # disable GPU\n",
    "        os.environ['PYTHONHASHSEED'] = '0'         # force reproducible hasing\n",
    "\n",
    "    #\n",
    "    #   Random seeds\n",
    "    #\n",
    "    print('Using random seed:', seed)\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "    # always call this, if not called expicitly, defaults to seed==0\n",
    "    gym.spaces.seed(seed)\n",
    "\n",
    "    #\n",
    "    #   Set TF session\n",
    "    #\n",
    "    if reproducible:\n",
    "        config = tf.ConfigProto()    \n",
    "        config.intra_op_parallelism_threads=1\n",
    "        config.inter_op_parallelism_threads=1\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "try_freeze_random_seeds(1234, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moving_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_freeze_random_seeds(1234, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, trace, model, mem = setup_experiment(env_name='MovingDot-v0',\n",
    "                                          mem_size=10000, mem_fill=1000,\n",
    "                                          tf_logdir='tf_log_2/movingdot/regression_seed1234_actmask_2',\n",
    "                                          seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.load('./gpuscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tensorflow init (seed=1234)\n",
    "arr = np.array([[-0.06707259,  0.01230972, -0.01680173,  0.06786124],\n",
    "       [ 0.04711869, -0.01441539,  0.04921476,  0.02538211],\n",
    "       [ 0.04901032, -0.03892146,  0.03339036,  0.01314713],\n",
    "       [ 0.01419152, -0.05306549, -0.03446849,  0.0440792 ],\n",
    "       [ 0.03037152, -0.06045384, -0.0203958 ,  0.042386  ],\n",
    "       [ 0.0487171 ,  0.04372656,  0.0037141 ,  0.01883662],\n",
    "       [-0.06191136,  0.01071654, -0.05378162, -0.03057779],\n",
    "       [-0.00868624, -0.04274424, -0.03951517,  0.04651371]],\n",
    "      dtype=np.float32)\n",
    "WC1_tensor, _ = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='NeuralNet/Conv_1/conv2d')\n",
    "WC1 = model._model._sess.run(WC1_tensor)\n",
    "np.allclose(WC1[:,:,:,0][0], arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC1[:,:,:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1234)\n",
    "run_experiment(env, trace, model, mem, epoch_size=200, nb_total_steps=2000, eps_decay_steps=2000,\n",
    "               test_frames=None, test_episodes=10, stop_filename='STOP_MOVINGDOT', render=False, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong NN Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(max_len=10000, state_shape=(), state_dtype=object)\n",
    "prefill_memory(env, mem, one_episode=False)\n",
    "print(len(mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, n_states, dones, _ = mem.pick_last(len(mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rewards==-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rewards==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rewards==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del states\n",
    "del n_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)\n",
    "#cnn.setup_logdb('outarray.h5', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS SHOULD CONVERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(50000):\n",
    "    states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "    targets = model.eval(n_states)\n",
    "    targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "    targets[dones] = rewards[dones]                # return of next-to-terminal state is just R\n",
    "    loss = model.train(states, actions, targets)\n",
    "    \n",
    "    losses.append(loss)\n",
    "    if i % 25 == 0:\n",
    "        print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CNN Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(max_len=1000, state_shape=(), state_dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefill_memory(env, mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.get_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('----')\n",
    "    print(rewards_1[i])\n",
    "    plot_frames(states[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.get_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_nn = states / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.forward(states_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(logdir='tf_log', graph=cnn._sess.graph)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CNN Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'outarray.h5'\n",
    "cnn.setup_logdb(filename, batch_save=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_logger.print_log(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Lazy Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1, 1, 1])\n",
    "B = np.array([2, 2, 2])\n",
    "C = np.array([3, 3, 3])\n",
    "\n",
    "lf1 = LazyFrame([A, B])\n",
    "lf2 = LazyFrame([B, C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = np.zeros(shape=[10], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem[0] = lf1\n",
    "mem[1] = lf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf1._frames[0][0] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf1._frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(mem[[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(mem[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = TFNeuralNet(nb_out=6)\n",
    "model = TFFunctApprox(cnn, st_low=0, st_high=255, rew_mean=0, rew_std=1, nb_actions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_disp(total_step, episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    if done_:\n",
    "        print(total_step)\n",
    "    # pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "tr = evaluate(env, 1000, None, eps=0.05, model=model, callback=callback_disp, render=True)\n",
    "print(time.time() - ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(10000, (), object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefill_memory(env, mem, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.pick_last(len(mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(dones_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rew  1', np.count_nonzero(rewards_1==1))\n",
    "print('rew  0', np.count_nonzero(rewards_1==0))\n",
    "print('rew -1', np.count_nonzero(rewards_1==-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Mem Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MovingDot3-v0')\n",
    "env = WrapAtari(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lframes = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lframes_, rew_, done_, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(10, (), object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem.append(lframes, 0, rew_, lframes_, done_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lframes = lframes_\n",
    "lframes_, rew_, done_, _ = env.step(0)\n",
    "mem.append(lframes, 0, rew_, lframes_, done_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mem._hist_St)\n",
    "print(mem._hist_At)\n",
    "print(mem._hist_Rt_1)\n",
    "print(mem._hist_St_1)\n",
    "print(mem._hist_done_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.take(mem._hist_St, np.array([0, 1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames(np.stack(arr)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frames(np.stack(arr)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards_1, states_1, dones_1, indices = mem.get_batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states.shape)\n",
    "print(actions.shape)\n",
    "print(rewards_1.shape)\n",
    "print(states_1.shape)\n",
    "print(dones_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_render(env, frames, episodes, eps, model, callback=None, trace=None, render=True, sleep=0):\n",
    "\n",
    "    rendered_frames = []\n",
    "    \n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            stack = np.stack([st])  # convert lazyframe to nn input shape [1, 84, 84, 4]\n",
    "            q_values = model.eval(stack)\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "        \n",
    "    total_reward = 0\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        if render:\n",
    "            rendered_frames.append(env.render(mode='rgb_array'))\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            \n",
    "            total_reward += R\n",
    "            \n",
    "            if render:\n",
    "                rendered_frames.append(env.render(mode='rgb_array'))\n",
    "                time.sleep(sleep)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(tts_, e_, t_, S, A, R, done, eps, model, None, trace)\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if frames is not None and tts_ >= frames:\n",
    "                return rendered_frames\n",
    "                \n",
    "            S = S_\n",
    "                \n",
    "            tts_ += 1\n",
    "            \n",
    "        if episodes is not None and e_ >= episodes-1:\n",
    "            return rendered_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_frames = frames_render(env, frames=None, episodes=1, eps=0.0, model=model, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "fig = plt.figure(figsize=(rendered_frames[0].shape[1] / 72.0, rendered_frames[0].shape[0] / 72.0), dpi = 72)\n",
    "ax = fig.add_subplot(111);\n",
    "\n",
    "patch = ax.imshow(rendered_frames[0])\n",
    "# plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(i):\n",
    "    patch.set_data(rendered_frames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(fig, animate, frames=len(rendered_frames), interval=20, repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
