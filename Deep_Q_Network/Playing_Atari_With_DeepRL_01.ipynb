{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "$$ \\huge{\\underline{\\textbf{ Playing Atari Games with Deep RL }}} $$\n",
    "\n",
    "$$ \\large{\\textbf{MountainCar + DQN + Memory Reply}} $$\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, frames, gamma, eps_decay_steps, eps_target,\n",
    "               batch_size, model, mem, callback=None, trace=None):\n",
    "    \"\"\"Episodic Semi-Gradient Sarsa\n",
    "    \n",
    "    Params:\n",
    "        env - environment\n",
    "        ep - number of episodes to run\n",
    "        gamma - discount factor [0..1]\n",
    "        eps - epsilon-greedy param\n",
    "        model      - function approximator, already initialised, with methods:\n",
    "                     eval(state, action) -> float\n",
    "                     train(state, target) -> None\n",
    "    \"\"\"\n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            q_values = model.eval(np.array([st]))\n",
    "            return argmax_rand(q_values)\n",
    "        else:\n",
    "            return np.random.choice(env.act_space)\n",
    "    \n",
    "    if eps_decay_steps is not None:\n",
    "        eps_delta = (1-eps_target) / eps_decay_steps\n",
    "        eps = 1\n",
    "    else:\n",
    "        eps = eps_target\n",
    "        \n",
    "    # Fill memory buffer using random policy\n",
    "    while len(mem) < mem.max_len:\n",
    "        S = env.reset();\n",
    "        while True:\n",
    "            A = policy(S, model, eps=1.0)  # random policy\n",
    "            S_, R, done = env.step(A)\n",
    "            mem.append(S, A, R, S_, done)\n",
    "            if done:\n",
    "                break\n",
    "            S = S_\n",
    "\n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done = env.step(A)\n",
    "            \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(e_, t_, S, A, R, done, eps, model, mem, trace)\n",
    "            \n",
    "            states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "            targets = model.eval(n_states)\n",
    "            targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "            targets[dones] = rewards[dones]                # return of next-to-terminal state is just R\n",
    "            model.train(states, actions, targets)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if tts_ >= frames:\n",
    "                return\n",
    "                \n",
    "            S = S_\n",
    "            \n",
    "            if eps > eps_target:\n",
    "                eps = max(eps - eps_delta, eps_target)\n",
    "                \n",
    "            tts_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_car(env, ep, model, callback=None, trace=None):\n",
    "\n",
    "    def policy(st, model):\n",
    "        q_values = model.eval([st])\n",
    "        return argmax_rand(q_values)\n",
    "    \n",
    "    for e_ in range(ep):\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        \n",
    "        for t_ in range(1000):       \n",
    "        \n",
    "            A = policy(S, model)\n",
    "            S_, R, done = env.step(A)\n",
    "            \n",
    "            if callback is not None:\n",
    "                if t_ == 999:  done=True\n",
    "                callback(e_, t_, S, A, R, done, 0, model, None, trace)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            S = S_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tile Coding - see [chapter 9.5](0905b_LM_Agg_Tile.ipynb) for introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_rand(arr):\n",
    "    # break ties randomly, np.argmax() always picks first max\n",
    "    return np.random.choice(np.flatnonzero(arr == np.max(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, n):\n",
    "    # res = []\n",
    "    # for i in range(len(x)):\n",
    "    #     res.append( sum(x[max(i-n+1, 0): i+1])   /   min(i+1, n) )\n",
    "        \n",
    "    return [sum(x[max(i-n+1, 0): i+1])   /   min(i+1, n) for i in range(len(x))]\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports (source file: [tiles3.py](tiles3.py), [helpers_1001.py](helpers_1001.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib.colors import ListedColormap\n",
    "import itertools\n",
    "import collections\n",
    "from mountain_car import MountainCarEnv\n",
    "import tiles3           # by Richard Sutton, http://incompleteideas.net/tiles/tiles3.html\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MountainCarEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendulumEnv():\n",
    "    def __init__(self):\n",
    "        self._env = gym.make('Pendulum-v0')\n",
    "        self.state_low = np.array([-np.pi, -8.0])\n",
    "        self.state_high = np.array([np.pi, 8.0])\n",
    "        self.act_space = np.array([0, 1, 2])\n",
    "        print(self._env)\n",
    "        \n",
    "    def reset(self):\n",
    "        sin, cos, vel = self._env.reset()\n",
    "        theta = np.arctan2(cos, sin)\n",
    "        return np.array([theta, vel])\n",
    "        \n",
    "    def step(self, action):\n",
    "        # torques = [-2.0, 0.0, 2.0]\n",
    "        torques = [-0.1, 0.0, 0.1]\n",
    "        joint_effort = torques[action]\n",
    "        \n",
    "        obs, rew, done, _ = self._env.step([joint_effort])\n",
    "        sin, cos, vel = obs\n",
    "        theta = np.arctan2(cos, sin)\n",
    "        return np.array([theta, vel]), rew, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PendulumEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need callback to capture q-value array for whole state-action space at specified episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trace():\n",
    "    def __init__(self):\n",
    "        self.total_tstep = 0\n",
    "        \n",
    "        self.q_values = collections.OrderedDict()\n",
    "        self.ep_end_idx = collections.OrderedDict()\n",
    "        self.ep_rewards = collections.defaultdict(float)\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []  # t+1\n",
    "        self.dones = []    # t+1\n",
    "        self.epsilons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    \"\"\"Called from gradient_MC after every episode.\n",
    "    \n",
    "    Params:\n",
    "        episode [int] - episode number\n",
    "        tstep [int]   - timestep within episode\n",
    "        model [obj]   - function approximator\n",
    "        trace [list]  - list to write results to\"\"\"\n",
    "        \n",
    "    if done_:\n",
    "        trace.ep_end_idx[episode] = trace.total_tstep\n",
    "    \n",
    "    trace.states.append(st)\n",
    "    trace.actions.append(act)\n",
    "    trace.rewards.append(rew_)\n",
    "    trace.dones.append(done_)\n",
    "    trace.epsilons.append(eps)\n",
    "    \n",
    "    trace.ep_rewards[episode] += rew_\n",
    "        \n",
    "    if trace.total_tstep % 1000 == 0:\n",
    "        print('-'*80)\n",
    "        print('episode:', episode, '\\t time step:', tstep,\n",
    "              '\\t total time step:', trace.total_tstep, '\\t eps:', round(eps,3))\n",
    "        \n",
    "    if trace.total_tstep % 1000 == 0:\n",
    "        q_arr = eval_state_action_space(model, env)\n",
    "        trace.q_values[trace.total_tstep] = q_arr\n",
    "        \n",
    "        plot_mountain_car(env, episode, trace.total_tstep, 1000, trace, memory)\n",
    "        \n",
    "    trace.total_tstep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve with Tiles (batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_tiles_single(frames, trace=None):\n",
    "    \n",
    "    model = TileCodingFuncApprox(env.state_low, env.state_high, env.act_space,\n",
    "                                 learn_rate=0.3, num_tilings=8, init_val=0)\n",
    "    \n",
    "    mem = Memory(max_len=1, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    q_learning(env, frames=frames, gamma=1.0, eps_random_steps=0, eps_decay_steps=None, eps_target=0.0, batch_size=1,\n",
    "               model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_t_s_1 = experiment_tiles_single(frames=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve with Tiles (batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_tiles_batch(frames):\n",
    "    \n",
    "    trace = Trace()\n",
    "    \n",
    "    model = TileCodingFuncApprox(env.state_low, env.state_high, env.act_space,\n",
    "                                 learn_rate=0.3, num_tilings=8, init_val=0)\n",
    "    \n",
    "    mem = Memory(max_len=1000, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    q_learning(env, frames=frames, gamma=1.0, eps_decay_steps=None, eps_target=0.0, batch_size=64,\n",
    "               model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trace_t_b_1 = experiment_tiles_batch(frames=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve with Neural Net (batch=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_keras(frames):\n",
    "    \n",
    "    keras_model = tf.keras.models.Sequential()\n",
    "    keras_model.add(tf.keras.layers.Dense(256, 'relu', input_dim=2))\n",
    "    keras_model.add(tf.keras.layers.Dense(256, 'relu'))\n",
    "    keras_model.add(tf.keras.layers.Dense(3, 'linear'))\n",
    "    keras_model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(lr=0.00025))\n",
    "    \n",
    "    model = KerasFunctApprox(keras_model, env.state_low, env.state_high, env.act_space)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace()\n",
    "    q_learning(env, frames=frames, gamma=.99, eps_decay_steps=50000, eps_target=0.1, batch_size=4096,\n",
    "               model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trace_ker_1 = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ker_2 = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ker_3 = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ker_4 = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ker_5 = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ker_6 = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ker_7 = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ker_8 = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ker_9 = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ker_10 = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mountain_car(env, episode, total_tstep, steps_to_plot, trace, mem):\n",
    "    \n",
    "    q_arr = trace.q_values[total_tstep]\n",
    "    states = trace.states[-steps_to_plot:]\n",
    "    actions = trace.actions[-steps_to_plot:]\n",
    "    \n",
    "    epsilons = trace.epsilons\n",
    "    \n",
    "    fig = plt.figure(figsize=[12,8])\n",
    "\n",
    "    if q_arr is not None:\n",
    "        ax = fig.add_subplot(231, projection='3d')\n",
    "        plot_q_max_3d(q_arr, env, title='Q_Max', \n",
    "                      labels=['Position', 'Velocity', ''], alpha=.4, axis=ax)\n",
    "\n",
    "    ax = fig.add_subplot(232)\n",
    "    plot_trajectory(states, actions, env, title='Trajectory', labels=['Position', 'Velocity'], axis=ax)\n",
    "    \n",
    "    ax = fig.add_subplot(233)\n",
    "    # ax.plot(epsilons)\n",
    "    \n",
    "    if q_arr is not None:\n",
    "        ax = fig.add_subplot(234)\n",
    "        plot_policy(q_arr, env, labels=['Position', 'Velocity'],\n",
    "                    colors=['red', 'blue','green'], collab=['left', 'idle', 'right'], axis=ax)\n",
    "        \n",
    "    ax = fig.add_subplot(235)\n",
    "    st, act, rew_1, st_1, dones_1, _ = mem.pick_last(len(mem))\n",
    "    plot_trajectory(st, act, env, title='Memory Buffer', labels=['Position', 'Velocity'], alpha=0.1, axis=ax)\n",
    "    \n",
    "    ax = fig.add_subplot(236)\n",
    "    plot_episode_rewards(trace.ep_end_idx, trace.ep_rewards, ax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_rewards(ep_end_dict, rew_dict, axis=None):\n",
    "    \n",
    "    tsteps = []    # episodes end tsteps\n",
    "    rewards = []   # episodes rewards\n",
    "    \n",
    "    for ep, end_tstep in ep_end_dict.items():\n",
    "        tsteps.append(end_tstep)\n",
    "        rewards.append(rew_dict[ep])\n",
    "    \n",
    "    rewards_avg = running_mean(rewards, 10)\n",
    "    \n",
    "    if axis is None:\n",
    "        fig = plt.figure()\n",
    "        axis = fig.add_subplot(111)\n",
    "    \n",
    "    if len(tsteps) > 0:\n",
    "        axis.scatter(tsteps, rewards, alpha=0.5, marker='.')\n",
    "        axis.plot(tsteps, rewards_avg, alpha=1)\n",
    "        axis.plot([0, tsteps[-1]], [-200, -200], color='gray', linestyle='--')\n",
    "    axis.set_xlabel('Time Step')\n",
    "    axis.set_ylabel('Episode Reward')\n",
    "    axis.set_title('Episode Rewards')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(states, actions, env, title, labels, alpha=1.0, axis=None):\n",
    "    if not isinstance(states, np.ndarray): states = np.array(states)\n",
    "    if not isinstance(actions, np.ndarray): actions = np.array(actions)\n",
    "    \n",
    "    if axis is None:\n",
    "        fig = plt.figure()\n",
    "        axis = fig.add_subplot(111)\n",
    "    \n",
    "    if len(states) == 0:\n",
    "        axis.scatter(np.array([]), np.array([]))\n",
    "    else:\n",
    "        axis.scatter(states[actions==0,0], states[actions==0,1], marker='.', s=1, color='red', alpha=alpha)\n",
    "        axis.scatter(states[actions==1,0], states[actions==1,1], marker='.', s=1, color='blue', alpha=alpha)\n",
    "        axis.scatter(states[actions==2,0], states[actions==2,1], marker='.', s=1, color='green', alpha=alpha)\n",
    "        \n",
    "    x_min, x_max = env.state_low[0], env.state_high[0]\n",
    "    y_min, y_max = env.state_low[1], env.state_high[1]\n",
    "    axis.set_xticks([x_min, x_max])\n",
    "    #axis.set_xticklabels([x_min,x_max])\n",
    "    axis.set_yticks([y_min, y_max])\n",
    "    #axis.set_yticklabels([y_min,y_max])\n",
    "    \n",
    "    axis.set_xlabel(labels[0])\n",
    "    axis.set_ylabel(labels[1])\n",
    "    axis.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(q_arr, env, labels, colors, collab, axis=None):\n",
    "    q_pol = np.argmax(q_arr, axis=-1)\n",
    "    \n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    if axis is None:\n",
    "        fig = plt.figure()\n",
    "        axis = fig.add_subplot(111)\n",
    "        \n",
    "    heatmap = axis.pcolormesh(q_pol.T, cmap=cmap)\n",
    "    axis.set_aspect('equal', 'datalim')\n",
    "    cbar = plt.colorbar(heatmap)\n",
    "    cbar.set_ticks(range(len(collab)))\n",
    "    cbar.set_ticklabels(collab)\n",
    "    \n",
    "    x_min, x_max = env.state_low[0], env.state_high[0]\n",
    "    y_min, y_max = env.state_low[1], env.state_high[1]\n",
    "    axis.set_xticks([0, q_arr.shape[0]])\n",
    "    axis.set_xticklabels([x_min,x_max])\n",
    "    axis.set_yticks([0, q_arr.shape[1]])\n",
    "    axis.set_yticklabels([y_min,y_max])\n",
    "    \n",
    "    axis.set_xlabel(labels[0])\n",
    "    axis.set_ylabel(labels[1])\n",
    "    axis.set_title('Policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_max_3d(q_arr, env, color='#1f77b4', alpha=1.,\n",
    "                  title='', labels=['x','y','z'], axis=None):\n",
    "    \"\"\"Plot 3D wireframe\n",
    "    \n",
    "    Params:\n",
    "        q_arr     - 2d array with dim: [state_x, state_y]\n",
    "        env       - environment with members:\n",
    "                      st_low - state space low boundry e.g. [-1.2, -0.07]\n",
    "                      st_high - state space high boundry\n",
    "        color     - plot color\n",
    "        alpha     - plot transparency\n",
    "        labels    - string array [label_x, label_y, label_z], len=3, empty str to omit\n",
    "        axis      - axis to plot to, if None create new figure\n",
    "    \"\"\"\n",
    "    q_max = np.max(q_arr, axis=-1)  # calc max and inverse\n",
    "    \n",
    "    x_min, x_max = env.state_low[0], env.state_high[0]\n",
    "    y_min, y_max = env.state_low[1], env.state_high[1]\n",
    "    x_space = np.linspace(x_min, x_max, num=q_max.shape[0])\n",
    "    y_space = np.linspace(y_min, y_max, num=q_max.shape[1])\n",
    "    Y, X = np.meshgrid(y_space, x_space)\n",
    "    \n",
    "    if axis is None:\n",
    "        fig = plt.figure()\n",
    "        axis = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    axis.plot_wireframe(X, Y, q_max, color=color, alpha=alpha)\n",
    "    axis.set_xlabel(labels[0])\n",
    "    axis.set_ylabel(labels[1])\n",
    "    axis.set_zlabel(labels[2])\n",
    "    axis.set_xticks([x_min, x_max])\n",
    "    axis.set_yticks([y_min, y_max])\n",
    "    axis.set_title(title)\n",
    "    \n",
    "    axis.view_init(40, -70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_state_action_space(model, env, split=[32,32]):\n",
    "    \"\"\"Evaluate 2d Q-function on area and return as 3d array\n",
    "    \n",
    "    Params:\n",
    "        model     - function approximator with method: model.eval(state, action) -> float\n",
    "        env       - environment with members:\n",
    "                      st_low - state space low boundry e.g. [-1.2, -0.07]\n",
    "                      st_high - state space high boundry\n",
    "                      act_space - action space, e.g. [0, 1, 2]\n",
    "        split     - number of data points in each dimensions, e.g. [20, 20]\n",
    "    \"\"\"\n",
    "    x_min, x_max = env.state_low[0], env.state_high[0]\n",
    "    y_min, y_max = env.state_low[1], env.state_high[1]\n",
    "    x_split, y_split = split\n",
    "    \n",
    "    q_arr = np.zeros([x_split, y_split, len(env.act_space)])\n",
    "\n",
    "    for pi, pos in enumerate(np.linspace(x_min, x_max, x_split)):\n",
    "        for vi, vel in enumerate(np.linspace(y_min, y_max, y_split)):\n",
    "            q_values = model.eval(states=np.array([[pos, vel]]))[0]\n",
    "            for act in env.act_space:\n",
    "                q_arr[pi, vi, act] = q_values[act]\n",
    "                \n",
    "    return q_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximators and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TileCodingFuncApprox():\n",
    "    def __init__(self, st_low, st_high, action_space, learn_rate, num_tilings, init_val):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            st_low       - state space low boundry, e.g. [-1.2, -0.07] for mountain car\n",
    "            st_high      - state space high boundry in all dimensions\n",
    "            action_space - list of possible actions\n",
    "            learn_rate   - step size, will be adjusted for nb_tilings automatically\n",
    "            num_tilings  - tiling layers - should be power of 2 and at least 4*len(st_low)\n",
    "            init_val     - initial state-action values\n",
    "        \"\"\"\n",
    "        assert len(st_low) == len(st_high)\n",
    "        self._n_dim = len(st_low)\n",
    "        self._act_space = action_space\n",
    "        self._lr = learn_rate / num_tilings\n",
    "        self._num_tilings = num_tilings\n",
    "        self._scales = self._num_tilings / (st_high - st_low)\n",
    "        \n",
    "        # e.g. 8 tilings, 2d space, 3 actions\n",
    "        # nb_total_tiles = (8+1) * (8+1) * 8 * 3\n",
    "        nb_total_tiles = (num_tilings+1)**self._n_dim * num_tilings * len(action_space)\n",
    "                \n",
    "        self._iht = tiles3.IHT(nb_total_tiles)\n",
    "        self._weights = np.zeros(nb_total_tiles) + init_val / num_tilings\n",
    "        \n",
    "    def eval(self, states):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "        \n",
    "        all_q_values = []\n",
    "        for state in states:\n",
    "            assert len(state) == self._n_dim\n",
    "            scaled_state = np.multiply(self._scales, state)  # scale state to map to tiles correctly\n",
    "            q_values = []\n",
    "            for action in self._act_space:\n",
    "                active_tiles = tiles3.tiles(                 # find active tiles\n",
    "                    self._iht, self._num_tilings,\n",
    "                    scaled_state, [action])\n",
    "                q_val = np.sum(self._weights[active_tiles])  # pick correct weights and sum up\n",
    "                q_values.append(q_val)                       # store result for this action\n",
    "            all_q_values.append(q_values)\n",
    "        return np.array(all_q_values)\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert isinstance(actions, np.ndarray)\n",
    "        assert isinstance(targets, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "        assert actions.ndim == 1\n",
    "        assert targets.ndim == 1\n",
    "        assert len(states) == len(actions) == len(targets)\n",
    "        \n",
    "        for i in range(len(states)):\n",
    "            state = states[i]\n",
    "            action = actions[i]\n",
    "            target = targets[i]\n",
    "            \n",
    "            assert len(state) == self._n_dim\n",
    "            assert np.isscalar(action)\n",
    "            assert np.isscalar(target)\n",
    "            \n",
    "            scaled_state = np.multiply(self._scales, state)  # scale state to map to tiles correctly\n",
    "            active_tiles = tiles3.tiles(                     # find active tiles\n",
    "                self._iht, self._num_tilings,\n",
    "                scaled_state, [action])\n",
    "            value = np.sum(self._weights[active_tiles])      # q-value for state-action pair\n",
    "            delta = self._lr * (target - value)              # grad is [0,1,0,0,..]\n",
    "            self._weights[active_tiles] += delta             # ..so we pick active weights instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasFunctApprox():\n",
    "\n",
    "    def __init__(self, model, st_low, st_high, action_space):\n",
    "        \"\"\"Q-function approximator using Keras model\n",
    "\n",
    "        Args:\n",
    "            model: Keras compiled model\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "    \n",
    "        first_layer = self._model.layers[0]\n",
    "        nn_input_shape = first_layer.input_shape[1:]\n",
    "        if st_low.shape != nn_input_shape:\n",
    "            raise ValueError('Input shape does not match state_space shape')\n",
    "\n",
    "        last_layer = self._model.layers[-1]\n",
    "        nn_output_shape = last_layer.output_shape[1:]\n",
    "        if action_space.shape != nn_output_shape:\n",
    "            raise ValueError('Output shape does not match action_space shape')\n",
    "\n",
    "        # normalise inputs\n",
    "        self._offsets = st_low + (st_high - st_low) / 2\n",
    "        self._scales = 1 / ((st_high - st_low) / 2)\n",
    "\n",
    "    def eval(self, states):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "\n",
    "        return self._model.predict(inputs, batch_size=len(inputs))\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert isinstance(actions, np.ndarray)\n",
    "        assert isinstance(targets, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "        assert actions.ndim == 1\n",
    "        assert targets.ndim == 1\n",
    "        assert len(states) == len(actions) == len(targets)\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "        all_targets = self._model.predict(inputs, batch_size=len(inputs))\n",
    "        all_targets[np.arange(len(all_targets)), actions] = targets\n",
    "        self._model.fit(inputs, all_targets, batch_size=len(inputs), epochs=1, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Circular buffer for DQN memory reply. Fairly fast.\"\"\"\n",
    "\n",
    "    def __init__(self, max_len, state_shape, state_dtype):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_len: maximum capacity\n",
    "        \"\"\"\n",
    "        assert isinstance(max_len, int)\n",
    "        assert max_len > 0\n",
    "\n",
    "        self.max_len = max_len                            # maximum length        \n",
    "        self._curr_insert_ptr = 0                          # index to insert next data sample\n",
    "        self._curr_len = 0                                 # number of currently stored elements\n",
    "\n",
    "        state_arr_shape = [max_len] + list(state_shape)\n",
    "\n",
    "        self._hist_St = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_At = np.zeros(max_len, dtype=int)\n",
    "        self._hist_Rt_1 = np.zeros(max_len, dtype=float)\n",
    "        self._hist_St_1 = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_done_1 = np.zeros(max_len, dtype=bool)\n",
    "\n",
    "    def append(self, St, At, Rt_1, St_1, done_1):\n",
    "        \"\"\"Add one sample to memory, override oldest if max_len reached.\n",
    "\n",
    "        Args:\n",
    "            St [np.ndarray]   - state\n",
    "            At [int]          - action\n",
    "            Rt_1 [float]      - reward\n",
    "            St_1 [np.ndarray] - next state\n",
    "            done_1 [bool]       - next state terminal?\n",
    "        \"\"\"\n",
    "        self._hist_St[self._curr_insert_ptr] = St\n",
    "        self._hist_At[self._curr_insert_ptr] = At\n",
    "        self._hist_Rt_1[self._curr_insert_ptr] = Rt_1\n",
    "        self._hist_St_1[self._curr_insert_ptr] = St_1\n",
    "        self._hist_done_1[self._curr_insert_ptr] = done_1\n",
    "        \n",
    "        if self._curr_len < self.max_len:                 # keep track of current length\n",
    "            self._curr_len += 1\n",
    "            \n",
    "        self._curr_insert_ptr += 1                         # increment insertion pointer\n",
    "        if self._curr_insert_ptr >= self.max_len:         # roll to zero if needed\n",
    "            self._curr_insert_ptr = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples in memory, 0 <= length <= max_len\"\"\"\n",
    "        return self._curr_len\n",
    "\n",
    "    def get_batch(self, batch_len):\n",
    "        \"\"\"Sample batch of data, with repetition\n",
    "\n",
    "        Args:\n",
    "            batch_len: nb of samples to pick\n",
    "\n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, next_done, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert self._curr_len > 0\n",
    "        assert batch_len > 0\n",
    "\n",
    "        \n",
    "        indices = np.random.randint(                   # randint much faster than np.random.sample\n",
    "            low=0, high=self._curr_len, size=batch_len, dtype=int)\n",
    "\n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "\n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "\n",
    "\n",
    "    \n",
    "    def pick_last(self, nb):\n",
    "        \"\"\"Pick last nb elements from memory\n",
    "        \n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, done_1, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert nb <= self._curr_len\n",
    "        \n",
    "        start = self._curr_insert_ptr - nb                # inclusive\n",
    "        end = self._curr_insert_ptr                       # not inclusive\n",
    "        indices = np.array(range(start,end), dtype=int)   # indices to pick, can be negative\n",
    "        indices[indices < 0] += self._curr_len            # loop negative to positive\n",
    "        \n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
