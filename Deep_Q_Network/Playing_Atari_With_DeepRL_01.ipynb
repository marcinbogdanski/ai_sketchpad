{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "$$ \\huge{\\underline{\\textbf{ Playing Atari Games with Deep RL }}} $$\n",
    "\n",
    "$$ \\large{\\textbf{MountainCar + DQN + Memory Reply}} $$\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, frames, gamma, eps_decay_steps, eps_target, action_repeat,\n",
    "               batch_size, model, mem, callback=None, trace=None):\n",
    "    \"\"\"Episodic Semi-Gradient Sarsa\n",
    "    \n",
    "    Params:\n",
    "        env - environment\n",
    "        ep - number of episodes to run\n",
    "        gamma - discount factor [0..1]\n",
    "        eps - epsilon-greedy param\n",
    "        model      - function approximator, already initialised, with methods:\n",
    "                     eval(state, action) -> float\n",
    "                     train(state, target) -> None\n",
    "    \"\"\"\n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            q_values = model.eval(np.array([st]))\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "    \n",
    "    if eps_decay_steps is not None:\n",
    "        eps_delta = (1-eps_target) / eps_decay_steps\n",
    "        eps = 1\n",
    "    else:\n",
    "        eps = eps_target\n",
    "        \n",
    "    # Fill memory buffer using random policy\n",
    "    while len(mem) < mem.max_len:\n",
    "        S = env.reset();\n",
    "        for t_ in itertools.count():\n",
    "            if t_ % action_repeat == 0:\n",
    "                A = env.action_space.sample()    # random policy\n",
    "            S_, R, done, _ = env.step(A)\n",
    "            mem.append(S, A, R, S_, done)\n",
    "            if done:\n",
    "                break\n",
    "            S = S_\n",
    "\n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            if t_ % action_repeat == 0:\n",
    "                A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(e_, t_, S, A, R, done, eps, model, mem, trace)\n",
    "            \n",
    "            states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "            targets = model.eval(n_states)\n",
    "            targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "            targets[dones] = rewards[dones]                # return of next-to-terminal state is just R\n",
    "            model.train(states, actions, targets)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if tts_ >= frames:\n",
    "                return\n",
    "                \n",
    "            S = S_\n",
    "            \n",
    "            if eps > eps_target:\n",
    "                eps = max(eps - eps_delta, eps_target)\n",
    "                \n",
    "            tts_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_car(env, ep, model, callback=None, trace=None):\n",
    "\n",
    "#     def policy(st, model):\n",
    "#         q_values = model.eval([st])\n",
    "#         return argmax_rand(q_values)\n",
    "    \n",
    "#     for e_ in range(ep):\n",
    "        \n",
    "#         S = env.reset()\n",
    "        \n",
    "        \n",
    "#         for t_ in range(1000):       \n",
    "        \n",
    "#             A = policy(S, model)\n",
    "#             S_, R, done, _ = env.step(A)\n",
    "            \n",
    "#             if callback is not None:\n",
    "#                 if t_ == 999:  done=True\n",
    "#                 callback(e_, t_, S, A, R, done, 0, model, None, trace)\n",
    "            \n",
    "#             if done:\n",
    "#                 break\n",
    "                \n",
    "#             S = S_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tile Coding - see [chapter 9.5](0905b_LM_Agg_Tile.ipynb) for introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports (source file: [tiles3.py](tiles3.py), [helpers_1001.py](helpers_1001.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendulumEnv():\n",
    "    def __init__(self):\n",
    "        self._env = gym.make('Pendulum-v0')\n",
    "        self.observation_space = helpers.BoxSpace(\n",
    "            shape=(2,), low=[-np.pi, -8.0], high=[np.pi, 8.0]\n",
    "        )\n",
    "        self.action_space = helpers.DiscreteSpace(n=3)\n",
    "        \n",
    "    def reset(self):\n",
    "        cos, sin, vel = self._env.reset()\n",
    "        theta = np.arctan2(sin, cos)\n",
    "        return np.array([theta, vel])\n",
    "        \n",
    "    def step(self, action):\n",
    "        torques = [-2.0, 0.0, 2.0]\n",
    "        # torques = [-2.0, -.5, 0.0, .5, 2.0]\n",
    "        joint_effort = torques[action]\n",
    "        \n",
    "        obs, rew, done, _ = self._env.step([joint_effort])\n",
    "        cos, sin, vel = obs\n",
    "        theta = np.arctan2(sin, cos)\n",
    "        return np.array([theta, vel]), rew, done, obs\n",
    "    \n",
    "    def render(self):\n",
    "        self._env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need callback to capture q-value array for whole state-action space at specified episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trace():\n",
    "    def __init__(self, eval_every):\n",
    "        self.eval_every = eval_every\n",
    "        \n",
    "        self.total_tstep = 0\n",
    "        \n",
    "        self.q_values = collections.OrderedDict()\n",
    "        self.ep_end_idx = collections.OrderedDict()\n",
    "        self.ep_rewards = collections.defaultdict(float)\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []  # t+1\n",
    "        self.dones = []    # t+1\n",
    "        self.epsilons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(episode, tstep, st, act, rew_, done_, eps, model, memory, trace):\n",
    "    \"\"\"Called from gradient_MC after every episode.\n",
    "    \n",
    "    Params:\n",
    "        episode [int] - episode number\n",
    "        tstep [int]   - timestep within episode\n",
    "        model [obj]   - function approximator\n",
    "        trace [list]  - list to write results to\"\"\"\n",
    "    \n",
    "    if trace.total_tstep == 0:\n",
    "        print('num exits in memory:', np.count_nonzero(memory._hist_done_1))\n",
    "        \n",
    "    if done_:\n",
    "        trace.ep_end_idx[episode] = trace.total_tstep\n",
    "    \n",
    "    trace.states.append(st)\n",
    "    trace.actions.append(act)\n",
    "    trace.rewards.append(rew_)\n",
    "    trace.dones.append(done_)\n",
    "    trace.epsilons.append(eps)\n",
    "    \n",
    "    trace.ep_rewards[episode] += rew_\n",
    "        \n",
    "    if trace.total_tstep % trace.eval_every == 0:\n",
    "        print('-'*80)\n",
    "        print('episode:', episode, '\\t time step:', tstep,\n",
    "              '\\t total time step:', trace.total_tstep, '\\t eps:', round(eps,3))\n",
    "        \n",
    "    if trace.total_tstep % trace.eval_every == 0:\n",
    "        q_arr = helpers.eval_state_action_space(model, env, split=[128,128])\n",
    "        trace.q_values[trace.total_tstep] = q_arr\n",
    "        \n",
    "        helpers.plot_mountain_car(env, episode, trace.total_tstep, 1000, trace, memory,\n",
    "                                 axis_labels=['Position', 'Velocity'],\n",
    "                                 action_labels=['left', 'idle', 'right'],\n",
    "                                 action_colors=['red', 'gray', 'green'])\n",
    "        \n",
    "    trace.total_tstep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve with Neural Net (batch=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_keras(frames):\n",
    "    \n",
    "    keras_model = tf.keras.models.Sequential()\n",
    "    keras_model.add(tf.keras.layers.Dense(256, 'relu', input_dim=2))\n",
    "    keras_model.add(tf.keras.layers.Dense(256, 'relu'))\n",
    "    keras_model.add(tf.keras.layers.Dense(3, 'linear'))\n",
    "    keras_model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(lr=0.00025))\n",
    "    \n",
    "    model = KerasFunctApprox(keras_model, env.observation_space.low, env.observation_space.high, env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    q_learning(env, frames=frames, gamma=.99, eps_decay_steps=50000, eps_target=0.1, action_repeat=4,\n",
    "               batch_size=4096, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0').env\n",
    "trace_nn = experiment_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve with Tiles (batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_tiles_batch(frames):\n",
    "    \n",
    "    model = TileCodingFuncApprox(env.observation_space.low, env.observation_space.high, env.action_space.n,\n",
    "                                 learn_rate=0.3, num_tilings=8, init_val=0)\n",
    "    \n",
    "    mem = Memory(max_len=1000, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    \n",
    "    q_learning(env, frames=frames, gamma=1.0, eps_decay_steps=None, eps_target=0.0, batch_size=64,\n",
    "               model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trace_t_b_1 = experiment_tiles_batch(frames=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve with Tiles (batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_tiles_single(frames):\n",
    "    \n",
    "    model = helpers.TileCodingFuncApprox(env.observation_space.low, env.observation_space.high, env.action_space.n,\n",
    "                                 learn_rate=0.3, num_tilings=8, init_val=0)\n",
    "    \n",
    "    mem = Memory(max_len=1, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    q_learning(env, frames=frames, gamma=1.0, eps_decay_steps=None, eps_target=0.0, action_repeat=4,\n",
    "               batch_size=1, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trace_t_s_1 = experiment_tiles_single(frames=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve Pendulum with Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pendulum_keras(frames):\n",
    "    \n",
    "    keras_model = tf.keras.models.Sequential()\n",
    "    keras_model.add(tf.keras.layers.Dense(256, 'relu', input_dim=2))\n",
    "    keras_model.add(tf.keras.layers.Dense(256, 'relu'))\n",
    "    keras_model.add(tf.keras.layers.Dense(3, 'linear'))\n",
    "    keras_model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(lr=0.00025))\n",
    "    \n",
    "    model = KerasFunctApprox(keras_model, env.observation_space.low, env.observation_space.high, env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=1000)\n",
    "    q_learning(env, frames=frames, gamma=.99, eps_decay_steps=50000, eps_target=0.1, action_repeat=1,\n",
    "               batch_size=4096, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = PendulumEnv()\n",
    "trace_nn = experiment_pendulum_keras(frames=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve Pendulum with Tiles (batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pendulum_tiles_single(frames):\n",
    "    \n",
    "    model = helpers.TileCodingFuncApprox(env.observation_space.low, env.observation_space.high, env.action_space.n,\n",
    "                                 learn_rate=0.3, num_tilings=32, init_val=0)\n",
    "    \n",
    "    mem = Memory(max_len=1, state_shape=(2,), state_dtype=float)\n",
    "    \n",
    "    trace = Trace(eval_every=25000)\n",
    "    q_learning(env, frames=frames, gamma=1.0, eps_decay_steps=None, eps_target=0.0, action_repeat=1,\n",
    "               batch_size=1, model=model, mem=mem, callback=callback, trace=trace)\n",
    "    \n",
    "    #test_car(env, ep=20, model=model, callback=callback, trace=trace)\n",
    "    print()\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = PendulumEnv()\n",
    "experiment_pendulum_tiles_single(frames=100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test trajectory plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PendulumEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_lengths = []\n",
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "observations = []\n",
    "# env = gym.make('MountainCar-v0').env\n",
    "# env = gym.make('Pendulum-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e_ in range(1):\n",
    "    S = env.reset();\n",
    "    for t_ in itertools.count():           # count to infinity\n",
    "        if t_ % 4 == 0:\n",
    "            A = 1 # env.action_space.sample()     # random action\n",
    "        S, R, done, O = env.step(A)\n",
    "        \n",
    "        # env.render()\n",
    "        \n",
    "        # print(env.env.state)\n",
    "        \n",
    "        states.append(S)\n",
    "        actions.append(A)\n",
    "        rewards.append(R)\n",
    "        observations.append(O)\n",
    "        \n",
    "        # time.sleep(1)\n",
    "        \n",
    "        # if t_ > 3:\n",
    "        #     break\n",
    "        \n",
    "        if done:\n",
    "            episode_lengths.append(t_)\n",
    "            break\n",
    "    if e_ % 10 == 0:\n",
    "        print(',', end='')\n",
    "    else:\n",
    "        print('.', end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test average episode length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_lengths = []\n",
    "states = []\n",
    "actions = []\n",
    "env = gym.make('MountainCar-v0').env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    S = env.reset();\n",
    "    for j in itertools.count():           # count to infinity\n",
    "        if j % 4 == 0:\n",
    "            A = env.action_space.sample()     # random action\n",
    "        S, _, done, _ = env.step(A)        \n",
    "        states.append(S)\n",
    "        actions.append(A)\n",
    "        if done:\n",
    "            episode_lengths.append(j)\n",
    "            break\n",
    "    if i % 10 == 0:\n",
    "        print(',', end='')\n",
    "    else:\n",
    "        print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_lengths)\n",
    "plt.plot(helpers.running_mean(episode_lengths, 1000))\n",
    "plt.grid()\n",
    "plt.title('OpenAI Gym Mountain Car, A*4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_lengths)\n",
    "plt.plot(running_mean(episode_lengths, 1000))\n",
    "plt.grid()\n",
    "plt.title('OpenAI Gym Mountain Car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximators and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasFunctApprox():\n",
    "\n",
    "    def __init__(self, model, st_low, st_high, nb_actions):\n",
    "        \"\"\"Q-function approximator using Keras model\n",
    "\n",
    "        Args:\n",
    "            model: Keras compiled model\n",
    "        \"\"\"\n",
    "        st_low = np.array(st_low);    st_high = np.array(st_high)\n",
    "        self._model = model\n",
    "    \n",
    "        first_layer = self._model.layers[0]\n",
    "        nn_input_shape = first_layer.input_shape[1:]\n",
    "        if st_low.shape != nn_input_shape:\n",
    "            raise ValueError('Input shape does not match state_space shape')\n",
    "\n",
    "        last_layer = self._model.layers[-1]\n",
    "        nn_output_shape = last_layer.output_shape[1:]\n",
    "        if (nb_actions,) != nn_output_shape:\n",
    "            raise ValueError('Output shape does not match action_space shape')\n",
    "\n",
    "        # normalise inputs\n",
    "        self._offsets = st_low + (st_high - st_low) / 2\n",
    "        self._scales = 1 / ((st_high - st_low) / 2)\n",
    "\n",
    "    def eval(self, states):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "\n",
    "        return self._model.predict(inputs, batch_size=len(inputs))\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert isinstance(actions, np.ndarray)\n",
    "        assert isinstance(targets, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "        assert actions.ndim == 1\n",
    "        assert targets.ndim == 1\n",
    "        assert len(states) == len(actions) == len(targets)\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "        all_targets = self._model.predict(inputs, batch_size=len(inputs))\n",
    "        all_targets[np.arange(len(all_targets)), actions] = targets\n",
    "        self._model.fit(inputs, all_targets, batch_size=len(inputs), epochs=1, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Circular buffer for DQN memory reply. Fairly fast.\"\"\"\n",
    "\n",
    "    def __init__(self, max_len, state_shape, state_dtype):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_len: maximum capacity\n",
    "        \"\"\"\n",
    "        assert isinstance(max_len, int)\n",
    "        assert max_len > 0\n",
    "\n",
    "        self.max_len = max_len                            # maximum length        \n",
    "        self._curr_insert_ptr = 0                          # index to insert next data sample\n",
    "        self._curr_len = 0                                 # number of currently stored elements\n",
    "\n",
    "        state_arr_shape = [max_len] + list(state_shape)\n",
    "\n",
    "        self._hist_St = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_At = np.zeros(max_len, dtype=int)\n",
    "        self._hist_Rt_1 = np.zeros(max_len, dtype=float)\n",
    "        self._hist_St_1 = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_done_1 = np.zeros(max_len, dtype=bool)\n",
    "\n",
    "    def append(self, St, At, Rt_1, St_1, done_1):\n",
    "        \"\"\"Add one sample to memory, override oldest if max_len reached.\n",
    "\n",
    "        Args:\n",
    "            St [np.ndarray]   - state\n",
    "            At [int]          - action\n",
    "            Rt_1 [float]      - reward\n",
    "            St_1 [np.ndarray] - next state\n",
    "            done_1 [bool]       - next state terminal?\n",
    "        \"\"\"\n",
    "        self._hist_St[self._curr_insert_ptr] = St\n",
    "        self._hist_At[self._curr_insert_ptr] = At\n",
    "        self._hist_Rt_1[self._curr_insert_ptr] = Rt_1\n",
    "        self._hist_St_1[self._curr_insert_ptr] = St_1\n",
    "        self._hist_done_1[self._curr_insert_ptr] = done_1\n",
    "        \n",
    "        if self._curr_len < self.max_len:                 # keep track of current length\n",
    "            self._curr_len += 1\n",
    "            \n",
    "        self._curr_insert_ptr += 1                         # increment insertion pointer\n",
    "        if self._curr_insert_ptr >= self.max_len:         # roll to zero if needed\n",
    "            self._curr_insert_ptr = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples in memory, 0 <= length <= max_len\"\"\"\n",
    "        return self._curr_len\n",
    "\n",
    "    def get_batch(self, batch_len):\n",
    "        \"\"\"Sample batch of data, with repetition\n",
    "\n",
    "        Args:\n",
    "            batch_len: nb of samples to pick\n",
    "\n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, next_done, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert self._curr_len > 0\n",
    "        assert batch_len > 0\n",
    "\n",
    "        \n",
    "        indices = np.random.randint(                   # randint much faster than np.random.sample\n",
    "            low=0, high=self._curr_len, size=batch_len, dtype=int)\n",
    "\n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "\n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "\n",
    "\n",
    "    \n",
    "    def pick_last(self, nb):\n",
    "        \"\"\"Pick last nb elements from memory\n",
    "        \n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, done_1, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert nb <= self._curr_len\n",
    "        \n",
    "        start = self._curr_insert_ptr - nb                # inclusive\n",
    "        end = self._curr_insert_ptr                       # not inclusive\n",
    "        indices = np.array(range(start,end), dtype=int)   # indices to pick, can be negative\n",
    "        indices[indices < 0] += self._curr_len            # loop negative to positive\n",
    "        \n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
