{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset\n",
    "\n",
    "The CIFAR-10 dataset consists of __60000 32x32__ colour images in __10 classes__, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each: \n",
    "\n",
    "<img src=\"cifar-10-random.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import hashlib\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download approximately 170MB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "fname = 'cifar-10-python.tar.gz'\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "fsum = hashlib.md5(open(fname, 'rb').read()).hexdigest()\n",
    "assert fsum == 'c58f30108f718f92721af3b95e74349a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(tfile, member):\n",
    "    buff = tfile.extractfile(member)\n",
    "    pdata = pickle.load(buff, encoding='latin1')\n",
    "    features = pdata['data']\n",
    "    features = features.reshape([len(pdata['data']), 3, 32, 32])\n",
    "    features = features.transpose(0, 2, 3, 1)\n",
    "    labels = pdata['labels']\n",
    "    return features, labels\n",
    "\n",
    "with tarfile.open(fname, 'r:gz') as tfile:\n",
    "    members = tfile.getmembers()\n",
    "    x1, y1 = extract(tfile, 'cifar-10-batches-py/data_batch_1')\n",
    "    x2, y2 = extract(tfile, 'cifar-10-batches-py/data_batch_2')\n",
    "    x3, y3 = extract(tfile, 'cifar-10-batches-py/data_batch_3')\n",
    "    x4, y4 = extract(tfile, 'cifar-10-batches-py/data_batch_4')\n",
    "    x5, y5 = extract(tfile, 'cifar-10-batches-py/data_batch_5')    \n",
    "    \n",
    "    train_x_raw = np.concatenate([x1, x2, x3, x4, x5])\n",
    "    train_y_raw = np.concatenate([y1, y2, y3, y4, y5])\n",
    "    \n",
    "    test_x_raw, test_y_raw = extract(tfile, 'cifar-10-batches-py/test_batch')\n",
    "    \n",
    "    del x1, x2, x3, x4, x5, y1, y2, y3, y4, y5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_mean = train_x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_std = train_x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = (train_x - train_x_mean) / train_x_std\n",
    "test_x = (test_x - train_x_mean) / train_x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x_raw / 255\n",
    "test_x = test_x_raw / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(data_0123):\n",
    "    N = len(data_0123)\n",
    "    res = np.zeros([N, 10])\n",
    "    res[range(N), data_0123] = 1\n",
    "    return res\n",
    "\n",
    "train_y = onehot(train_y_raw)\n",
    "test_y = onehot(test_y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split - dataset is already shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(len(train_x)*0.1)\n",
    "\n",
    "valid_x = train_x[-idx:]\n",
    "valid_y = train_y[-idx:]\n",
    "\n",
    "train_x = train_x[:-idx]\n",
    "train_y = train_y[:-idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise  # Sentinel\n",
    "with gzip.open('CIFAR-10.gz', 'wb') as gfile:\n",
    "    pickle_dict = {'train_x': train_x, 'train_y': train_y,\n",
    "                   'valid_x': valid_x, 'valid_y': valid_y,\n",
    "                   'test_x': test_x, 'test_y': test_y }\n",
    "    pickle.dump(pickle_dict, gfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess:\n",
    "    devs = sess.list_devices()\n",
    "    print('\\n'.join([x.name for x in devs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('CIFAR-10.gz', 'rb') as gfile:\n",
    "    pickle_dict = pickle.load(gfile)\n",
    "    train_x = pickle_dict['train_x']\n",
    "    train_y = pickle_dict['train_y']\n",
    "    valid_x = pickle_dict['valid_x']\n",
    "    valid_y = pickle_dict['valid_y']\n",
    "    test_x = pickle_dict['test_x']\n",
    "    test_y = pickle_dict['test_y']\n",
    "    del pickle_dict  # free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2txt = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "idx = 43\n",
    "plt.imshow(train_x[idx])\n",
    "print(lab2txt[np.argmax(train_y[idx])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Level Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "xi = tf.placeholder(name='xi', shape=[None, 32, 32, 3], dtype=tf.float32)\n",
    "yi = tf.placeholder(name='yi', shape=[None, 10], dtype=tf.float32)\n",
    "kp = tf.placeholder(name='kp', shape=[], dtype=tf.float32)\n",
    "\n",
    "model = tf.layers.conv2d(xi,\n",
    "    filters=32, kernel_size=[5, 5], strides=[1, 1], padding='SAME', activation=tf.nn.relu, name='Conv_1')\n",
    "model = tf.layers.max_pooling2d(model,\n",
    "    pool_size=[2, 2], strides=[2, 2], padding='SAME', name='Pool_1')\n",
    "\n",
    "model = tf.layers.conv2d(model,\n",
    "    filters=64, kernel_size=[5, 5], strides=[1, 1], padding='SAME', activation=tf.nn.relu, name='Conv_2')\n",
    "model = tf.layers.max_pooling2d(model,\n",
    "    pool_size=[2, 2], strides=[2, 2], padding='SAME', name='Pool_2')\n",
    "\n",
    "model = tf.layers.conv2d(model,\n",
    "    filters=128, kernel_size=[5, 5], strides=[1, 1], padding='SAME', activation=tf.nn.relu, name='Conv_3')\n",
    "model = tf.layers.max_pooling2d(model,\n",
    "    pool_size=[2, 2], strides=[2, 2], padding='SAME', name='Pool_3')\n",
    "\n",
    "model = tf.layers.flatten(model)\n",
    "model = tf.layers.dense(model, 128, activation=tf.nn.relu, name='Dense_1')\n",
    "model = tf.nn.dropout(model, kp)\n",
    "logits = tf.layers.dense(model, 10, activation=None, name='Logits')\n",
    "\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(yi, logits)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(yi, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_inout = {\n",
    "    'batch_x' : xi[0:batch_save,:,:,:],\n",
    "    'batch_y' : yi[0:batch_save,:],\n",
    "    'probabilities' : probabilities[0:batch_save,:]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_conv_1 = {\n",
    "'W': graph.get_tensor_by_name('Conv_1/kernel:0'),\n",
    "'b': graph.get_tensor_by_name('Conv_1/bias:0'),\n",
    "'dW': graph.get_tensor_by_name('gradients/Conv_1/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "'db': graph.get_tensor_by_name('gradients/Conv_1/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "'z': graph.get_tensor_by_name('Conv_1/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_conv_2 = {\n",
    "'W': graph.get_tensor_by_name('Conv_2/kernel:0'),\n",
    "'b': graph.get_tensor_by_name('Conv_2/bias:0'),\n",
    "'dW': graph.get_tensor_by_name('gradients/Conv_2/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "'db': graph.get_tensor_by_name('gradients/Conv_2/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "'z': graph.get_tensor_by_name('Conv_2/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_conv_3 = {\n",
    "'W': graph.get_tensor_by_name('Conv_3/kernel:0'),\n",
    "'b': graph.get_tensor_by_name('Conv_3/bias:0'),\n",
    "'dW': graph.get_tensor_by_name('gradients/Conv_3/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "'db': graph.get_tensor_by_name('gradients/Conv_3/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "'z': graph.get_tensor_by_name('Conv_3/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dense_1 = {\n",
    "'W': graph.get_tensor_by_name('Dense_1/kernel:0'),\n",
    "'b': graph.get_tensor_by_name('Dense_1/bias:0'),\n",
    "'dW': graph.get_tensor_by_name('gradients/Dense_1/MatMul_grad/tuple/control_dependency_1:0'),\n",
    "'db': graph.get_tensor_by_name('gradients/Dense_1/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "'z': graph.get_tensor_by_name('Dense_1/BiasAdd:0')[0:batch_save,:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_logits = {\n",
    "'W': graph.get_tensor_by_name('Logits/kernel:0'),\n",
    "'b': graph.get_tensor_by_name('Logits/bias:0'),\n",
    "'dW': graph.get_tensor_by_name('gradients/Logits/MatMul_grad/tuple/control_dependency_1:0'),\n",
    "'db': graph.get_tensor_by_name('gradients/Logits/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "'z': graph.get_tensor_by_name('Logits/BiasAdd:0')[0:batch_save,:],\n",
    "}\n",
    "\n",
    "dict_metrics = {\n",
    "    'loss': loss,\n",
    "    'accuracy': accuracy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_layers = {\n",
    "    'inout': dict_inout,\n",
    "    'conv_1': dict_conv_1,\n",
    "    'conv_2': dict_conv_2,\n",
    "    'conv_3': dict_conv_3,\n",
    "    'dense_1': dict_dense_1,\n",
    "    'logits': dict_logits,\n",
    "    'metrics': dict_metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_and_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    out2 = sess.run( tf.get_default_graph().get_tensor_by_name('Conv_1/kernel:0') )\n",
    "    out1 = sess.run( tf.trainable_variables()[0] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.alltrue(out1 == out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log(filename, all_layers_dict):\n",
    "    with tables.open_file(filename, mode='w') as f:\n",
    "        for layer_name, layer_dict in all_layers_dict.items():\n",
    "            group = f.create_group('/', layer_name)\n",
    "            for tensor_name, tensor in layer_dict.items():\n",
    "                tensor_shape = tensor.get_shape().as_list()\n",
    "                if len(tensor_shape) > 0 and tensor_shape[0] is None:\n",
    "                    tensor_shape[0] = batch_save\n",
    "                f.create_earray(group, tensor_name, atom=tables.Float32Atom(), shape=[0, *tensor_shape])\n",
    "                # print(layer_name, tensor_name, tensor_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(filename):\n",
    "    with tables.open_file(filename, mode='a') as f:\n",
    "        for group in f.root:\n",
    "            print(group._v_name)\n",
    "            for tensor_earray in group:\n",
    "                print('{0:10} {1:20} {2:30}'.format(group._v_name, tensor_earray._v_name, str(tensor_earray.shape)),\n",
    "                      '{0:12.2f}'.format(tensor_earray[:].nbytes/1e6), 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_log(filename, all_layers_dict):\n",
    "    with tables.open_file(filename, mode='a') as f:\n",
    "        for layer_name, layer_dict in all_layers_dict.items():\n",
    "            group = f.root[layer_name]\n",
    "            for tensor_name, tensor_data in layer_dict.items():\n",
    "                group[tensor_name].append( np.expand_dims(tensor_data, axis=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'outarray.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_log(filename, dict_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_log(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tables.open_file('outarray.h5', mode='r') as f:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(logdir='tf_log', graph=sess.graph)\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5 # 40\n",
    "batch_size = 250\n",
    "batch_save = 5\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, idx in enumerate(range(0, len(train_x), batch_size)):\n",
    "            \n",
    "            xb = train_x[idx:idx+batch_size]\n",
    "            yb = train_y[idx:idx+batch_size]\n",
    "            \n",
    "            summary, _, _ = sess.run([dict_layers, train_op, loss], feed_dict={xi: xb, yi: yb, kp: keep_probability})\n",
    "            \n",
    "            append_log(filename, summary)\n",
    "            \n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        train_cost, train_acc = sess.run([loss, accuracy], feed_dict={xi: xb, yi: yb, kp: 1.0})\n",
    "        valid_cost, valid_acc = sess.run([loss, accuracy], feed_dict={xi: valid_x, yi: valid_y, kp: 1.0})\n",
    "        print('tc, vc, tacc, vacc', train_cost, valid_cost, train_acc, valid_acc)\n",
    "        \n",
    "    saver.save(sess, './model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './model.ckpt')\n",
    "    \n",
    "    train_cost, train_acc = sess.run([loss, accuracy], feed_dict={xi: train_x[:10000], yi: train_y[:10000], kp: 1.0})\n",
    "    valid_cost, valid_acc = sess.run([loss, accuracy], feed_dict={xi: valid_x, yi: valid_y, kp: 1.0})\n",
    "    test_cost, test_acc   = sess.run([loss, accuracy], feed_dict={xi: test_x,  yi: test_y,  kp: 1.0})\n",
    "    print('Train Set Accuracy:', train_acc)\n",
    "    print('Validation Set Acc:', valid_acc)\n",
    "    print('Test Set Accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition - Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "    TODO: \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess:\n",
    "    devs = sess.list_devices()\n",
    "    print('\\n'.join([x.name for x in devs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import PIL.ImageOps\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path():\n",
    "    PATH = '~/'\n",
    "    full_path = os.path.expanduser(PATH)\n",
    "    all_in_home = os.listdir(full_path)\n",
    "\n",
    "    screenshot_paths = []\n",
    "    for file_name in all_in_home:\n",
    "        file_path = os.path.join(full_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_name.startswith('Screenshot'):\n",
    "                screenshot_paths.append(file_path)\n",
    "\n",
    "    image_path = sorted(screenshot_paths)[-1]\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(image_path):\n",
    "    # fetch image\n",
    "    img_orig = PIL.Image.open(image_path).convert('RGB')\n",
    "    w, h = img_orig.size\n",
    "    ms = min(img_orig.size)\n",
    "    img_cropped = img_orig.crop([w//2-ms//2, h//2-ms//2, w//2+ms//2, h//2+ms//2])\n",
    "\n",
    "    # thumbnail 32x32\n",
    "    thumb = PIL.ImageOps.fit(img_cropped, [32, 32], PIL.Image.ANTIALIAS)\n",
    "\n",
    "    # numpy array 32x32x3\n",
    "    arr3 = np.array(thumb, dtype=np.float32) / 255\n",
    "\n",
    "    # array 1x32x32x3\n",
    "    arr4 = np.reshape(arr3, [-1, *arr.shape])\n",
    "    return arr3, arr4, img_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_get_probabilities(x_data):    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, './model.ckpt')\n",
    "\n",
    "        out_logits, out_prob = sess.run([logits, probabilities], feed_dict={xi: x_data, kp:1.0})\n",
    "\n",
    "#         print('logits:', out_logits)\n",
    "#         print('probabilities', out_prob)\n",
    "#         print('argmax', np.argmax(out_prob))\n",
    "#         print('category:', lab2txt[np.argmax(out_prob)])\n",
    "        \n",
    "        return out_prob.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_predictions(features, labels, predictions):\n",
    "    n_classes = 10\n",
    "    label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "\n",
    "    fig, axies = plt.subplots(nrows=4, ncols=2)\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
    "\n",
    "    n_predictions = 3\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "\n",
    "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
    "        pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "        correct_name = label_names[label_id]\n",
    "\n",
    "        axies[image_i][0].imshow(feature)\n",
    "        axies[image_i][0].set_title(correct_name)\n",
    "        axies[image_i][0].set_axis_off()\n",
    "\n",
    "        axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
    "        axies[image_i][1].set_yticks(ind + margin)\n",
    "        axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
    "        axies[image_i][1].set_xticks([0, 0.5, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue'] * 10\n",
    "colors[3] = 'red'\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2txt = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def run_neural_net():\n",
    "    x_3d, x_4d, img = get_input_data(get_image_path())\n",
    "    prob = tf_get_probabilities(x_4d)\n",
    "\n",
    "    fig = plt.figure(figsize=[16,6])\n",
    "    axi = fig.add_subplot(131)\n",
    "    axi.imshow(img)\n",
    "    axi.set_title('Original Image')\n",
    "    \n",
    "    axn = fig.add_subplot(132)\n",
    "    axn.imshow(x_4d[0])\n",
    "    axn.set_title('Neural Network Sees')\n",
    "\n",
    "    axp = fig.add_subplot(133)\n",
    "    colors = ['blue'] * 10\n",
    "    colors[np.argmax(prob)] = 'green'\n",
    "    axp.barh(range(10), prob,   align='center', color=colors)\n",
    "    axp.set_yticks(range(10))\n",
    "    strings = map('{:.2f}'.format, prob*100)\n",
    "    labels = map('  '.join, zip(lab2txt, strings))\n",
    "    axp.set_yticklabels(labels)\n",
    "    axp.set_title('Predictions')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# run_neural_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2txt = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "idx = 120\n",
    "x = np.expand_dims(train_x[idx], axis=0)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './model.ckpt')\n",
    "    \n",
    "    out_logits = sess.run([logits], feed_dict={xi: x, kp:1.0})\n",
    "    \n",
    "    index = np.argmax(out_logits)\n",
    "    print()\n",
    "    print(lab2txt[index])\n",
    "    plt.imshow(train_x[idx])\n",
    "    \n",
    "    \n",
    "#     train_cost, train_acc = sess.run([loss, accuracy], feed_dict={xi: train_x[:10000], yi: train_y[:10000], kp: 1.0})\n",
    "#     valid_cost, valid_acc = sess.run([loss, accuracy], feed_dict={xi: valid_x, yi: valid_y, kp: 1.0})\n",
    "#     test_cost, test_acc   = sess.run([loss, accuracy], feed_dict={xi: test_x,  yi: test_y,  kp: 1.0})\n",
    "#     print('Train Set Accuracy:', train_acc)\n",
    "#     print('Validation Set Acc:', valid_acc)\n",
    "#     print('Test Set Accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net can recognize following categories:\n",
    "\n",
    "<img src=\"cifar-10-random.png\">\n",
    "\n",
    "To run this:\n",
    "\n",
    "1. Take screenshot with SHIFT-PrintScreen\n",
    "2. Run cell bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_neural_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
