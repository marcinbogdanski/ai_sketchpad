{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset\n",
    "\n",
    "The CIFAR-10 dataset consists of __60000 32x32__ colour images in __10 classes__, with 6000 images per class. There are 50000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import hashlib\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download approximately 170MB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "fname = 'cifar-10-python.tar.gz'\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "fsum = hashlib.md5(open(fname, 'rb').read()).hexdigest()\n",
    "assert fsum == 'c58f30108f718f92721af3b95e74349a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(tfile, member):\n",
    "    buff = tfile.extractfile(member)\n",
    "    pdata = pickle.load(buff, encoding='latin1')\n",
    "    features = pdata['data']\n",
    "    features = features.reshape([len(pdata['data']), 3, 32, 32])\n",
    "    features = features.transpose(0, 2, 3, 1)\n",
    "    labels = pdata['labels']\n",
    "    return features, labels\n",
    "\n",
    "with tarfile.open(fname, 'r:gz') as tfile:\n",
    "    members = tfile.getmembers()\n",
    "    x1, y1 = extract(tfile, 'cifar-10-batches-py/data_batch_1')\n",
    "    x2, y2 = extract(tfile, 'cifar-10-batches-py/data_batch_2')\n",
    "    x3, y3 = extract(tfile, 'cifar-10-batches-py/data_batch_3')\n",
    "    x4, y4 = extract(tfile, 'cifar-10-batches-py/data_batch_4')\n",
    "    x5, y5 = extract(tfile, 'cifar-10-batches-py/data_batch_5')    \n",
    "    \n",
    "    train_x_raw = np.concatenate([x1, x2, x3, x4, x5])\n",
    "    train_y_raw = np.concatenate([y1, y2, y3, y4, y5])\n",
    "    \n",
    "    test_x_raw, test_y_raw = extract(tfile, 'cifar-10-batches-py/test_batch')\n",
    "    \n",
    "    del x1, x2, x3, x4, x5, y1, y2, y3, y4, y5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_mean = train_x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_std = train_x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = (train_x - train_x_mean) / train_x_std\n",
    "test_x = (test_x - train_x_mean) / train_x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x_raw / 255\n",
    "test_x = test_x_raw / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(data_0123):\n",
    "    N = len(data_0123)\n",
    "    res = np.zeros([N, 10])\n",
    "    res[range(N), data_0123] = 1\n",
    "    return res\n",
    "\n",
    "train_y = onehot(train_y_raw)\n",
    "test_y = onehot(test_y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split - dataset is already shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(len(train_x)*0.1)\n",
    "\n",
    "valid_x = train_x[-idx:]\n",
    "valid_y = train_y[-idx:]\n",
    "\n",
    "train_x = train_x[:-idx]\n",
    "train_y = train_y[:-idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise  # Sentinel\n",
    "with gzip.open('CIFAR-10.gz', 'wb') as gfile:\n",
    "    pickle_dict = {'train_x': train_x, 'train_y': train_y,\n",
    "                   'valid_x': valid_x, 'valid_y': valid_y,\n",
    "                   'test_x': test_x, 'test_y': test_y }\n",
    "    pickle.dump(pickle_dict, gfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess:\n",
    "    devs = sess.list_devices()\n",
    "    print('\\n'.join([x.name for x in devs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('CIFAR-10.gz', 'rb') as gfile:\n",
    "    pickle_dict = pickle.load(gfile)\n",
    "    train_x = pickle_dict['train_x']\n",
    "    train_y = pickle_dict['train_y']\n",
    "    valid_x = pickle_dict['valid_x']\n",
    "    valid_y = pickle_dict['valid_y']\n",
    "    test_x = pickle_dict['test_x']\n",
    "    test_y = pickle_dict['test_y']\n",
    "    del pickle_dict  # free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2txt = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "idx = 43\n",
    "plt.imshow(train_x[idx])\n",
    "print(lab2txt[np.argmax(train_y[idx])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Level Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "xi = tf.placeholder(name='xi', shape=[None, 32, 32, 3], dtype=tf.float32)\n",
    "yi = tf.placeholder(name='yi', shape=[None, 10], dtype=tf.float32)\n",
    "kp = tf.placeholder(name='kp', shape=[], dtype=tf.float32)\n",
    "\n",
    "model = tf.layers.conv2d(xi,\n",
    "    filters=32, kernel_size=[5, 5], strides=[1, 1], padding='SAME', activation=tf.nn.relu, name='Conv_1')\n",
    "model = tf.layers.max_pooling2d(model,\n",
    "    pool_size=[2, 2], strides=[2, 2], padding='SAME', name='Pool_1')\n",
    "\n",
    "model = tf.layers.conv2d(model,\n",
    "    filters=64, kernel_size=[5, 5], strides=[1, 1], padding='SAME', activation=tf.nn.relu, name='Conv_2')\n",
    "model = tf.layers.max_pooling2d(model,\n",
    "    pool_size=[2, 2], strides=[2, 2], padding='SAME', name='Pool_2')\n",
    "\n",
    "model = tf.layers.conv2d(model,\n",
    "    filters=128, kernel_size=[5, 5], strides=[1, 1], padding='SAME', activation=tf.nn.relu, name='Conv_3')\n",
    "model = tf.layers.max_pooling2d(model,\n",
    "    pool_size=[2, 2], strides=[2, 2], padding='SAME', name='Pool_3')\n",
    "\n",
    "model = tf.layers.flatten(model)\n",
    "model = tf.layers.dense(model, 128, activation=tf.nn.relu, name='Dense_1')\n",
    "model = tf.nn.dropout(model, kp)\n",
    "logits = tf.layers.dense(model, 10, activation=None, name='Logits')\n",
    "\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(yi, logits)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(yi, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all tensor names in graph\n",
    "[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all trainable variables\n",
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display gradient tensors\n",
    "grads_and_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tensorboard graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(logdir='tf_log', graph=sess.graph)\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build logging dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_save = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_inout = {\n",
    "    'batch_x' : xi[0:batch_save,:,:,:],\n",
    "    'batch_y' : yi[0:batch_save,:],\n",
    "    'probabilities' : probabilities[0:batch_save,:]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_conv_1 = {\n",
    "    'W': graph.get_tensor_by_name('Conv_1/kernel:0'),\n",
    "    'b': graph.get_tensor_by_name('Conv_1/bias:0'),\n",
    "    'dW': graph.get_tensor_by_name('gradients/Conv_1/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "    'db': graph.get_tensor_by_name('gradients/Conv_1/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "    'z': graph.get_tensor_by_name('Conv_1/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_conv_2 = {\n",
    "    'W': graph.get_tensor_by_name('Conv_2/kernel:0'),\n",
    "    'b': graph.get_tensor_by_name('Conv_2/bias:0'),\n",
    "    'dW': graph.get_tensor_by_name('gradients/Conv_2/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "    'db': graph.get_tensor_by_name('gradients/Conv_2/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "    'z': graph.get_tensor_by_name('Conv_2/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_conv_3 = {\n",
    "    'W': graph.get_tensor_by_name('Conv_3/kernel:0'),\n",
    "    'b': graph.get_tensor_by_name('Conv_3/bias:0'),\n",
    "    'dW': graph.get_tensor_by_name('gradients/Conv_3/Conv2D_grad/tuple/control_dependency_1:0'),\n",
    "    'db': graph.get_tensor_by_name('gradients/Conv_3/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "    'z': graph.get_tensor_by_name('Conv_3/BiasAdd:0')[0:batch_save,:,:,:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dense_1 = {\n",
    "    'W': graph.get_tensor_by_name('Dense_1/kernel:0'),\n",
    "    'b': graph.get_tensor_by_name('Dense_1/bias:0'),\n",
    "    'dW': graph.get_tensor_by_name('gradients/Dense_1/MatMul_grad/tuple/control_dependency_1:0'),\n",
    "    'db': graph.get_tensor_by_name('gradients/Dense_1/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "    'z': graph.get_tensor_by_name('Dense_1/BiasAdd:0')[0:batch_save,:],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_logits = {\n",
    "    'W': graph.get_tensor_by_name('Logits/kernel:0'),\n",
    "    'b': graph.get_tensor_by_name('Logits/bias:0'),\n",
    "    'dW': graph.get_tensor_by_name('gradients/Logits/MatMul_grad/tuple/control_dependency_1:0'),\n",
    "    'db': graph.get_tensor_by_name('gradients/Logits/BiasAdd_grad/tuple/control_dependency_1:0'),\n",
    "    'z': graph.get_tensor_by_name('Logits/BiasAdd:0')[0:batch_save,:],\n",
    "}\n",
    "\n",
    "dict_metrics = {\n",
    "    'loss': loss,\n",
    "    'accuracy': accuracy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_layers = {\n",
    "    'inout': dict_inout,\n",
    "    'conv_1': dict_conv_1,\n",
    "    'conv_2': dict_conv_2,\n",
    "    'conv_3': dict_conv_3,\n",
    "    'dense_1': dict_dense_1,\n",
    "    'logits': dict_logits,\n",
    "    'metrics': dict_metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables_logger\n",
    "import importlib\n",
    "importlib.reload(tables_logger)\n",
    "filename = 'outarray.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_logger.create_log(filename, dict_layers, batch_save)  # this overrides any existing file with empty lotc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_logger.print_raw(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_logger.print_log(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 # 5 # 40\n",
    "batch_size = 250\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, idx in enumerate(range(0, len(train_x), batch_size)):\n",
    "            \n",
    "            xb = train_x[idx:idx+batch_size]\n",
    "            yb = train_y[idx:idx+batch_size]\n",
    "            \n",
    "            summary, _, _ = sess.run([dict_layers, train_op, loss], feed_dict={xi: xb, yi: yb, kp: keep_probability})\n",
    "            \n",
    "            tables_logger.append_log(filename, summary)\n",
    "            \n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        train_cost, train_acc = sess.run([loss, accuracy], feed_dict={xi: xb, yi: yb, kp: 1.0})\n",
    "        valid_cost, valid_acc = sess.run([loss, accuracy], feed_dict={xi: valid_x, yi: valid_y, kp: 1.0})\n",
    "        print('tc, vc, tacc, vacc', train_cost, valid_cost, train_acc, valid_acc)\n",
    "        \n",
    "    saver.save(sess, './model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './model.ckpt')\n",
    "    \n",
    "    train_cost, train_acc = sess.run([loss, accuracy], feed_dict={xi: train_x[:10000], yi: train_y[:10000], kp: 1.0})\n",
    "    valid_cost, valid_acc = sess.run([loss, accuracy], feed_dict={xi: valid_x, yi: valid_y, kp: 1.0})\n",
    "    test_cost, test_acc   = sess.run([loss, accuracy], feed_dict={xi: test_x,  yi: test_y,  kp: 1.0})\n",
    "    print('Train Set Accuracy:', train_acc)\n",
    "    print('Validation Set Acc:', valid_acc)\n",
    "    print('Test Set Accuracy: ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
