{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pdb\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotann\n",
    "import importlib\n",
    "importlib.reload(plotann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('MNIST-Dataset', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_raw = mnist.train.images\n",
    "train_y_raw = mnist.train.labels\n",
    "valid_x_raw = mnist.validation.images\n",
    "valid_y_raw = mnist.validation.labels\n",
    "test_x_raw = mnist.test.images\n",
    "test_y_raw = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist(data_x, data_y, n):\n",
    "    fig = plt.figure(figsize=[16,9])\n",
    "    for i in range(n):\n",
    "        ax = fig.add_subplot(n//8, 8, i+1)\n",
    "        ax.imshow(data_x[i].reshape([28,28]))\n",
    "        ax.axis('off')\n",
    "        idx = int(np.nonzero(data_y[i])[0])\n",
    "        ax.set_title(idx)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_mnist(train_x_raw, train_y_raw, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(valid_x_raw, valid_y_raw, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(test_x_raw, test_y_raw, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_x_raw[0:100].flatten(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train mean:', train_x_raw.mean(), 'std', train_x_raw.std())\n",
    "print('valid mean:', valid_x_raw.mean(), 'std', valid_x_raw.std())\n",
    "print('test mean:', test_x_raw.mean(), 'std', test_x_raw.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_mean = train_x_raw.mean()\n",
    "tx_std = train_x_raw.std()\n",
    "train_x = (train_x_raw - tx_mean)/tx_std\n",
    "valid_x = (valid_x_raw - tx_mean)/tx_std\n",
    "test_x = (test_x_raw - tx_mean)/tx_std\n",
    "train_y = train_y_raw\n",
    "valid_y = valid_y_raw\n",
    "test_y = test_y_raw\n",
    "print('train mean:', train_x.mean(), 'std', train_x.std())\n",
    "print('valid mean:', valid_x.mean(), 'std', valid_x.std())\n",
    "print('test mean:', test_x.mean(), 'std', test_x.std())\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(valid_x.shape, valid_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(train_x, train_y, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast import/restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import plotann\n",
    "import pdb\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST-Dataset', one_hot=True)\n",
    "\n",
    "train_x_raw = mnist.train.images\n",
    "train_y_raw = mnist.train.labels\n",
    "valid_x_raw = mnist.validation.images\n",
    "valid_y_raw = mnist.validation.labels\n",
    "test_x_raw = mnist.test.images\n",
    "test_y_raw = mnist.test.labels\n",
    "\n",
    "# Preprocess\n",
    "tx_mean = train_x_raw.mean()\n",
    "tx_std = train_x_raw.std()\n",
    "train_x = (train_x_raw - tx_mean)/tx_std\n",
    "valid_x = (valid_x_raw - tx_mean)/tx_std\n",
    "test_x = (test_x_raw - tx_mean)/tx_std\n",
    "train_y = train_y_raw\n",
    "valid_y = valid_y_raw\n",
    "test_y = test_y_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_to_14_14(data):\n",
    "    assert data.ndim == 2\n",
    "    assert data.shape[1] == 784\n",
    "    data_28x28 = data.reshape([len(data), 28, 28])  # reshape to match image resolution, new shape (nb_samples, 28, 28)\n",
    "    print('28', data_28x28.shape)\n",
    "    data_14x14 = data_28x28[:,::2,::2] # subsample, new shape (nb_samples, 14, 14)\n",
    "    print('14', data_14x14.shape)\n",
    "    data_196 = data_14x14.reshape([len(data),14*14])\n",
    "    print('data_196', data_196.shape)\n",
    "    assert data_196.ndim == 2\n",
    "    assert data_196.shape[1] == 196\n",
    "    return data_196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = subsample_to_14_14(train_x)\n",
    "valid_x = subsample_to_14_14(valid_x)\n",
    "test_x = subsample_to_14_14(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "These are used for numpy model, but also for plotting later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1. - np.tanh(x)**2\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softssign(x, deriv=False):\n",
    "    if deriv:\n",
    "        dd = 1 + np.abs(x)\n",
    "        return (dd - x*np.sign(x)) / dd**2\n",
    "    return x / (1+np.abs(x))\n",
    "\n",
    "def relu(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1. * (x>0)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def lrelu(x, deriv=False):\n",
    "    if deriv:\n",
    "        dx = np.ones_like(x)\n",
    "        dx[x < 0] = 0.01\n",
    "        return dx\n",
    "    return np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "act_fun_dict = {'sigmoid': sigmoid, 'tanh': tanh, 'softssign':softssign,\n",
    "                'relu':relu, 'lrelu':lrelu}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_hat):\n",
    "    assert y.ndim == 2\n",
    "    assert y_hat.ndim == 2\n",
    "    \n",
    "    # avg over batch, sum over outputs (inner)\n",
    "    return .5 * np.mean(np.sum((y-y_hat)**2, axis=-1))\n",
    "    \n",
    "    # no innner sum, becouse only one output\n",
    "    return np.mean((y-y_hat)**2)\n",
    "\n",
    "def acc(y, y_hat):\n",
    "    return np.mean(np.argmax(y_hat, axis=-1)==np.argmax(y, axis=-1))\n",
    "\n",
    "def fwd(x, W_hid, W_out, act_fun, ret=False):\n",
    "    assert x.ndim == 2\n",
    "    z_hid = x @ W_hid\n",
    "    h_hid = act_fun(z_hid)  # hidden output\n",
    "\n",
    "    z_out = h_hid @ W_out\n",
    "    y_hat = sigmoid(z_out)  # SIGMOID!\n",
    "\n",
    "    if ret:\n",
    "        return y_hat, z_hid, h_hid, z_out\n",
    "    return y_hat\n",
    "\n",
    "def backprop(x, y, W_hid, W_out, act_fun):\n",
    "    assert x.ndim == 2\n",
    "    assert y.ndim == 2\n",
    "    \n",
    "    y_hat, z_hid, h_hid, z_out = fwd(x, W_hid, W_out, act_fun, ret=True)\n",
    "    \n",
    "    ro_out = (y-y_hat) * -1 * sigmoid(z_out, deriv=True)  # SIGMOID\n",
    "    dW_out = h_hid.T @ ro_out / len(x)\n",
    "    \n",
    "    ro_hid = (ro_out @ W_out.T) * act_fun(z_hid, deriv=True)\n",
    "    dW_hid = x.T @ ro_hid / len(x)\n",
    "    \n",
    "    return dW_hid, dW_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical gradient check (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrad(x, y, W_hid, W_out, act_fun):\n",
    "    \n",
    "    eps = 1e-6\n",
    "    \n",
    "    gW_hid = np.zeros_like(W_hid)\n",
    "    for r in range(W_hid.shape[0]):\n",
    "        for c in range(W_hid.shape[1]):\n",
    "            W_hid_plus = W_hid.copy()\n",
    "            W_hid_minus = W_hid.copy()\n",
    "            W_hid_plus[r,c] += eps\n",
    "            W_hid_minus[r,c] -= eps\n",
    "            loss_plus = MSE(y, fwd(x, W_hid_plus, W_out, act_fun))\n",
    "            loss_minus = MSE(y, fwd(x, W_hid_minus, W_out, act_fun))\n",
    "            gW_hid[r,c] = (loss_plus-loss_minus) / (2*eps)\n",
    "\n",
    "    gW_out = np.zeros_like(W_out)\n",
    "    for r in range(W_out.shape[0]):\n",
    "        for c in range(W_out.shape[1]):\n",
    "            W_out_plus = W_out.copy()\n",
    "            W_out_minus = W_out.copy()\n",
    "            W_out_plus[r,c] += eps\n",
    "            W_out_minus[r,c] -= eps\n",
    "            loss_plus = MSE(y, fwd(x, W_hid, W_out_plus, act_fun))\n",
    "            loss_minus = MSE(y, fwd(x, W_hid, W_out_minus, act_fun))\n",
    "            gW_out[r,c] = (loss_plus-loss_minus) / (2*eps)\n",
    "    return gW_hid, gW_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_dW, out_dW = backprop(train_x[0:3], train_y[0:3], W_hid, W_out, act_fun)\n",
    "hid_ngW, out_ngW = ngrad(train_x[0:3], train_y[0:3], W_hid, W_out, act_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(hid_dW, hid_ngW)\n",
    "assert np.allclose(out_dW, out_ngW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop - with traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 196   # 784\n",
    "n_hid = 128  # 128  # 128             # sigmoid try 8, 128(def.), 2048\n",
    "n_out = 10\n",
    "lr = 0.03    # 0.03         # sigmoid try 10, 1(best), 0.03, 0.0003\n",
    "\n",
    "np_or_tf = 'tf2'\n",
    "\n",
    "n_batch = 100\n",
    "act_fun = 'relu'\n",
    "completed_epochs = 0\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize weights\n",
    "var_hid = np.sqrt(1/n_in)       # sigmoid try:  0.001,  sqrt(1/n_in),  1\n",
    "var_out = np.sqrt(1/n_hid)\n",
    "W_hid = np.random.normal(0.0, var_hid, [n_in, n_hid])\n",
    "W_out = np.random.normal(0.0, var_out, [n_hid, n_out])\n",
    "\n",
    "batches = {'iter':[], 'loss':[], 'acc':[]}\n",
    "valids = {'iter':[], 'loss':[]}\n",
    "traces = {'hid_z':[], 'out_z':[],\n",
    "          'hid_dW':[], 'out_dW':[],\n",
    "          'hid_W':[], 'out_W':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np_or_tf == 'np':\n",
    "    print('Skipping graph build')\n",
    "    \n",
    "elif np_or_tf == 'tf':\n",
    "    print('Initializing TensorFlow graph')\n",
    "    \n",
    "    try:    sess.close()\n",
    "    except: pass\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    nn_x = tf.placeholder(shape=[None, n_in], dtype=tf.float32)\n",
    "    nn_y = tf.placeholder(shape=[None, n_out], dtype=tf.float32)\n",
    "\n",
    "    nn_hid_W = tf.get_variable('nn_hid_W', shape=W_hid.shape, dtype=tf.float32, initializer=tf.constant_initializer(W_hid))\n",
    "    nn_hid_z = tf.matmul(nn_x, nn_hid_W)\n",
    "    if act_fun == 'sigmoid':\n",
    "        nn_hid_h = tf.nn.sigmoid(nn_hid_z)\n",
    "    elif act_fun == 'relu':\n",
    "        nn_hid_h = tf.nn.relu(nn_hid_z)\n",
    "    else:\n",
    "        raise ValueError('unknown activation function')\n",
    "\n",
    "    nn_out_W = tf.get_variable('nn_out_W', shape=W_out.shape, dtype=tf.float32, initializer=tf.constant_initializer(W_out))\n",
    "    nn_out_z = tf.matmul(nn_hid_h, nn_out_W)\n",
    "    nn_y_hat = tf.nn.sigmoid(nn_out_z)  # output always sigmoid\n",
    "\n",
    "    nn_mse = .5 * tf.reduce_mean( tf.reduce_sum(tf.pow(nn_y-nn_y_hat, 2), axis=-1) )\n",
    "    nn_acc = tf.reduce_mean( \n",
    "                tf.cast( \n",
    "                    tf.equal( tf.argmax(nn_y_hat, axis=-1), tf.argmax(nn_y, axis=-1) )\n",
    "                , tf.float32)\n",
    "            )\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(learning_rate=lr)\n",
    "    # train_op = optimizer.minimize(nn_mse)\n",
    "\n",
    "    grads_and_vars = optimizer.compute_gradients(nn_mse)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    nn_hid_dW = grads_and_vars[0][0]\n",
    "    nn_out_dW = grads_and_vars[1][0]\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "elif np_or_tf == 'tf2':\n",
    "    print('Initializing TensorFlow graph - High Level API')\n",
    "    \n",
    "    try:    sess.close()\n",
    "    except: pass\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    nn_x = tf.placeholder(shape=[None, n_in], dtype=tf.float32)\n",
    "    nn_y = tf.placeholder(shape=[None, n_out], dtype=tf.float32)\n",
    "    \n",
    "    nn_hid_h = tf.layers.dense(inputs=nn_x,\n",
    "                               units=n_hid,\n",
    "                               activation=tf.nn.relu,\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=tf.constant_initializer(W_hid),\n",
    "                               name='Hidden')\n",
    "    nn_hid_z = tf.get_default_graph().get_tensor_by_name('Hidden/MatMul:0')\n",
    "    \n",
    "    nn_y_hat = tf.layers.dense(inputs=nn_hid_h,\n",
    "                               units=n_out,\n",
    "                               activation=tf.nn.sigmoid,\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=tf.constant_initializer(W_out),\n",
    "                               name='Output')\n",
    "    nn_out_z = tf.get_default_graph().get_tensor_by_name('Output/MatMul:0')\n",
    "        \n",
    "    nn_mse = .5 * tf.reduce_mean( tf.reduce_sum(tf.pow(nn_y-nn_y_hat, 2), axis=-1) )\n",
    "    nn_acc = tf.reduce_mean( \n",
    "                tf.cast( \n",
    "                    tf.equal( tf.argmax(nn_y_hat, axis=-1), tf.argmax(nn_y, axis=-1) )\n",
    "                , tf.float32)\n",
    "            )\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    \n",
    "    grads_and_vars = optimizer.compute_gradients(nn_mse)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "    \n",
    "    nn_hid_dW = grads_and_vars[0][0]\n",
    "    nn_out_dW = grads_and_vars[1][0]\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.summary.FileWriter(logdir='tf_log', graph=sess.graph)\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_ = 0\n",
    "train_i = np.array(range(len(train_x)))\n",
    "for e in range(10):\n",
    "    print(e)\n",
    "    np.random.shuffle(train_i)\n",
    "            \n",
    "    for i in range(0, len(train_x), n_batch):\n",
    "\n",
    "        # Get 128 sized batch, both as 2d arrays   \n",
    "        batch = train_i[i:i+n_batch]\n",
    "        x = train_x[batch]\n",
    "        y = train_y[batch]\n",
    "        \n",
    "        if np_or_tf == 'np':\n",
    "            \n",
    "            if ti_ == 0:\n",
    "                print('Executing Numpy version')\n",
    "            \n",
    "            # Forward pass\n",
    "            y_hat, hid_z, _, out_z = fwd(x, W_hid, W_out, act_fun_dict[act_fun], ret=True)\n",
    "            loss = MSE(y, y_hat)\n",
    "            accuracy = acc(y, y_hat)\n",
    "            \n",
    "            # Backpropagation\n",
    "            hid_dW, out_dW = backprop(x, y, W_hid, W_out, act_fun_dict[act_fun])\n",
    "            W_hid += -lr * hid_dW\n",
    "            W_out += -lr * out_dW\n",
    "            \n",
    "        elif np_or_tf == 'tf':\n",
    "            \n",
    "            if ti_ == 0:\n",
    "                print('Executing Tensorflow version')\n",
    "            \n",
    "            _, y_hat, hid_z, out_z, loss, accuracy, hid_dW, out_dW = sess.run(\n",
    "                [train_op, nn_y_hat, nn_hid_z, nn_out_z, nn_mse, nn_acc, nn_hid_dW, nn_out_dW],\n",
    "                feed_dict={nn_x: x, nn_y:y})\n",
    "            W_hid, W_out = sess.run(tf.trainable_variables())\n",
    "            \n",
    "            loss_val = sess.run(nn_mse, feed_dict={nn_x: valid_x, nn_y:valid_y})\n",
    "            \n",
    "            assert y_hat.shape == y.shape\n",
    "            assert np.isscalar(loss)\n",
    "            assert np.isscalar(accuracy)\n",
    "            \n",
    "        elif np_or_tf == 'tf2':\n",
    "            \n",
    "            if ti_ == 0:\n",
    "                print('Executing Tensorflow version - High Level API')\n",
    "            \n",
    "            _, y_hat, hid_z, out_z, loss, accuracy, hid_dW, out_dW = sess.run(\n",
    "                [train_op, nn_y_hat, nn_hid_z, nn_out_z, nn_mse, nn_acc, nn_hid_dW, nn_out_dW],\n",
    "                feed_dict={nn_x: x, nn_y:y})\n",
    "            W_hid, W_out = sess.run(tf.trainable_variables())\n",
    "            \n",
    "            loss_val = sess.run(nn_mse, feed_dict={nn_x: valid_x, nn_y:valid_y})\n",
    "            \n",
    "            \n",
    "            assert y_hat.shape == y.shape\n",
    "            assert np.isscalar(loss)\n",
    "            assert np.isscalar(accuracy)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            raise ValueError('np_or_tf must be \"np\" or \"tf\"')\n",
    "        \n",
    "        batches['iter'].append(ti_)\n",
    "        batches['loss'].append(loss)\n",
    "        batches['acc'].append(accuracy)\n",
    "        \n",
    "        valids['iter'].append(ti_)\n",
    "        valids['loss'].append(loss_val)\n",
    "        \n",
    "        traces['hid_z'].append(hid_z)\n",
    "        traces['out_z'].append(out_z)\n",
    "        traces['hid_dW'].append(hid_dW)\n",
    "        traces['out_dW'].append(out_dW)\n",
    "        traces['hid_W'].append(W_hid.copy())\n",
    "        traces['out_W'].append(W_out.copy())\n",
    "\n",
    "        ti_ += 1\n",
    "        \n",
    "    completed_epochs += 1\n",
    "    \n",
    "expstr = 'Network(' + np_or_tf + '): '\n",
    "expstr += str(n_in) + 'in->' + str(n_hid) + act_fun + '->' + str(n_out) + 'sig   '\n",
    "expstr += 'init_var=[' + str(round(var_hid, 3)) + ',' + str(round(var_out,3))+ ']   '\n",
    "expstr += 'lr=' + str(lr) + '   '\n",
    "expstr += 'batch=' + str(n_batch) + '   '\n",
    "expstr += 'epochs=' + str(completed_epochs)\n",
    "\n",
    "tr_hid_z = np.array(traces['hid_z'])\n",
    "tr_out_z = np.array(traces['out_z'])\n",
    "tr_hid_dW = np.array(traces['hid_dW'])\n",
    "tr_out_dW = np.array(traces['out_dW'])\n",
    "tr_hid_W = np.array(traces['hid_W'])\n",
    "tr_out_W = np.array(traces['out_W'])\n",
    "\n",
    "print('tr_hid_z', tr_hid_z.shape, tr_hid_z.size/1e6)\n",
    "print('tr_out_z', tr_out_z.shape, tr_out_z.size/1e6)\n",
    "print('tr_hid_dW', tr_hid_dW.shape, tr_hid_dW.size/1e6)\n",
    "print('tr_out_dW', tr_out_dW.shape, tr_out_dW.size/1e6)\n",
    "print('tr_hid_W', tr_hid_W.shape, tr_hid_W.size/1e6)\n",
    "print('tr_out_W', tr_out_W.shape, tr_out_W.size/1e6)\n",
    "\n",
    "print(expstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Plot Loss, Accuracy\n",
    "#\n",
    "print(expstr)\n",
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "ax.plot(batches['iter'], batches['loss'], label='Mini-Batch loss', alpha=.5)\n",
    "ax.plot(batches['iter'], batches['acc'], label='Mini-Batch accuracy', color='red', alpha=.5)\n",
    "ax.plot(valids['iter'], valids['loss'], label='Validation loss', alpha=.5)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_coverage(y, y_hat, axis=None):\n",
    "    #\n",
    "    #   Per-class accuracy\n",
    "    #\n",
    "    assert y.ndim == 2 and y_hat.ndim == 2\n",
    "    assert y.shape == y_hat.shape\n",
    "    \n",
    "    nb_classses = y.shape[-1]\n",
    "    \n",
    "    y_correct = np.argmax(y, axis=-1) == np.argmax(y_hat, axis=-1)\n",
    "    classes_correct = []\n",
    "    classes_all = []\n",
    "    for i in range(nb_classses):\n",
    "        is_y_class_i = y_correct * (np.argmax(y, axis=-1)==i)\n",
    "        nb_correct_class_i = np.sum(is_y_class_i)\n",
    "        classes_correct.append(nb_correct_class_i)\n",
    "        classes_all.append(np.count_nonzero(np.argmax(y, axis=-1)==i))\n",
    "    classes_correct = np.array(classes_correct)\n",
    "    classes_all = np.array(classes_all)\n",
    "\n",
    "    if axis is None:\n",
    "        fig, axis = plt.subplots()\n",
    "    \n",
    "    axis.bar(range(10), classes_all, label='All Member')\n",
    "    axis.bar(range(10), classes_correct, label='Correctly Predicted')\n",
    "    axis.legend(loc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = train_x, train_y\n",
    "if np_or_tf == 'np':              y_hat = fwd(data_x, W_hid, W_out, act_fun_dict[act_fun])\n",
    "elif np_or_tf in ['tf', 'tf2']:   y_hat = sess.run(nn_y_hat, feed_dict={nn_x: data_x})\n",
    "else:                             raise ValueError()\n",
    "plot_class_coverage(data_y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = valid_x, valid_y\n",
    "if np_or_tf == 'np':              y_hat = fwd(data_x, W_hid, W_out, act_fun_dict[act_fun])\n",
    "elif np_or_tf in ['tf', 'tf2']:   y_hat = sess.run(nn_y_hat, feed_dict={nn_x: data_x})\n",
    "else:                             raise ValueError()\n",
    "plot_class_coverage(data_y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, n):\n",
    "    return np.array([ np.mean(x[max(i-n+1, 0): i+1]) for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_std(x, n):\n",
    "    return np.array([ np.std(x[max(i-n+1, 0): i+1]) for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotann.show_neurons_weights(tr_hid_W, tr_hid_dW, neurons=range(3),\n",
    "                            title_prefix='Hidden', color='red', figsize=[16,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotann.show_layer_summary(tr_hid_W, tr_hid_dW,\n",
    "                           title_prefix='Hidden', color='red', figsize=[16,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotann.show_neurons_weights(tr_out_W, tr_out_dW, neurons=range(tr_out_W.shape[-1]),\n",
    "                            title_prefix='Output', color='blue', figsize=[16,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotann.show_layer_summary(tr_out_W, tr_out_dW, title_prefix='Output', color='blue', figsize=[16,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es=55000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotann.show_layer_activations(tr_hid_z, epoch_size=es, activation_function=act_fun,\n",
    "                               title_prefix='Hidden', color=(1,0,0,1), figsize=[16,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotann.show_neurons_activations(tr_hid_z, epoch_size=es, activation_function=act_fun, neurons=range(10),\n",
    "                                 title_prefix='Hidden', color=(1,0,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotann.show_layer_activations(tr_out_z, epoch_size=es, activation_function='sigmoid',\n",
    "                               title_prefix='Output', color=(0,0,1,1), figsize=[16,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotann.show_neurons_activations(tr_out_z, epoch_size=es, activation_function='sigmoid', neurons=range(tr_out_z.shape[-1]),\n",
    "                                 title_prefix='Output', color=(0,0,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
