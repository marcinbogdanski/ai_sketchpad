{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "$$ \\huge{\\underline{\\textbf{ Bag of Words - Fill in the Blank }}} $$\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content:\n",
    "\n",
    "$ \\color{red}{TODO} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\color{red}{TODO} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read text file, each line is one movie review. We don't need sentiment labels for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../021 Sentiment Analysis - Trask/reviews.txt','r') as f:\n",
    "    reviews = list(map(str.strip, f.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many times each word occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_all = collections.Counter()  # how many times each word occurs is whole dataset \n",
    "for review in reviews:\n",
    "    for word in review.split():\n",
    "        counter_all[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74073\n"
     ]
    }
   ],
   "source": [
    "# review_vocab = set(counter_all.keys())\n",
    "# review_vocab_size = len(review_vocab)\n",
    "# print(review_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19869\n"
     ]
    }
   ],
   "source": [
    "min_occurances = 10\n",
    "\n",
    "review_vocab = set()\n",
    "for word, nb_occurances in counter_all.items():\n",
    "    if nb_occurances >= min_occurances:\n",
    "        review_vocab.add(word)\n",
    "review_vocab_size = len(review_vocab)\n",
    "print(review_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "index2word = {}\n",
    "for i, word in enumerate(review_vocab):\n",
    "    word2index[word] = i\n",
    "    index2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_indices(review):\n",
    "    res = []\n",
    "    for word in review.split():\n",
    "        if word in word2index:\n",
    "            res.append(word2index[word])\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "for review in reviews:\n",
    "    review_as_ints = review_to_indices(review)\n",
    "    for i in range(2, len(review_as_ints)-2):\n",
    "        inputs.append(review_as_ints[[i-2, i-1, i+1, i+2]])\n",
    "        targets.append(review_as_ints[i])\n",
    "inputs = np.array(inputs)\n",
    "targets = np.expand_dims(targets, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    return .5 * np.sum((y-y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(xi, yi, Wh, Wo):\n",
    "    \"\"\"Params:\n",
    "        xi - indices [1234, 2345, ... ]\n",
    "        yi - indices [9876, 8765, ... ] - target words, including correct target\n",
    "    \"\"\"\n",
    "    assert xi.ndim == 1\n",
    "    assert yi.ndim == 1\n",
    "    \n",
    "    z_hid = np.sum(Wh[xi], axis=0, keepdims=True)   # select and sum embeddings\n",
    "                                                    # no hidden activation\n",
    "    z_out = z_hid @ Wo[:,yi]                        # linear transform for selected outputs only (for efficiency)\n",
    "    y_hat = sigmoid(z_out)                          # output activation function\n",
    "    \n",
    "    return y_hat, z_out, z_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(xi, yi, y, Wh, Wo, lr, mode='apply_gradient'):\n",
    "    \"\"\"Params:\n",
    "        xi - indices [1234, 2345]\n",
    "    \"\"\"\n",
    "    assert xi.ndim == 1\n",
    "    assert yi.ndim == 1\n",
    "    assert y.ndim == 2\n",
    "    \n",
    "    y_hat, z_out, z_hid = forward(xi, yi, Wh, Wo)\n",
    "    \n",
    "    ro_out = -(y-y_hat) * sigmoid_deriv(z_out)  # scalar\n",
    "    del_Wo_i = np.dot(z_hid.T, ro_out)\n",
    "    \n",
    "    ro_hid = np.dot(ro_out, Wo[:,yi].T)      # 1 x hid_n\n",
    "        \n",
    "    if mode == 'apply_gradient': \n",
    "        Wh[xi] += -lr * ro_hid\n",
    "        Wo[:,yi] += -lr * del_Wo_i\n",
    "        return y_hat\n",
    "\n",
    "    elif mode == 'return_gradients':\n",
    "        # This is for numerical gradient checks, slow\n",
    "        del_Wh = np.zeros_like(Wh)\n",
    "        del_Wh[xi] = ro_hid\n",
    "        del_Wo = np.zeros_like(Wo)\n",
    "        del_Wo[:,yi] = del_Wo_i\n",
    "        return del_Wo, del_Wh\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Unknown mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(word):\n",
    "    embedding = W_hid[word2index[word]]               # target word embedding; shape: (N_hid,) type: float\n",
    "    norms = np.linalg.norm(embedding-W_hid, axis=-1)  # euclidian disances; shape: (N_words,), type: float\n",
    "    sorted_similar = np.argsort(norms)                # indices in sorted order; shape: (N_words,), type: int\n",
    "    return sorted_similar, norms[sorted_similar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_from_indices(indices, how_many):\n",
    "    result = []\n",
    "    for i in range(how_many):\n",
    "        index = indices[i]\n",
    "        result.append(index2word[index])\n",
    "    print(' '.join(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_in = len(review_vocab)\n",
    "N_hid = 50\n",
    "N_out = len(review_vocab)\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_nn():\n",
    "    global W_hid, W_out\n",
    "    np.random.seed(1)\n",
    "    W_hid = np.random.normal(0, N_in**-.5, [N_in, N_hid])\n",
    "    W_out = np.random.normal(0, N_hid**-.5, [N_hid, N_out])\n",
    "    \n",
    "    # Trask\n",
    "#     W_hid = (np.random.rand(len(review_vocab),N_hid) - 0.5) * 0.2\n",
    "#     W_out = np.random.rand(N_hid,len(review_vocab))*0\n",
    "reset_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Iteration: 0 \tProgress 0.0 \tSpeed(rev/sec): 0.0\n",
      "terrible cupboard susie afoul sickest tics cults baio smitten consumers\n",
      "[0.     0.0451 0.0456 0.0463 0.0467 0.0468 0.047  0.0472 0.0475 0.0475]\n",
      "----\n",
      "Iteration: 100000 \tProgress 0.016357354544678314 \tSpeed(rev/sec): 7884.0\n",
      "terrible stewart plus billy danny motion frank italian talent hair\n",
      "[0.     0.1158 0.1198 0.1225 0.124  0.1252 0.1268 0.1269 0.1278 0.1287]\n",
      "----\n",
      "Iteration: 200000 \tProgress 0.03271470908935663 \tSpeed(rev/sec): 7987.0\n",
      "terrible boring johnny awful hilarious produced pure important harry william\n",
      "[0.     0.2321 0.2342 0.2347 0.2397 0.2398 0.2416 0.2421 0.2422 0.2431]\n",
      "----\n",
      "Iteration: 300000 \tProgress 0.04907206363403494 \tSpeed(rev/sec): 8015.0\n",
      "terrible boring awful totally awesome horrible predictable amazing harry unfortunately\n",
      "[0.     0.2684 0.2739 0.2773 0.3028 0.3038 0.31   0.3109 0.3113 0.313 ]\n",
      "----\n",
      "Iteration: 400000 \tProgress 0.06542941817871326 \tSpeed(rev/sec): 8034.0\n",
      "terrible totally boring amazing hilarious fantastic completely simply awesome annoying\n",
      "[0.     0.3265 0.3346 0.3411 0.3445 0.3487 0.3488 0.3524 0.3535 0.3537]\n",
      "----\n",
      "Iteration: 500000 \tProgress 0.08178677272339158 \tSpeed(rev/sec): 8039.0\n",
      "terrible brilliant fantastic awful boring pathetic amazing horrible completely totally\n",
      "[0.     0.3435 0.3505 0.3576 0.362  0.3634 0.3778 0.3788 0.392  0.3926]\n",
      "----\n",
      "Iteration: 600000 \tProgress 0.09814412726806988 \tSpeed(rev/sec): 8037.0\n",
      "terrible boring pathetic horrible brilliant fantastic awful amazing awesome pure\n",
      "[0.     0.35   0.3537 0.3637 0.3638 0.3701 0.3831 0.3988 0.405  0.412 ]\n",
      "----\n",
      "Iteration: 700000 \tProgress 0.1145014818127482 \tSpeed(rev/sec): 8037.0\n",
      "terrible horrible awful amazing okay fantastic basically pathetic superb brilliant\n",
      "[0.     0.3707 0.3836 0.4406 0.4519 0.4542 0.4587 0.4617 0.4678 0.4679]\n",
      "----\n",
      "Iteration: 800000 \tProgress 0.1308588363574265 \tSpeed(rev/sec): 8038.0\n",
      "terrible horrible awful brilliant stupid boring basically beautifully okay fantastic\n",
      "[0.     0.3991 0.4722 0.4723 0.4933 0.4989 0.5177 0.526  0.5288 0.529 ]\n",
      "----\n",
      "Iteration: 900000 \tProgress 0.14721619090210483 \tSpeed(rev/sec): 8039.0\n",
      "terrible horrible brilliant okay basically stupid boring beautifully awful fantastic\n",
      "[0.     0.4158 0.4658 0.5201 0.5279 0.5292 0.5294 0.536  0.5412 0.5419]\n",
      "----\n",
      "Iteration: 1000000 \tProgress 0.16357354544678315 \tSpeed(rev/sec): 8039.0\n",
      "terrible horrible brilliant okay boring awful superb fantastic beautifully basically\n",
      "[0.     0.4759 0.5344 0.5496 0.553  0.5672 0.5756 0.5762 0.579  0.58  ]\n",
      "----\n",
      "Iteration: 1100000 \tProgress 0.17993089999146147 \tSpeed(rev/sec): 8039.0\n",
      "terrible horrible brilliant superb weak pathetic basically beautifully okay lame\n",
      "[0.     0.4395 0.5082 0.5485 0.561  0.5654 0.5706 0.5797 0.593  0.6053]\n",
      "----\n",
      "Iteration: 1200000 \tProgress 0.19628825453613977 \tSpeed(rev/sec): 8041.0\n",
      "terrible horrible brilliant pathetic weak superb beautifully fine basically fantastic\n",
      "[0.     0.397  0.5358 0.5462 0.5728 0.5746 0.5921 0.5954 0.6006 0.6187]\n",
      "----\n",
      "Iteration: 1300000 \tProgress 0.2126456090808181 \tSpeed(rev/sec): 8036.0\n",
      "terrible horrible brilliant awful pathetic okay beautifully superb weak basically\n",
      "[0.     0.4432 0.6108 0.6289 0.6482 0.6482 0.652  0.658  0.6581 0.669 ]\n",
      "----\n",
      "Iteration: 1400000 \tProgress 0.2290029636254964 \tSpeed(rev/sec): 8036.0\n",
      "terrible horrible brilliant superb pathetic okay fine fantastic beautifully weak\n",
      "[0.     0.4165 0.5664 0.6382 0.6442 0.663  0.6651 0.675  0.68   0.6826]\n",
      "----\n",
      "Iteration: 1500000 \tProgress 0.24536031817017473 \tSpeed(rev/sec): 8038.0\n",
      "terrible horrible brilliant awful pathetic okay superb fine poor weak\n",
      "[0.     0.4946 0.6087 0.6604 0.6725 0.706  0.7154 0.7222 0.7268 0.7278]\n",
      "----\n",
      "Iteration: 1600000 \tProgress 0.261717672714853 \tSpeed(rev/sec): 8040.0\n",
      "terrible horrible brilliant awful pathetic poor okay weak boring fine\n",
      "[0.     0.5401 0.6559 0.7233 0.7332 0.7363 0.7495 0.7527 0.7695 0.7766]\n",
      "----\n",
      "Iteration: 1700000 \tProgress 0.27807502725953137 \tSpeed(rev/sec): 8040.0\n",
      "terrible horrible brilliant poor fine awful pathetic weak cool okay\n",
      "[0.     0.5771 0.6295 0.6915 0.7317 0.7383 0.7477 0.7581 0.7844 0.7884]\n",
      "----\n",
      "Iteration: 1800000 \tProgress 0.29443238180420966 \tSpeed(rev/sec): 8035.0\n",
      "terrible horrible brilliant poor awful fine weak superb pathetic ridiculous\n",
      "[0.     0.5225 0.6853 0.7129 0.7837 0.7931 0.8037 0.8072 0.8137 0.8211]\n",
      "----\n",
      "Iteration: 1900000 \tProgress 0.31078973634888796 \tSpeed(rev/sec): 8032.0\n",
      "terrible horrible brilliant poor superb fine weak totally awful fantastic\n",
      "[0.     0.5811 0.7273 0.7725 0.8225 0.8355 0.839  0.8435 0.8507 0.8598]\n",
      "----\n",
      "Iteration: 2000000 \tProgress 0.3271470908935663 \tSpeed(rev/sec): 8027.0\n",
      "terrible horrible brilliant okay superb fine totally awful fantastic poor\n",
      "[0.     0.5855 0.685  0.8119 0.8171 0.8365 0.8374 0.8394 0.8444 0.8488]\n",
      "----\n",
      "Iteration: 2100000 \tProgress 0.3435044454382446 \tSpeed(rev/sec): 8020.0\n",
      "terrible horrible brilliant poor superb totally okay fine scary cool\n",
      "[0.     0.5849 0.7682 0.824  0.8257 0.8269 0.8615 0.8754 0.8805 0.8825]\n",
      "----\n",
      "Iteration: 2200000 \tProgress 0.35986179998292295 \tSpeed(rev/sec): 8018.0\n",
      "terrible horrible brilliant poor totally awful superb ridiculous okay wonderful\n",
      "[0.     0.584  0.8245 0.8362 0.8578 0.8796 0.8808 0.8868 0.8896 0.9159]\n",
      "----\n",
      "Iteration: 2300000 \tProgress 0.37621915452760124 \tSpeed(rev/sec): 8016.0\n",
      "terrible horrible poor brilliant totally mostly fine pathetic ridiculous superb\n",
      "[0.     0.6247 0.8209 0.865  0.912  0.9235 0.9273 0.931  0.9312 0.9329]\n",
      "----\n",
      "Iteration: 2400000 \tProgress 0.39257650907227953 \tSpeed(rev/sec): 8008.0\n",
      "terrible horrible poor brilliant fine superb ridiculous pathetic totally cool\n",
      "[0.     0.7011 0.8646 0.9    0.9094 0.9174 0.9376 0.9448 0.9461 0.9473]\n",
      "----\n",
      "Iteration: 2500000 \tProgress 0.4089338636169579 \tSpeed(rev/sec): 8005.0\n",
      "terrible horrible poor totally ridiculous brilliant superb okay cool awful\n",
      "[0.     0.7866 0.8906 0.9505 0.9512 0.9568 0.9569 0.9678 0.9753 0.9771]\n",
      "----\n",
      "Iteration: 2600000 \tProgress 0.4252912181616362 \tSpeed(rev/sec): 8000.0\n",
      "terrible horrible poor ridiculous awful brilliant lame cool superb okay\n",
      "[0.     0.6993 0.8935 0.9403 0.9568 0.9594 0.9659 0.9715 0.9763 0.9798]\n",
      "----\n",
      "Iteration: 2700000 \tProgress 0.44164857270631447 \tSpeed(rev/sec): 7998.0\n",
      "terrible horrible poor lame brilliant ridiculous weak okay awful cool\n",
      "[0.     0.7696 0.8259 0.9373 0.9392 0.9442 0.9663 0.9716 0.9773 0.9801]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1c766e0933c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0myy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0myi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np_iterations = len(inputs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(np_iterations):\n",
    "    idx = np.random.randint(len(inputs))\n",
    "    xi = inputs[idx]\n",
    "    yy = np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]])\n",
    "    yi = np.concatenate([targets[idx], np.random.choice(targets.reshape(-1), 5)])\n",
    "    \n",
    "    y_hat = backward(xi, yi, yy, W_hid, W_out, lr)\n",
    "    \n",
    "    if i % 100000 == 0:\n",
    "        \n",
    "        timespan = time.time() - start_time\n",
    "        rev_per_sec = i / timespan\n",
    "            \n",
    "        print('----')\n",
    "        print('Iteration:', i, '\\tProgress', i/np_iterations, '\\tSpeed(rev/sec):', round(rev_per_sec, 0))\n",
    "        similar, distances = find_similar('terrible')\n",
    "        print_from_indices(similar, 10)\n",
    "        print(np.round(distances[:10], 4))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in the Blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:      a couple [ of ] bad dreams\n",
      "Predictions:\n",
      "                        nerdy\n",
      "                        burgeoning\n",
      "                        sam\n",
      "                        ground\n",
      "                        stance\n",
      "                        dares\n",
      "                        inventiveness\n",
      "                        attendant\n",
      "                        traci\n",
      "                        hammered\n"
     ]
    }
   ],
   "source": [
    "ii = np.random.randint(len(inputs))\n",
    "xi = inputs[ii]\n",
    "yi = np.array(range(N_out))\n",
    "print('Original sentence:     ',\n",
    "      index2word[xi[0]], index2word[xi[1]],\n",
    "      '[', index2word[targets[ii,0]] ,']',\n",
    "      index2word[xi[2]], index2word[xi[3]])\n",
    "\n",
    "y_hat, _, _ = forward(xi, yi, W_hid, W_out)\n",
    "prediction_indices = np.argsort(y_hat.flatten())[::-1]\n",
    "print('Predictions:')\n",
    "for i in range(10):\n",
    "    print('                       ', index2word[prediction_indices[i]])\n",
    "print('Random:')\n",
    "for i in range(10):\n",
    "    random_index = np.random.randint(len(review_vocab))\n",
    "    print('                       ', index2word[random_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar, distances = find_similar('terrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_from_indices(similar, 10)\n",
    "print(np.round(distances[:10], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(positive=['terrible','good'],negative=['bad']):\n",
    "    \n",
    "    norms = np.sum(W_hid * W_hid,axis=1)\n",
    "    norms.resize(norms.shape[0],1)\n",
    "    \n",
    "    normed_weights = W_hid * norms\n",
    "    \n",
    "    query_vect = np.zeros(len(W_hid[0]))\n",
    "    for word in positive:\n",
    "        query_vect += normed_weights[word2index[word]]\n",
    "    for word in negative:\n",
    "        query_vect -= normed_weights[word2index[word]]\n",
    "    \n",
    "    scores = collections.Counter()\n",
    "    for word,index in word2index.items():\n",
    "        raw_difference = W_hid[index] - query_vect\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -np.sqrt(sum(squared_difference))\n",
    "        \n",
    "    return scores.most_common(10)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(['actor','woman'],['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(['terrible','good'],['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(['elizabeth','man'],['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_vec(xv, Wh, Wo):\n",
    "    \"\"\"Forward pass, input is full vector and not indices\n",
    "    \n",
    "    Params:\n",
    "        xv - sparse vector, e.g. [[0, 0, 1, 0, 1, ...]]\n",
    "        Wh - weights hidden\n",
    "        Wo - weights output\n",
    "    \"\"\"\n",
    "    assert xv.ndim == 2\n",
    "    z_hid = xv @ Wh\n",
    "    z_out = z_hid @ Wo\n",
    "    y_hat = sigmoid(z_out)\n",
    "    return y_hat, z_out, z_hid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical gradient check for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(xv, y, Wh, Wo):\n",
    "    \"\"\"Params:\n",
    "    xv - sparse vector [0, 0, 1, 0, 1, 0, ...]\n",
    "    \"\"\"\n",
    "    assert xv.ndim == 2\n",
    "    assert y.ndim == 2\n",
    "    \n",
    "    eps = 1e-4\n",
    "    \n",
    "    # numerical gradient check for output layer\n",
    "    ngrad_Wo = np.zeros_like(Wo)\n",
    "    for r in range(Wo.shape[0]):\n",
    "        for c in range(Wo.shape[1]):\n",
    "            W_min = Wo.copy()\n",
    "            W_pls = Wo.copy()\n",
    "            W_min[r, c] -= eps\n",
    "            W_pls[r, c] += eps\n",
    "            \n",
    "            y_hat_pls, _, _ = forward_vec(xv, Wh, W_pls)\n",
    "            l_pls = loss(y, y_hat_pls)\n",
    "            y_hat_min, _, _ = forward_vec(xv, Wh, W_min)\n",
    "            l_min = loss(y, y_hat_min)\n",
    "\n",
    "            ngrad_Wo[r, c] = (l_pls - l_min) / (eps * 2)\n",
    "    \n",
    "    # numerical gradient check for hidden layer\n",
    "    ngrad_Wh = np.zeros_like(Wh)\n",
    "    _, idx_nonzero = np.nonzero(xv)\n",
    "    for r in idx_nonzero:\n",
    "        for c in range(Wh.shape[1]):\n",
    "            W_min = Wh.copy()\n",
    "            W_pls = Wh.copy()\n",
    "            W_min[r, c] -= eps\n",
    "            W_pls[r, c] += eps\n",
    "\n",
    "            y_hat_pls, _, _ = forward_vec(xv, W_pls, Wo)\n",
    "            l_pls = loss(y, y_hat_pls)\n",
    "            y_hat_min, _, _ = forward_vec(xv, W_min, Wo)\n",
    "            l_min = loss(y, y_hat_min)\n",
    "\n",
    "            ngrad_Wh[r, c] = (l_pls - l_min) / (eps * 2)\n",
    "            \n",
    "    return ngrad_Wo, ngrad_Wh     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build small neural network to do gradient checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_in = 10\n",
    "N_hid = 8\n",
    "N_out = 12\n",
    "np.random.seed(1)\n",
    "W_hid = np.random.normal(0, N_in**-.5, [N_in, N_hid])\n",
    "W_out = np.random.normal(0, N_hid**-.5, [N_hid, N_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "for i in range(10):\n",
    "\n",
    "    xx = np.random.randint(0, 2, size=[1, N_in])\n",
    "    yy = np.random.normal(0, 1, [1, N_out])\n",
    "    xi = np.nonzero(xx)[1]\n",
    "    yi = np.random.choice(range(N_out), size=5, replace=False)\n",
    "\n",
    "    temp_yi = np.random.choice(range(N_out), size=5, replace=False)\n",
    "    temp_yy = yy[:,temp_yi]\n",
    "\n",
    "    dWo, dWh = backward(xi, temp_yi, temp_yy, W_hid, W_out, None, mode='return_gradients')\n",
    "\n",
    "    y_hat, z_out, z_hid = forward_vec(xx, W_hid, W_out)\n",
    "    y_hat[:,temp_yi] = yy[:,temp_yi]\n",
    "    ngWo, ngWh = numerical_gradient(xx, y_hat, W_hid, W_out)\n",
    "\n",
    "    assert np.allclose(ngWo, dWo)\n",
    "    assert np.allclose(ngWh, dWh)\n",
    "    \n",
    "    print(np.max(np.abs(ngWh-dWh)), np.max(np.abs(ngWo-dWo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
