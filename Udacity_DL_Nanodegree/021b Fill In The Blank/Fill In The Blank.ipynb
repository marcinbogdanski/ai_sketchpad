{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../021 Sentiment Analysis - Trask/reviews.txt','r') as f:\n",
    "    reviews = list(map(str.strip, f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_all = collections.Counter()  # how many times each word occurs is WHOLE DATASET \n",
    "for review in reviews:\n",
    "    for word in review.split():\n",
    "        counter_all[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74073\n"
     ]
    }
   ],
   "source": [
    "review_vocab = set(counter_all.keys())\n",
    "review_vocab_size = len(review_vocab)\n",
    "print(review_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "index2word = {}\n",
    "for i, word in enumerate(review_vocab):\n",
    "    word2index[word] = i\n",
    "    index2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_indices(review):\n",
    "    res = []\n",
    "    for word in review.split():\n",
    "        res.append(word2index[word])\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "for review in reviews:\n",
    "    review_as_ints = review_to_indices(review)\n",
    "    for i in range(2, len(review_as_ints)-2):\n",
    "        inputs.append(review_as_ints[[i-2, i-1, i+1, i+2]])\n",
    "        targets.append(review_as_ints[i])\n",
    "inputs = np.array(inputs)\n",
    "targets = np.expand_dims(targets, -1)  # ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk:   the quirky insane robin williams\n",
      "inputs:  the quirky        robin williams\n",
      "target:             insane\n"
     ]
    }
   ],
   "source": [
    "n_rev = 10\n",
    "n_word = 12\n",
    "\n",
    "# find location\n",
    "print('chunk:  ', ' '.join(reviews[n_rev].split()[n_word-2:n_word+3]))\n",
    "\n",
    "# find index in target array\n",
    "lens = np.array([len(x.split()) for x in reviews])\n",
    "ii = np.sum(lens[0:n_rev]-4) + n_word-2\n",
    "\n",
    "print('inputs: ', index2word[inputs[ii, 0]],\n",
    "                 index2word[inputs[ii, 1]],\n",
    "                 '      ',\n",
    "                 index2word[inputs[ii, 2]],\n",
    "                 index2word[inputs[ii, 3]])\n",
    "print('target:            ', index2word[targets[ii, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(xi, yi, Wh, Wo):\n",
    "    \"\"\"Params:\n",
    "        xi - indices [1234, 2345, ... ]\n",
    "        yi - indices [9876, 8765, ... ] - target words, including correct target\n",
    "    \"\"\"\n",
    "    assert xi.ndim == 1\n",
    "    assert yi.ndim == 1\n",
    "    \n",
    "    z_hid = np.sum(Wh[xi], axis=0, keepdims=True)\n",
    "    # do not do hidden activation\n",
    "    z_out = z_hid @ Wo[:,yi]\n",
    "    y_hat = sigmoid(z_out)\n",
    "    \n",
    "    return y_hat, z_out, z_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(xi, yi, y, Wh, Wo, lr):\n",
    "    \"\"\"Params:\n",
    "        xi - indices [1234, 2345]\n",
    "    \"\"\"\n",
    "    assert xi.ndim == 1\n",
    "    assert yi.ndim == 1\n",
    "    assert y.ndim == 2\n",
    "    \n",
    "    y_hat, z_out, z_hid = forward(xi, yi, Wh, Wo)\n",
    "    \n",
    "    ro_out = -(y-y_hat) * sigmoid_deriv(z_out)  # scalar\n",
    "    del_Wo_i = np.dot(z_hid.T, ro_out)\n",
    "    # Wo[:,yi] += -lr * del_Wo_i\n",
    "    \n",
    "    ro_hid = np.dot(ro_out, Wo[:,yi].T)      # 1 x hid_n\n",
    "        \n",
    "    # Optional: numerical gradient check\n",
    "#     xv = np.zeros([1, len(Wh)])           \n",
    "#     xv[0,xi] = 1\n",
    "#     del_Wh = np.zeros_like(Wh)\n",
    "#     del_Wh[xi] = ro_hid\n",
    "#     ngrad_Wo, ngrad_Wh = ngrad(xv, y, Wh, Wo)\n",
    "#     if not np.allclose(del_Wo, ngrad_Wo):\n",
    "#         raise ValueError('Gradient check fail output')\n",
    "#     if not np.allclose(del_Wh, ngrad_Wh):\n",
    "#         raise ValueError('Gradient check fail hidden')\n",
    "#     print('ngrad OK')\n",
    "        \n",
    "        \n",
    "#     Wh[xi] += -lr * ro_hid\n",
    "#     Wo[:,yi] += -lr * del_Wo_i\n",
    "#     return y_hat\n",
    "    \n",
    "    del_Wh = np.zeros_like(Wh)\n",
    "    del_Wh[xi] = ro_hid\n",
    "    \n",
    "    del_Wo = np.zeros_like(Wo)\n",
    "    del_Wo[:,yi] = del_Wo_i\n",
    "\n",
    "    return del_Wo, del_Wh\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array(range(9)).reshape([3,3])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 6],\n",
       "       [5, 6],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAi = np.array([[5,6], [5,6], [5,6]])\n",
    "dAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_in = 10\n",
    "N_hid = 8\n",
    "N_out = 12\n",
    "np.random.seed(1)\n",
    "W_hid = np.random.normal(0, N_in**-.5, [N_in, N_hid])\n",
    "W_out = np.random.normal(0, N_hid**-.5, [N_hid, N_out])\n",
    "xx = np.random.randint(0, 2, size=[1, N_in])\n",
    "yy = np.random.normal(0, 1, [1, N_out])\n",
    "xi = np.nonzero(xx)[1]\n",
    "yi = np.array(range(N_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx [[0 1 0 1 1 1 1 1 0 1]]\n",
      "xi [1 3 4 5 6 7 9]\n",
      "yy [[ 0.53317198 -0.58513304 -0.56053692  0.14077318  1.76760095 -0.78777374\n",
      "  -0.06172207  0.23842679  0.07477245  0.33830476  0.27807663  1.22396588]]\n",
      "yi [ 0  1  2  3  4  5  6  7  8  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "print('xx', xx)\n",
    "print('xi', xi)\n",
    "print('yy', yy)\n",
    "print('yi', yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 5.551115123125783e-17 0.0\n",
      "0.0 5.551115123125783e-17 0.0\n",
      "5.551115123125783e-17 1.1102230246251565e-16 0.0\n",
      "5.551115123125783e-17 1.1102230246251565e-16 0.0\n",
      "0.0 1.1102230246251565e-16 0.0\n",
      "5.551115123125783e-17 1.6653345369377348e-16 0.0\n",
      "5.551115123125783e-17 1.1102230246251565e-16 0.0\n",
      "5.551115123125783e-17 1.1102230246251565e-16 0.0\n",
      "0.0 1.1102230246251565e-16 0.0\n",
      "5.551115123125783e-17 1.1102230246251565e-16 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass against non-index version\n",
    "np.random.seed(1)\n",
    "\n",
    "for i in range(10):\n",
    "    xx = np.random.randint(0, 2, size=[1, N_in])\n",
    "    xi = np.nonzero(xx)[1]\n",
    "    yi = np.random.choice(range(N_out), size=5, replace=False)\n",
    "\n",
    "    y_hat, z_out, z_hid = forward_vec(xx, W_hid, W_out)\n",
    "    y_hati, z_outi, z_hidi = forward(xi, yi, W_hid, W_out)\n",
    "\n",
    "    assert np.allclose(y_hat[:,yi], y_hati)\n",
    "    assert np.allclose(z_out[:,yi], z_outi)\n",
    "    assert np.allclose(z_hid, z_hidi)\n",
    "    \n",
    "    print(np.max(np.abs(y_hat[:,yi]-y_hati)), np.max(np.abs(z_out[:,yi]-z_outi)), np.max(np.abs(z_hid-z_hidi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngWo, ngWh = ngrad(xx, yy, W_hid, W_out)\n",
    "dWo, dWh = backward(xi, yi, yy, W_hid, W_out, None)\n",
    "assert np.allclose(ngWo, dWo)\n",
    "assert np.allclose(ngWh, dWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9941970297310263e-10 8.94230023362752e-10\n",
      "9.992788541079989e-11 3.041814022886058e-10\n",
      "2.3733581766549605e-10 4.022676636239453e-10\n",
      "4.238850243032388e-11 9.43689570931383e-12\n",
      "1.7664786300386481e-10 2.0341855977434875e-10\n",
      "1.0316739129656582e-10 3.1466040883998403e-10\n",
      "4.986805413054185e-11 3.680737936662126e-10\n",
      "4.5849102292550015e-11 1.6478229891703222e-10\n",
      "1.237336344495077e-10 2.739610482915822e-10\n",
      "1.2854056707922723e-10 1.6746970477043988e-10\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "np.random.seed(1)\n",
    "for i in range(10):\n",
    "\n",
    "    xx = np.random.randint(0, 2, size=[1, N_in])\n",
    "    yy = np.random.normal(0, 1, [1, N_out])\n",
    "    xi = np.nonzero(xx)[1]\n",
    "    yi = np.random.choice(range(N_out), size=5, replace=False)\n",
    "\n",
    "    temp_yi = np.random.choice(range(N_out), size=5, replace=False)\n",
    "    temp_yy = yy[:,temp_yi]\n",
    "\n",
    "    dWo, dWh = backward(xi, temp_yi, temp_yy, W_hid, W_out, None)\n",
    "\n",
    "    y_hat, z_out, z_hid = forward_vec(xx, W_hid, W_out)\n",
    "    y_hat[:,temp_yi] = yy[:,temp_yi]\n",
    "    ngWo, ngWh = ngrad(xx, y_hat, W_hid, W_out)\n",
    "\n",
    "    assert np.allclose(ngWo, dWo)\n",
    "    assert np.allclose(ngWh, dWh)\n",
    "    \n",
    "    print(np.max(np.abs(ngWh-dWh)), np.max(np.abs(ngWo-dWo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653]])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.01122393, -0.02214843,  0.0408137 , -0.0321047 ,  0.02919282,\n",
       "         0.10094825,  0.26497728,  0.09398653]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01597556,  0.12691019,  0.12848776,  0.04808367, -0.08283452,\n",
       "         0.12665826,  0.02648427,  0.02151276,  0.05937474,  0.01601468,\n",
       "         0.0080473 , -0.09041316],\n",
       "       [-0.00264196, -0.0209878 , -0.02124869, -0.00795185,  0.01369877,\n",
       "        -0.02094613, -0.00437984, -0.00355768, -0.00981911, -0.00264843,\n",
       "        -0.00133082,  0.01495209],\n",
       "       [ 0.0089963 ,  0.07146683,  0.07235521,  0.02707732, -0.04664653,\n",
       "         0.07132496,  0.01491407,  0.01211446,  0.03343565,  0.00901834,\n",
       "         0.00453167, -0.05091429],\n",
       "       [-0.03069609, -0.24385034, -0.24688156, -0.0923899 ,  0.15916157,\n",
       "        -0.24336628, -0.05088795, -0.04133548, -0.11408502, -0.03077126,\n",
       "        -0.01546241,  0.17372349],\n",
       "       [-0.04223405, -0.3355082 , -0.33967878, -0.12711718,  0.21898683,\n",
       "        -0.33484218, -0.07001559, -0.05687255, -0.15696701, -0.04233748,\n",
       "        -0.02127438,  0.23902223],\n",
       "       [ 0.01614138,  0.12822747,  0.12982143,  0.04858276, -0.08369431,\n",
       "         0.12797293,  0.02675917,  0.02173605,  0.05999103,  0.01618091,\n",
       "         0.00813083, -0.09135162],\n",
       "       [ 0.03093162,  0.24572146,  0.24877594,  0.09309882, -0.16038285,\n",
       "         0.24523368,  0.05127842,  0.04165265,  0.11496042,  0.03100737,\n",
       "         0.01558106, -0.1750565 ],\n",
       "       [ 0.02621838,  0.20827938,  0.21086843,  0.07891279, -0.13594434,\n",
       "         0.20786593,  0.04346482,  0.03530578,  0.0974432 ,  0.02628259,\n",
       "         0.01320688, -0.14838208]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dWo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  6,  8],\n",
       "       [ 3,  9, 11],\n",
       "       [ 6, 12, 14]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_vec(xv, Wh, Wo):\n",
    "    \"\"\"Params:\n",
    "        xv - sparse [[0, 0, 1, 0, 1, ...]]\n",
    "        Wh - weights hidden\n",
    "        Wo - weights output\n",
    "    \"\"\"\n",
    "    assert xv.ndim == 2\n",
    "    z_hid = xv @ Wh\n",
    "    # do not do hidden activation\n",
    "    z_out = z_hid @ Wo\n",
    "    y_hat = sigmoid(z_out)\n",
    "    return y_hat, z_out, z_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrad(xv, y, Wh, Wo):\n",
    "    \"\"\"Params:\n",
    "    xv - sparse vector [0, 0, 1, 0, 1, 0, ...]\n",
    "    \"\"\"\n",
    "    assert xv.ndim == 2\n",
    "    assert y.ndim == 2\n",
    "    \n",
    "    eps = 1e-4\n",
    "    \n",
    "    # numerical gradient check output\n",
    "    ngrad_Wo = np.zeros_like(Wo)\n",
    "    for r in range(Wo.shape[0]):\n",
    "        for c in range(Wo.shape[1]):\n",
    "            W_min = Wo.copy()\n",
    "            W_pls = Wo.copy()\n",
    "            W_min[r, c] -= eps\n",
    "            W_pls[r, c] += eps\n",
    "\n",
    "            l_pls = loss(xv, y, Wh, W_pls)\n",
    "            l_min = loss(xv, y, Wh, W_min)\n",
    "\n",
    "            ngrad_Wo[r, c] = (l_pls - l_min) / (eps * 2)\n",
    "    \n",
    "    # numerical gradient check hidden\n",
    "    ngrad_Wh = np.zeros_like(Wh)\n",
    "    _, idx_nonzero = np.nonzero(xv)\n",
    "    for r in idx_nonzero: #range(self.weights_0_1.shape[0]):\n",
    "        for c in range(Wh.shape[1]):\n",
    "            W_min = Wh.copy()\n",
    "            W_pls = Wh.copy()\n",
    "            W_min[r, c] -= eps\n",
    "            W_pls[r, c] += eps\n",
    "\n",
    "            l_pls = loss(xv, y, W_pls, Wo)\n",
    "            l_min = loss(xv, y, W_min, Wo)\n",
    "\n",
    "            ngrad_Wh[r, c] = (l_pls - l_min) / (eps * 2)\n",
    "            \n",
    "    return ngrad_Wo, ngrad_Wh     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(xv, y, Wh, Wo):\n",
    "    y_hat, z_out, z_hid = forward_vec(xv, Wh, Wo)\n",
    "    return .5 * np.sum((y-y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(lens[0:n_rev]-4) + n_word-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in enumerate(targets[150:180,0]):\n",
    "    print(i+150, index2word[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word[inputs[0][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word[targets[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_as_ints = review_to_indices(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_as_ints = np.array(review_as_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([0, 1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,len(A)-2):\n",
    "    print(A[[i-2,i-1,i+1,i+2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(map(lambda x:(x.split(\" \")),reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcnt = collections.Counter()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        wordcnt[word] -= 1\n",
    "vocab = list(set(map(lambda x:x[0],wordcnt.most_common())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(map(lambda x:(x.split(\" \")),raw_reviews))\n",
    "wordcnt = Counter()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        wordcnt[word] -= 1\n",
    "vocab = list(set(map(lambda x:x[0],wordcnt.most_common())))\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "concatenated = list()\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "            concatenated.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(sent_indices)\n",
    "concatenated = np.array(concatenated)\n",
    "np.random.shuffle(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(np.random.rand(negative)*len(concatenated)).astype('int').tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated[(np.random.rand(negative)*len(concatenated)).astype('int').tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "f = open('../021 Sentiment Analysis - Trask/reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list(map(lambda x:(x.split(\" \")),raw_reviews))\n",
    "wordcnt = Counter()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        wordcnt[word] -= 1\n",
    "vocab = list(set(map(lambda x:x[0],wordcnt.most_common())))\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "concatenated = list()\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "            concatenated.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(sent_indices)\n",
    "concatenated = np.array(concatenated)\n",
    "random.shuffle(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, iterations = (0.05, 2)\n",
    "hidden_size,window,negative = (50,2,5)\n",
    "\n",
    "weights_0_1 = (np.random.rand(len(vocab),hidden_size) - 0.5) * 0.2\n",
    "weights_1_2 = np.random.rand(len(vocab),hidden_size)*0\n",
    "\n",
    "layer_2_target = np.zeros(negative+1)\n",
    "layer_2_target[0] = 1\n",
    "\n",
    "def similar(target='beautiful'):\n",
    "    target_index = word2index[target]\n",
    "\n",
    "    scores = Counter()\n",
    "    for word,index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.random.rand(negative) * len(concatenated)).astype('int').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit len(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.random.randint(0, len(concatenated), size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit (np.random.rand(negative) * len(concatenated)).astype('int').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_5 = np.random.randint(0, len(concatenated), size=5)\n",
    "review = input_dataset[0]\n",
    "target_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[review[target_i]]+list(concatenated[rand_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for rev_i, review in enumerate(input_dataset * iterations):\n",
    "        for target_i in range(len(review)):\n",
    "\n",
    "            # since it's really expensive to predict every vocabulary\n",
    "            # we're only going to predict a random subset\n",
    "            ##rand_5 = (np.random.rand(negative) * len(concatenated)).astype('int').tolist()\n",
    "            rand_5 = np.random.randint(0, len(concatenated), size=5)\n",
    "            target_samples = [review[target_i]]+list(concatenated[rand_5])\n",
    "\n",
    "            left_context = review[max(0,target_i-window):target_i]\n",
    "            right_context = review[target_i+1:min(len(review),target_i+window)]\n",
    "\n",
    "            layer_1 = np.mean(weights_0_1[left_context+right_context],axis=0)\n",
    "            layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n",
    "            layer_2_delta = layer_2 - layer_2_target\n",
    "            layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\n",
    "\n",
    "            weights_0_1[left_context+right_context] -= layer_1_delta * alpha\n",
    "            weights_1_2[target_samples] -= np.outer(layer_2_delta,layer_1)*alpha\n",
    "\n",
    "#         if(rev_i % 250 == 0):\n",
    "#             sys.stdout.write('\\rProgress:' + str(rev_i/float(len(input_dataset) * iterations)) + \"   \" + str(similar('terrible')))\n",
    "#         sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset) * iterations)))\n",
    "        \n",
    "        if rev_i >= 100:\n",
    "            break\n",
    "#     print(similar('terrible'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f train train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(positive=['terrible','good'],negative=['bad']):\n",
    "    \n",
    "    norms = np.sum(weights_0_1 * weights_0_1,axis=1)\n",
    "    norms.resize(norms.shape[0],1)\n",
    "    \n",
    "    normed_weights = weights_0_1 * norms\n",
    "    \n",
    "    query_vect = np.zeros(len(weights_0_1[0]))\n",
    "    for word in positive:\n",
    "        query_vect += normed_weights[word2index[word]]\n",
    "    for word in negative:\n",
    "        query_vect -= normed_weights[word2index[word]]\n",
    "    \n",
    "    scores = Counter()\n",
    "    for word,index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - query_vect\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -np.sqrt(sum(squared_difference))\n",
    "        \n",
    "    return scores.most_common(10)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(['terrible','good'],['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(['elizabeth','he'],['she'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
