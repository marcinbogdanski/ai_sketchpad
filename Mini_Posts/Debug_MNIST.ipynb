{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pdb\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('MNIST-Dataset', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_raw = mnist.train.images\n",
    "train_y_raw = mnist.train.labels\n",
    "valid_x_raw = mnist.validation.images\n",
    "valid_y_raw = mnist.validation.labels\n",
    "test_x_raw = mnist.test.images\n",
    "test_y_raw = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist(data_x, data_y, n):\n",
    "    fig = plt.figure(figsize=[16,9])\n",
    "    for i in range(n):\n",
    "        ax = fig.add_subplot(n//8, 8, i+1)\n",
    "        ax.imshow(data_x[i].reshape([28,28]))\n",
    "        ax.axis('off')\n",
    "        idx = int(np.nonzero(data_y[i])[0])\n",
    "        ax.set_title(idx)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_mnist(train_x_raw, train_y_raw, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(valid_x_raw, valid_y_raw, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(test_x_raw, test_y_raw, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_x_raw[0:100].flatten(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train mean:', train_x_raw.mean(), 'std', train_x_raw.std())\n",
    "print('valid mean:', valid_x_raw.mean(), 'std', valid_x_raw.std())\n",
    "print('test mean:', test_x_raw.mean(), 'std', test_x_raw.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_mean = train_x_raw.mean()\n",
    "tx_std = train_x_raw.std()\n",
    "train_x = (train_x_raw - tx_mean)/tx_std\n",
    "valid_x = (valid_x_raw - tx_mean)/tx_std\n",
    "test_x = (test_x_raw - tx_mean)/tx_std\n",
    "train_y = train_y_raw\n",
    "valid_y = valid_y_raw\n",
    "test_y = test_y_raw\n",
    "print('train mean:', train_x.mean(), 'std', train_x.std())\n",
    "print('valid mean:', valid_x.mean(), 'std', valid_x.std())\n",
    "print('test mean:', test_x.mean(), 'std', test_x.std())\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(valid_x.shape, valid_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(train_x, train_y, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast import/restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pdb\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST-Dataset', one_hot=True)\n",
    "\n",
    "train_x_raw = mnist.train.images\n",
    "train_y_raw = mnist.train.labels\n",
    "valid_x_raw = mnist.validation.images\n",
    "valid_y_raw = mnist.validation.labels\n",
    "test_x_raw = mnist.test.images\n",
    "test_y_raw = mnist.test.labels\n",
    "\n",
    "# Preprocess\n",
    "tx_mean = train_x_raw.mean()\n",
    "tx_std = train_x_raw.std()\n",
    "train_x = (train_x_raw - tx_mean)/tx_std\n",
    "valid_x = (valid_x_raw - tx_mean)/tx_std\n",
    "test_x = (test_x_raw - tx_mean)/tx_std\n",
    "train_y = train_y_raw\n",
    "valid_y = valid_y_raw\n",
    "test_y = test_y_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1. - np.tanh(x)**2\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softssign(x, deriv=False):\n",
    "    if deriv:\n",
    "        dd = 1 + np.abs(x)\n",
    "        return (dd - x*np.sign(x)) / dd**2\n",
    "    return x / (1+np.abs(x))\n",
    "\n",
    "def relu(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1. * (x>0)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def lrelu(x, deriv=False):\n",
    "    if deriv:\n",
    "        dx = np.ones_like(x)\n",
    "        dx[x < 0] = 0.01\n",
    "        return dx\n",
    "    return np.where(x > 0, x, x * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def MSE(y, y_hat):\n",
    "    assert y.ndim == 2\n",
    "    assert y_hat.ndim == 2\n",
    "    \n",
    "    # avg over batch, sum over outputs (inner)\n",
    "    return .5 * np.mean(np.sum((y-y_hat)**2, axis=-1))\n",
    "    \n",
    "    # no innner sum, becouse only one output\n",
    "    return np.mean((y-y_hat)**2)\n",
    "\n",
    "def acc(y, y_hat):\n",
    "    return np.mean(np.argmax(y_hat, axis=-1)==np.argmax(y, axis=-1))\n",
    "\n",
    "def fwd(x, W_hid, W_out, act_fun, ret=False):\n",
    "    assert x.ndim == 2\n",
    "    z_hid = x @ W_hid\n",
    "    h_hid = act_fun(z_hid)  # hidden output\n",
    "\n",
    "    z_out = h_hid @ W_out\n",
    "    y_hat = sigmoid(z_out)  # SIGMOID!\n",
    "\n",
    "    if ret:\n",
    "        return y_hat, z_hid, h_hid, z_out\n",
    "    return y_hat\n",
    "\n",
    "def backprop(x, y, W_hid, W_out, act_fun):\n",
    "    assert x.ndim == 2\n",
    "    assert y.ndim == 2\n",
    "    \n",
    "    y_hat, z_hid, h_hid, z_out = fwd(x, W_hid, W_out, act_fun, ret=True)\n",
    "    \n",
    "    ro_out = (y-y_hat) * -1 * sigmoid(z_out, deriv=True)  # SIGMOID\n",
    "    dW_out = h_hid.T @ ro_out / len(x)\n",
    "    \n",
    "    ro_hid = (ro_out @ W_out.T) * act_fun(z_hid, deriv=True)\n",
    "    dW_hid = x.T @ ro_hid / len(x)\n",
    "    \n",
    "    return dW_hid, dW_out\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical gradient check (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrad(x, y, W_hid, W_out, act_fun):\n",
    "    \n",
    "    eps = 1e-6\n",
    "    \n",
    "    gW_hid = np.zeros_like(W_hid)\n",
    "    for r in range(W_hid.shape[0]):\n",
    "        for c in range(W_hid.shape[1]):\n",
    "            W_hid_plus = W_hid.copy()\n",
    "            W_hid_minus = W_hid.copy()\n",
    "            W_hid_plus[r,c] += eps\n",
    "            W_hid_minus[r,c] -= eps\n",
    "            loss_plus = MSE(y, fwd(x, W_hid_plus, W_out, act_fun))\n",
    "            loss_minus = MSE(y, fwd(x, W_hid_minus, W_out, act_fun))\n",
    "            gW_hid[r,c] = (loss_plus-loss_minus) / (2*eps)\n",
    "\n",
    "    gW_out = np.zeros_like(W_out)\n",
    "    for r in range(W_out.shape[0]):\n",
    "        for c in range(W_out.shape[1]):\n",
    "            W_out_plus = W_out.copy()\n",
    "            W_out_minus = W_out.copy()\n",
    "            W_out_plus[r,c] += eps\n",
    "            W_out_minus[r,c] -= eps\n",
    "            loss_plus = MSE(y, fwd(x, W_hid, W_out_plus, act_fun))\n",
    "            loss_minus = MSE(y, fwd(x, W_hid, W_out_minus, act_fun))\n",
    "            gW_out[r,c] = (loss_plus-loss_minus) / (2*eps)\n",
    "    return gW_hid, gW_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW_hid, dW_out = backprop(train_x[0:3], train_y[0:3], W_hid, W_out, act_fun)\n",
    "ngW_hid, ngW_out = ngrad(train_x[0:3], train_y[0:3], W_hid, W_out, act_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(dW_hid, ngW_hid)\n",
    "assert np.allclose(dW_out, ngW_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop - with traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 784\n",
    "n_hid = 128 # 12\n",
    "n_out = 10\n",
    "lr = 0.01  # 0.55\n",
    "\n",
    "n_batch = 100\n",
    "act_fun = sigmoid\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize weights\n",
    "W_hid = np.random.normal(0.0, n_in**-.5, [n_in, n_hid])\n",
    "W_out = np.random.normal(0.0, n_hid**-.5, [n_hid, n_out])\n",
    "\n",
    "# W_hid = np.random.uniform(0.0, .01, [n_in, n_hid])\n",
    "# W_out = np.random.uniform(0.0, .01, [n_hid, n_out])\n",
    "\n",
    "losses = {'batch':[], 'train':[], 'valid':[]}\n",
    "accurs = {'batch':[], 'train':[], 'valid':[]}\n",
    "traces = {'z_hid':[], 'z_out':[],\n",
    "          #'dW_hid':[], 'dW_out':[],\n",
    "          'W_hid':[], 'W_out':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train_x)//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_ = 0\n",
    "train_i = np.array(range(len(train_x)))\n",
    "for e in range(10):\n",
    "    print(e)\n",
    "    np.random.shuffle(train_i)\n",
    "    \n",
    "    for k, v in traces.items():\n",
    "        v.append([])\n",
    "    \n",
    "    for i in range(0, len(train_x), n_batch):\n",
    "        # print(i,',', end='')\n",
    "        # Get 128 sized batch, both as 2d arrays   \n",
    "        batch = train_i[i:i+n_batch]\n",
    "        x = train_x[batch]\n",
    "        y = train_y[batch]\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat, z_hid, _, z_out = fwd(x, W_hid, W_out, act_fun, ret=True)\n",
    "        \n",
    "        # Calc batch loss (before update!)\n",
    "        losses['batch'].append(MSE(y, y_hat))\n",
    "        accurs['batch'].append(acc(y, y_hat))\n",
    "\n",
    "        # Backpropagation\n",
    "        dW_hid, dW_out = backprop(x, y, W_hid, W_out, act_fun)\n",
    "        W_hid += -lr * dW_hid\n",
    "        W_out += -lr * dW_out\n",
    "\n",
    "        # Calc full loss, usually only every epoch or so\n",
    "#         if i == 0:\n",
    "#             train_y_hat = fwd(train_x, W_hid, W_out, act_fun)\n",
    "#             train_loss = MSE(train_y, train_y_hat)\n",
    "#             losses['train'].append(train_loss)\n",
    "\n",
    "#         valid_y_hat = fwd(valid_x, W_hid, W_out, act_fun)\n",
    "#         valid_loss = MSE(valid_y, valid_y_hat)\n",
    "#         losses['valid'].append(valid_loss)\n",
    "\n",
    "        # Trace\n",
    "#         traces['z_hid'][-1] += list(z_hid)\n",
    "#         traces['z_out'][-1] += list(z_out)\n",
    "        \n",
    "        traces['z_hid'][-1].append(z_hid)\n",
    "        traces['z_out'][-1].append(z_out)\n",
    "        \n",
    "        if ti_ % 10 == 0:\n",
    "        #traces['dW_hid'][-1].append(dW_hid)\n",
    "        #traces['dW_out'][-1].append(dW_out)\n",
    "            traces['W_hid'][-1].append(W_hid.copy())\n",
    "            traces['W_out'][-1].append(W_out.copy())\n",
    "        \n",
    "        ti_ += 1\n",
    "\n",
    "tr_z_hid = np.array(traces['z_hid'])\n",
    "tr_z_out = np.array(traces['z_out'])\n",
    "#tr_dW_hid = np.array(traces['dW_hid'])\n",
    "#tr_dW_out = np.array(traces['dW_out'])\n",
    "tr_W_hid = np.array(traces['W_hid'])\n",
    "tr_W_out = np.array(traces['W_out'])\n",
    "\n",
    "print('tr_z_hid', tr_z_hid.shape)\n",
    "print('tr_z_out', tr_z_out.shape)\n",
    "#print('tr_dW_hid', tr_dW_hid.shape)\n",
    "#print('tr_dW_out', tr_dW_out.shape)\n",
    "print('tr_W_hid', tr_W_hid.shape)\n",
    "print('tr_W_out', tr_W_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final train loss:', losses['train'][-1])\n",
    "print('Final valid loss:', losses['valid'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_hat = fwd(train_x, W_hid, W_out, act_fun)\n",
    "train_loss = MSE(train_y, train_y_hat)\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y_hat = fwd(valid_x, W_hid, W_out, act_fun)\n",
    "valid_loss = MSE(valid_y, valid_y_hat)\n",
    "valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "ax.plot(losses['batch'], label='Mini-Batch loss')\n",
    "#ax.plot(losses['train'], label='Training loss')\n",
    "ax.plot(losses['valid'], label='Validation loss')\n",
    "ax.plot(accurs['batch'], label='Mini-Batch accuracy', color='red')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "ax.plot(losses['batch'], label='Mini-Batch loss')\n",
    "#ax.plot(losses['train'], label='Training loss')\n",
    "ax.plot(losses['valid'], label='Validation loss')\n",
    "ax.plot(accurs['batch'], label='Mini-Batch accuracy', color='red')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = fwd(train_x, W_hid, W_out, act_fun)\n",
    "y_correct = np.argmax(train_y, axis=-1) == np.argmax(y_hat, axis=-1)\n",
    "classes_correct = []\n",
    "classes_all = []\n",
    "for i in range(10):\n",
    "    is_y_class_i = y_correct * (np.argmax(train_y, axis=-1)==i)\n",
    "    nb_correct_class_i = np.sum(is_y_class_i)\n",
    "    classes_correct.append(nb_correct_class_i)\n",
    "    classes_all.append(np.count_nonzero(np.argmax(train_y,axis=-1)==i))\n",
    "classes_correct = np.array(classes_correct)\n",
    "classes_all = np.array(classes_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(10), classes_all, label='All Member')\n",
    "plt.bar(range(10), classes_correct, label='Correctly Predicted')\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, n):\n",
    "    return np.array([ np.mean(x[max(i-n+1, 0): i+1]) for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_std(x, n):\n",
    "    return np.array([ np.std(x[max(i-n+1, 0): i+1]) for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot output neuron weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_W_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = 9\n",
    "\n",
    "ne, ni, nw, nn = tr_W_out.shape\n",
    "\n",
    "fig = plt.figure(figsize=[16,6])\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(tr_W_out.reshape([ne*ni,nw,nn])[:,:,neuron])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del ne, ni, nw, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot hidden neuron weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_neuron = 0\n",
    "end_neuron = 12\n",
    "start_weight = 0\n",
    "end_weight = 50\n",
    "\n",
    "fig = plt.figure(figsize=[16,280])\n",
    "ne, ni, nw, nn = tr_W_hid.shape\n",
    "for i in range(start_neuron, end_neuron):\n",
    "    ax = fig.add_subplot(80,3,i+1)\n",
    "    tmp = tr_W_hid.reshape(ne*ni, nw, nn)\n",
    "    ax.plot(tmp[:,start_weight:end_weight,i], alpha=0.5)\n",
    "    ax.set_title('Neuron #'+str(i))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[16,180])\n",
    "ne, ni, nw, nn = tr_W_hid.shape\n",
    "for i in range(nn):\n",
    "    ax = fig.add_subplot(80,3,i+1)\n",
    "    tmp = np.reshape(tr_W_hid, (ne*ni, nw, nn))\n",
    "    ax.plot(tmp[:,:,i], alpha=0.5)\n",
    "    ax.set_title('Neuron #'+str(i))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_histogram(data, funct=lambda x: x, color=(1,0,0,1), alpha=1, ax=None, figsize=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Params:\n",
    "        data - 2d array, dims: [epochs, samples].\n",
    "               E.g. for single neuron activations: [[ 0.1, 0.2, 0.3, ... ]  activations in epoch 0, sample 1, 2, 3...\n",
    "                                                    [ 0.2, 0.1, 0.3, ... ]  activations in epoch 1, ...\n",
    "                                                    ...\n",
    "                                                    [ 0.3, 0.1, 0.2, ... ]] activations is last training epoch\n",
    "        funct - function to apply to data before plotting, e.g. sigmoid, tanh, usually none\n",
    "    \"\"\"\n",
    "    assert data.ndim==2\n",
    "    \n",
    "    def interpolate_colors(cstart, cend, n):\n",
    "        cstart, cend = np.array(cstart), np.array(cend)\n",
    "        assert cstart.shape == (4,)\n",
    "        assert cend.shape == (4,)\n",
    "        if n == 1:  return cend    # if one step, then return end color\n",
    "\n",
    "        cols = []\n",
    "        for i in range(n):\n",
    "            step = i/(n-1)\n",
    "            cols.append( (1-step)*cstart + step*cend)\n",
    "        return np.array(cols)\n",
    "    \n",
    "    color = np.array(color)\n",
    "    color_start = np.array(color/4, dtype=float)  # transparent black\n",
    "    color_end = np.array(color)\n",
    "    colors = interpolate_colors(color_start, color_end, len(data))\n",
    "    \n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    ax.set_xlabel('value'); ax.set_ylabel('epoch'); ax.set_zlabel('n')\n",
    "    ax.view_init(30, -85)\n",
    "    # ax.set_xlim(0, 1)\n",
    "    \n",
    "    for epoch in range(len(data)):                                     # One line per epoch\n",
    "        hist, bins = np.histogram(funct(data[epoch,:]), bins=100)      # apply funct and create histogram\n",
    "        bins = (bins[:-1] + bins[1:])/2                                # center bins\n",
    "        hist = hist / np.sum(hist)\n",
    "        ax.plot(xs=bins, ys=hist,\n",
    "                zs=-epoch,\n",
    "                zdir='y', \n",
    "                color=colors[epoch])\n",
    "        if epoch == 0:\n",
    "            nb_epochs = len(data)\n",
    "            #ax.plot(xs=[-1,-1], ys=[0,0], zs=[-nb_epochs,0], zdir='y', color='k', ls='--')\n",
    "            ax.plot(xs=[0,0], ys=[0,0], zs=[-nb_epochs,0], zdir='y', color='k')\n",
    "            ax.plot(xs=[1,1], ys=[0,0], zs=[-nb_epochs,0], zdir='y', color='k', ls='--')\n",
    "        if epoch == len(data)-1:\n",
    "            ax.plot(xs=[bins[0],bins[-1]], ys=[0,0], zs=-nb_epochs, zdir='y', color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[16,9])\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ne, ni, na, nn = tr_z_out.shape\n",
    "do = tr_z_out.reshape([ne*ni,-1])\n",
    "do = act_fun(do)  # linear output!\n",
    "xx = list(range(ne*ni))\n",
    "yy = np.mean(do, axis=-1)\n",
    "err = np.std(do, axis=-1)\n",
    "line = ax.plot(xx, yy, label='Output mean/std')[0]\n",
    "ax.errorbar(xx, yy, err, alpha=0.1, ls='none', color=line.get_color())\n",
    "\n",
    "ne, ni, na, nn = tr_z_hid.shape\n",
    "dd = tr_z_hid.reshape([ne*ni,-1])\n",
    "dd = act_fun(dd)\n",
    "xx = list(range(ne*ni))\n",
    "yy = np.mean(dd, axis=-1)\n",
    "err = np.std(dd, axis=-1)\n",
    "line = ax.plot(xx, yy, label='Hidden mean/std')[0]\n",
    "ax.errorbar(xx, yy, err, alpha=0.1, ls='none', color=line.get_color())\n",
    "\n",
    "#ax.set_ylim(0,1)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[16,200])\n",
    "\n",
    "ne, ni, na, nn = tr_z_hid.shape\n",
    "tmp_act_hid = tr_z_hid.reshape([ne,ni*na,nn])\n",
    "\n",
    "for n in range(tr_z_hid.shape[-1]):\n",
    "    ax = fig.add_subplot(80, 4, n+1, projection='3d')\n",
    "    plot_3d_histogram(tmp_act_hid[::,:,n], funct=lambda x: sigmoid(x), ax=ax)\n",
    "    ax.set_title('Neuron #' + str(n))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[16,6])\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ne, ni, na, nn = tr_z_hid.shape\n",
    "dd = tr_z_hid.reshape([ne,-1])\n",
    "plot_3d_histogram(dd, funct=lambda x: x, color=(0,0,0,1), ax=ax)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "plot_3d_histogram(dd, funct=lambda x: sigmoid(x), color=(1,0,0,1), ax=ax)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[16,6])\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ne, ni, na, nn = tr_z_out.shape\n",
    "dd = tr_z_out.reshape([ne,-1])\n",
    "plot_3d_histogram(dd, funct=lambda x: x, color=(0,0,0,1), ax=ax)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "plot_3d_histogram(dd, funct=lambda x: sigmoid(x), color=(0,0,1,1), ax=ax)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[16,45])\n",
    "\n",
    "ne, ni, na, nn = tr_z_out.shape\n",
    "tmp_act_out = tr_z_out.reshape([ne,ni*na,nn])\n",
    "\n",
    "for n in range(tr_z_out.shape[-1]):\n",
    "    \n",
    "    ax = fig.add_subplot(10, 3, (n*3)+1, projection='3d')\n",
    "    plot_3d_histogram(tmp_act_out[::,:,n], funct=lambda x: x, color=(0,0,0,1), ax=ax)\n",
    "    ax.set_title('Neuron #' + str(n))\n",
    "    \n",
    "    ax = fig.add_subplot(10, 3, (n*3)+2, projection='3d')\n",
    "    plot_3d_histogram(tmp_act_out[::,:,n], funct=lambda x: sigmoid(x), color=(0,0,1,1), ax=ax)\n",
    "    ax.set_title('Neuron #' + str(n))\n",
    "    \n",
    "    ax = fig.add_subplot(10, 3, (n*3)+3)\n",
    "    hist, bins = np.histogram(train_y[:,0])\n",
    "    bins = (bins[:-1] + bins[1:])/2    # center bins\n",
    "    hist = hist / sum(hist)\n",
    "    ax.bar(bins,hist, width=0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aa_W_out = tr_W_out.reshape([-1,12,1])[1:]\n",
    "bb_W_out = tr_W_out.reshape([-1,12,1])[:-1]\n",
    "#print('aa_W_out', aa_W_out.shape)\n",
    "#print('bb_W_out', bb_W_out.shape)\n",
    "diff_W_out = aa_W_out - bb_W_out\n",
    "#print('diff_W_out', diff_W_out.shape)\n",
    "relat_out = diff_W_out / aa_W_out\n",
    "print('relat_out', relat_out.shape)\n",
    "\n",
    "fig = plt.figure(figsize=[16,9])\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in range(3): #relat_out.shape[1]):\n",
    "    to_plot = relat_out[:,i:i+1,0]\n",
    "    #print('to_plot', to_plot.shape)\n",
    "    \n",
    "    \n",
    "    line = ax.plot(to_plot, alpha=0.1)[0]\n",
    "    ax.plot(running_mean(to_plot, n=1000), color=line.get_color());\n",
    "    ax.plot(running_std(to_plot, n=1000), color=line.get_color());\n",
    "    ax.plot([0,len(to_plot)],[0.0, 0.0], linestyle='--', color='k')\n",
    "    #ax.plot([0,900],[0.01, 0.01], linestyle='--')\n",
    "    # ax.ylim(0, 0.2)\n",
    "    # ax.set_yscale('log')\n",
    "    ax.set_ylim([-.1,0.1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_W_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aa_W_out = tr_W_out[:,1:,:,:]\n",
    "bb_W_out = tr_W_out[:,:-1,:,:]\n",
    "print('aa_W_out', aa_W_out.shape)\n",
    "print('bb_W_out', bb_W_out.shape)\n",
    "diff_W_out = aa_W_out - bb_W_out\n",
    "print('diff_W_out', diff_W_out.shape)\n",
    "relat_out = diff_W_out / aa_W_out\n",
    "print('relat_out', relat_out.shape)\n",
    "\n",
    "out_weight = relat_out[:,:,0,0]\n",
    "print('out_weight', out_weight.shape)\n",
    "\n",
    "plot_3d_histogram(np.clip(out_weight, -0.1, 0.1), figsize=[16,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aa_W_out = tr_W_out.reshape([-1,12,1])[1:]\n",
    "bb_W_out = tr_W_out.reshape([-1,12,1])[:-1]\n",
    "print('aa_W_out', aa_W_out.shape)\n",
    "print('bb_W_out', bb_W_out.shape)\n",
    "diff_W_out = aa_W_out - bb_W_out\n",
    "print('diff_W_out', diff_W_out.shape)\n",
    "relat_out = diff_W_out / aa_W_out\n",
    "print('relat_out', relat_out.shape)  \n",
    "to_plot = relat_out[:,-1:,0]\n",
    "#print('to_plot', to_plot.shape)\n",
    "fig = plt.figure(figsize=[16,3])\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(to_plot, alpha=0.2);\n",
    "ax.plot(running_mean(to_plot, n=100));\n",
    "ax.plot(running_mean(np.abs(to_plot), n=100));\n",
    "ax.plot(running_std(np.abs(to_plot), n=100));\n",
    "ax.plot([0,900],[0.0, 0.0], linestyle='--')\n",
    "ax.plot([0,900],[0.01, 0.01], linestyle='--')\n",
    "# ax.ylim(0, 0.2)\n",
    "# ax.set_yscale('log')\n",
    "ax.set_ylim([-.02,0.02])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratios(weights):\n",
    "    assert weights.ndim==3\n",
    "    \n",
    "    aa_W = weights.reshape([-1,12,1])[1:]\n",
    "    bb_W = weights.reshape([-1,12,1])[:-1]\n",
    "    diff_W = bb_W - aa_W\n",
    "    relat = diff_W / aa_W\n",
    "    \n",
    "    assert relat.ndim == 3\n",
    "    return relat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = get_ratios(tr_W_out.reshape([-1,12,1]))\n",
    "to_plot = ratios[-1000:,:,0]\n",
    "\n",
    "print('to_plot', to_plot.shape)\n",
    "plt.plot(np.abs(to_plot), alpha=0.5);\n",
    "#plt.plot(running_mean(np.abs(to_plot), n=100)[100:]);\n",
    "plt.plot([0,900],[0.01, 0.01], linestyle='--')\n",
    "# plt.ylim(0, 0.2)\n",
    "plt.yscale('log')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_W_out = tr_W_out.reshape([-1,12,1])\n",
    "bb_dW_out = tr_dW_out.reshape([-1,12,1])\n",
    "print('aa_W_out', aa_W_out.shape)\n",
    "print('bb_dW_out', bb_dW_out.shape)\n",
    "relat_out = (bb_dW_out / aa_W_out)\n",
    "print('relat_out', relat_out.shape)\n",
    "to_plot = relat_out[-1000:,:,0]\n",
    "print('to_plot', to_plot.shape)\n",
    "\n",
    "plt.plot(running_mean(lr*to_plot, n=100)[100:]);\n",
    "\n",
    "\n",
    "#plt.plot(running_mean(bb_dW_out / aa_W_out, n=100)[-1000:,:,0], alpha=1)\n",
    "# plt.ylim(0, 0.2)\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_dW_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_z_hid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom in one neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[16,9])\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plot_3d_histogram(tr_z_hid[:,:,1], funct=lambda x: x, ax=ax)\n",
    "ax.set_title('Neuron #1')\n",
    "#ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[16,18])\n",
    "\n",
    "for n in range(tr_z_hid.shape[-1]):\n",
    "    ax = fig.add_subplot(8, 3, n+1, projection='3d')\n",
    "    plot_3d_histogram(tr_z_hid[:,:,n], funct=lambda x: x, ax=ax)\n",
    "    ax.set_title('Neuron #' + str(n))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne, na, nn = tr_z_hid.shape\n",
    "tmp = tr_z_hid.reshape([-1,nn])\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_mean(act_fun(tmp[:,0]), n=1000))\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_std(act_fun(tmp[:,0]), n=1000))\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_z_hid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=[16,18])\n",
    "#ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "ne, na, nn = tr_z_hid.shape\n",
    "plt.plot(np.mean(act_fun(tr_z_hid), axis=1))\n",
    "plt.show()\n",
    "plt.plot(np.std(act_fun(tr_z_hid), axis=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_z_hid.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
