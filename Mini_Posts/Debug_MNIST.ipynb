{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pdb\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('MNIST-Dataset', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_raw = mnist.train.images\n",
    "train_y_raw = mnist.train.labels\n",
    "valid_x_raw = mnist.validation.images\n",
    "valid_y_raw = mnist.validation.labels\n",
    "test_x_raw = mnist.test.images\n",
    "test_y_raw = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist(data_x, data_y, n):\n",
    "    fig = plt.figure(figsize=[16,9])\n",
    "    for i in range(n):\n",
    "        ax = fig.add_subplot(n//8, 8, i+1)\n",
    "        ax.imshow(data_x[i].reshape([28,28]))\n",
    "        ax.axis('off')\n",
    "        idx = int(np.nonzero(data_y[i])[0])\n",
    "        ax.set_title(idx)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_mnist(train_x_raw, train_y_raw, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(valid_x_raw, valid_y_raw, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(test_x_raw, test_y_raw, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_x_raw[0:100].flatten(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train mean:', train_x_raw.mean(), 'std', train_x_raw.std())\n",
    "print('valid mean:', valid_x_raw.mean(), 'std', valid_x_raw.std())\n",
    "print('test mean:', test_x_raw.mean(), 'std', test_x_raw.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_mean = train_x_raw.mean()\n",
    "tx_std = train_x_raw.std()\n",
    "train_x = (train_x_raw - tx_mean)/tx_std\n",
    "valid_x = (valid_x_raw - tx_mean)/tx_std\n",
    "test_x = (test_x_raw - tx_mean)/tx_std\n",
    "train_y = train_y_raw\n",
    "valid_y = valid_y_raw\n",
    "test_y = test_y_raw\n",
    "print('train mean:', train_x.mean(), 'std', train_x.std())\n",
    "print('valid mean:', valid_x.mean(), 'std', valid_x.std())\n",
    "print('test mean:', test_x.mean(), 'std', test_x.std())\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(valid_x.shape, valid_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(train_x, train_y, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast import/restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pdb\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST-Dataset', one_hot=True)\n",
    "\n",
    "train_x_raw = mnist.train.images\n",
    "train_y_raw = mnist.train.labels\n",
    "valid_x_raw = mnist.validation.images\n",
    "valid_y_raw = mnist.validation.labels\n",
    "test_x_raw = mnist.test.images\n",
    "test_y_raw = mnist.test.labels\n",
    "\n",
    "# Preprocess\n",
    "tx_mean = train_x_raw.mean()\n",
    "tx_std = train_x_raw.std()\n",
    "train_x = (train_x_raw - tx_mean)/tx_std\n",
    "valid_x = (valid_x_raw - tx_mean)/tx_std\n",
    "test_x = (test_x_raw - tx_mean)/tx_std\n",
    "train_y = train_y_raw\n",
    "valid_y = valid_y_raw\n",
    "test_y = test_y_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_to_14_14(data):\n",
    "    assert data.ndim == 2\n",
    "    assert data.shape[1] == 784\n",
    "    data_28x28 = data.reshape([len(data), 28, 28])  # reshape to match image resolution, new shape (nb_samples, 28, 28)\n",
    "    print('28', data_28x28.shape)\n",
    "    data_14x14 = data_28x28[:,::2,::2] # subsample, new shape (nb_samples, 14, 14)\n",
    "    print('14', data_14x14.shape)\n",
    "    data_196 = data_14x14.reshape([len(data),14*14])\n",
    "    print('data_196', data_196.shape)\n",
    "    assert data_196.ndim == 2\n",
    "    assert data_196.shape[1] == 196\n",
    "    return data_196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = subsample_to_14_14(train_x)\n",
    "valid_x = subsample_to_14_14(valid_x)\n",
    "test_x = subsample_to_14_14(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "These are used for numpy model, but also for plotting later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1. - np.tanh(x)**2\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softssign(x, deriv=False):\n",
    "    if deriv:\n",
    "        dd = 1 + np.abs(x)\n",
    "        return (dd - x*np.sign(x)) / dd**2\n",
    "    return x / (1+np.abs(x))\n",
    "\n",
    "def relu(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1. * (x>0)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def lrelu(x, deriv=False):\n",
    "    if deriv:\n",
    "        dx = np.ones_like(x)\n",
    "        dx[x < 0] = 0.01\n",
    "        return dx\n",
    "    return np.where(x > 0, x, x * 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_hat):\n",
    "    assert y.ndim == 2\n",
    "    assert y_hat.ndim == 2\n",
    "    \n",
    "    # avg over batch, sum over outputs (inner)\n",
    "    return .5 * np.mean(np.sum((y-y_hat)**2, axis=-1))\n",
    "    \n",
    "    # no innner sum, becouse only one output\n",
    "    return np.mean((y-y_hat)**2)\n",
    "\n",
    "def acc(y, y_hat):\n",
    "    return np.mean(np.argmax(y_hat, axis=-1)==np.argmax(y, axis=-1))\n",
    "\n",
    "def fwd(x, W_hid, W_out, act_fun, ret=False):\n",
    "    assert x.ndim == 2\n",
    "    z_hid = x @ W_hid\n",
    "    h_hid = act_fun(z_hid)  # hidden output\n",
    "\n",
    "    z_out = h_hid @ W_out\n",
    "    y_hat = sigmoid(z_out)  # SIGMOID!\n",
    "\n",
    "    if ret:\n",
    "        return y_hat, z_hid, h_hid, z_out\n",
    "    return y_hat\n",
    "\n",
    "def backprop(x, y, W_hid, W_out, act_fun):\n",
    "    assert x.ndim == 2\n",
    "    assert y.ndim == 2\n",
    "    \n",
    "    y_hat, z_hid, h_hid, z_out = fwd(x, W_hid, W_out, act_fun, ret=True)\n",
    "    \n",
    "    ro_out = (y-y_hat) * -1 * sigmoid(z_out, deriv=True)  # SIGMOID\n",
    "    dW_out = h_hid.T @ ro_out / len(x)\n",
    "    \n",
    "    ro_hid = (ro_out @ W_out.T) * act_fun(z_hid, deriv=True)\n",
    "    dW_hid = x.T @ ro_hid / len(x)\n",
    "    \n",
    "    return dW_hid, dW_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical gradient check (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrad(x, y, W_hid, W_out, act_fun):\n",
    "    \n",
    "    eps = 1e-6\n",
    "    \n",
    "    gW_hid = np.zeros_like(W_hid)\n",
    "    for r in range(W_hid.shape[0]):\n",
    "        for c in range(W_hid.shape[1]):\n",
    "            W_hid_plus = W_hid.copy()\n",
    "            W_hid_minus = W_hid.copy()\n",
    "            W_hid_plus[r,c] += eps\n",
    "            W_hid_minus[r,c] -= eps\n",
    "            loss_plus = MSE(y, fwd(x, W_hid_plus, W_out, act_fun))\n",
    "            loss_minus = MSE(y, fwd(x, W_hid_minus, W_out, act_fun))\n",
    "            gW_hid[r,c] = (loss_plus-loss_minus) / (2*eps)\n",
    "\n",
    "    gW_out = np.zeros_like(W_out)\n",
    "    for r in range(W_out.shape[0]):\n",
    "        for c in range(W_out.shape[1]):\n",
    "            W_out_plus = W_out.copy()\n",
    "            W_out_minus = W_out.copy()\n",
    "            W_out_plus[r,c] += eps\n",
    "            W_out_minus[r,c] -= eps\n",
    "            loss_plus = MSE(y, fwd(x, W_hid, W_out_plus, act_fun))\n",
    "            loss_minus = MSE(y, fwd(x, W_hid, W_out_minus, act_fun))\n",
    "            gW_out[r,c] = (loss_plus-loss_minus) / (2*eps)\n",
    "    return gW_hid, gW_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW_hid, dW_out = backprop(train_x[0:3], train_y[0:3], W_hid, W_out, act_fun)\n",
    "ngW_hid, ngW_out = ngrad(train_x[0:3], train_y[0:3], W_hid, W_out, act_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(dW_hid, ngW_hid)\n",
    "assert np.allclose(dW_out, ngW_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop - with traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 196   # 784\n",
    "n_hid = 128  # 128             # try 8, 128(def.), 2048\n",
    "n_out = 10\n",
    "lr = .03     # 0.03                # try 10, 1(best), 0.03, 0.0003\n",
    "\n",
    "np_or_tf = 'tf'\n",
    "\n",
    "n_batch = 100\n",
    "act_fun = sigmoid\n",
    "completed_epochs = 0\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize weights\n",
    "var_hid = np.sqrt(1/n_in)             # try:  0.001,  sqrt(1/n_in),  1\n",
    "var_out = np.sqrt(1/n_hid)\n",
    "W_hid = np.random.normal(0.0, var_hid, [n_in, n_hid])\n",
    "W_out = np.random.normal(0.0, var_out, [n_hid, n_out])\n",
    "\n",
    "losses = {'batch':[], 'train':[], 'valid':[]}\n",
    "accurs = {'batch':[], 'train':[], 'valid':[]}\n",
    "traces = {'z_hid':[], 'z_out':[],\n",
    "          'dW_hid':[], 'dW_out':[],\n",
    "          'W_hid':[], 'W_out':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np_or_tf == 'np':\n",
    "    print('Skipping graph build')\n",
    "    \n",
    "elif np_or_tf == 'tf':\n",
    "    print('Initializing TensorFlow graph')\n",
    "    \n",
    "    try:    sess.close()\n",
    "    except: pass\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    nn_x = tf.placeholder(shape=[None, n_in], dtype=tf.float32)\n",
    "    nn_y = tf.placeholder(shape=[None, n_out], dtype=tf.float32)\n",
    "\n",
    "    nn_W_hid = tf.get_variable('nn_W_hid', shape=W_hid.shape, dtype=tf.float32, initializer=tf.constant_initializer(W_hid))\n",
    "    nn_z_hid = tf.matmul(nn_x, nn_W_hid)\n",
    "    if act_fun == sigmoid:\n",
    "        nn_h_hid = tf.nn.sigmoid(nn_z_hid)\n",
    "    else:\n",
    "        raise ValueError('unknown activation function')\n",
    "\n",
    "    nn_W_out = tf.get_variable('nn_W_out', shape=W_out.shape, dtype=tf.float32, initializer=tf.constant_initializer(W_out))\n",
    "    nn_z_out = tf.matmul(nn_h_hid, nn_W_out)\n",
    "    nn_y_hat = tf.nn.sigmoid(nn_z_out)  # output always sigmoid\n",
    "\n",
    "    nn_mse = .5 * tf.reduce_mean( tf.reduce_sum(tf.pow(nn_y-nn_y_hat, 2), axis=-1) )\n",
    "    nn_acc = tf.reduce_mean( \n",
    "                tf.cast( \n",
    "                    tf.equal( tf.argmax(nn_y_hat, axis=-1), tf.argmax(nn_y, axis=-1) )\n",
    "                , tf.float32)\n",
    "            )\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(nn_mse)\n",
    "\n",
    "    grads_and_vars = optimizer.compute_gradients(nn_mse)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    nn_dW_hid = grads_and_vars[0][0]\n",
    "    nn_dW_out = grads_and_vars[1][0]\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_ = 0\n",
    "train_i = np.array(range(len(train_x)))\n",
    "for e in range(10):\n",
    "    print(e)\n",
    "    np.random.shuffle(train_i)\n",
    "            \n",
    "    for k, v in traces.items():\n",
    "        v.append([])\n",
    "    \n",
    "    for i in range(0, len(train_x), n_batch):\n",
    "        # print(i,',', end='')\n",
    "        # Get 128 sized batch, both as 2d arrays   \n",
    "        batch = train_i[i:i+n_batch]\n",
    "        x = train_x[batch]\n",
    "        y = train_y[batch]\n",
    "        \n",
    "        \n",
    "        if np_or_tf == 'np':\n",
    "            \n",
    "            if ti_ == 0:\n",
    "                print('Executing Numpy version')\n",
    "            \n",
    "            # Forward pass\n",
    "            y_hat, z_hid, _, z_out = fwd(x, W_hid, W_out, act_fun, ret=True)\n",
    "            loss = MSE(y, y_hat)\n",
    "            accuracy = acc(y, y_hat)\n",
    "            \n",
    "            # Backpropagation\n",
    "            dW_hid, dW_out = backprop(x, y, W_hid, W_out, act_fun)\n",
    "            W_hid += -lr * dW_hid\n",
    "            W_out += -lr * dW_out\n",
    "            \n",
    "        elif np_or_tf == 'tf':\n",
    "            \n",
    "            if ti_ == 0:\n",
    "                print('Executing Tensorflow version')\n",
    "            \n",
    "            _, y_hat, z_hid, z_out, loss, accuracy, dW_hid, dW_out, W_hid, W_out = sess.run(\n",
    "                [train_op, nn_y_hat, nn_z_hid, nn_z_out, nn_mse, nn_acc, nn_dW_hid, nn_dW_out, nn_W_hid, nn_W_out],\n",
    "                feed_dict={nn_x: x, nn_y:y})\n",
    "            W_hid, W_out = sess.run(tf.trainable_variables())\n",
    "\n",
    "            assert y_hat.shape == y.shape\n",
    "            assert np.isscalar(loss)\n",
    "            assert np.isscalar(accuracy)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('np_or_tf must be \"np\" or \"tf\"')\n",
    "        \n",
    "        # Calc batch loss (before update!)\n",
    "        losses['batch'].append(loss)\n",
    "        accurs['batch'].append(accuracy)\n",
    "\n",
    "#         if ti_ % 10 == 0:\n",
    "#             valid_y_hat = fwd(valid_x, W_hid, W_out, act_fun)\n",
    "#             valid_loss = MSE(valid_y, valid_y_hat)\n",
    "#             losses['valid'].append(valid_loss)\n",
    "        \n",
    "        traces['z_hid'][-1].append(z_hid)\n",
    "        traces['z_out'][-1].append(z_out)\n",
    "        \n",
    "        if ti_ % 10 == 0:\n",
    "            traces['dW_hid'][-1].append(dW_hid)\n",
    "            traces['dW_out'][-1].append(dW_out)\n",
    "            traces['W_hid'][-1].append(W_hid.copy())\n",
    "            traces['W_out'][-1].append(W_out.copy())\n",
    "\n",
    "        ti_ += 1\n",
    "        \n",
    "    completed_epochs += 1\n",
    "    \n",
    "act_fun_to_str = {sigmoid:'sig', tanh:'tanh', softssign:'softsign', relu:'relu'}\n",
    "expstr = 'Network(' + np_or_tf + '): '\n",
    "expstr += str(n_in) + 'in->' + str(n_hid) + act_fun_to_str[act_fun] + '->' + str(n_out) + 'sig   '\n",
    "expstr += 'init_var=[' + str(round(var_hid, 3)) + ',' + str(round(var_out,3))+ ']   '\n",
    "expstr += 'lr=' + str(lr) + '   '\n",
    "expstr += 'batch=' + str(n_batch) + '   '\n",
    "expstr += 'epochs=' + str(completed_epochs)\n",
    "\n",
    "tr_z_hid = np.array(traces['z_hid'])\n",
    "tr_z_out = np.array(traces['z_out'])\n",
    "tr_dW_hid = np.array(traces['dW_hid'])\n",
    "tr_dW_out = np.array(traces['dW_out'])\n",
    "tr_W_hid = np.array(traces['W_hid'])\n",
    "tr_W_out = np.array(traces['W_out'])\n",
    "\n",
    "print('tr_z_hid', tr_z_hid.shape, tr_z_hid.size/1e6)\n",
    "print('tr_z_out', tr_z_out.shape, tr_z_out.size/1e6)\n",
    "print('tr_dW_hid', tr_dW_hid.shape, tr_dW_hid.size/1e6)\n",
    "print('tr_dW_out', tr_dW_out.shape, tr_dW_out.size/1e6)\n",
    "print('tr_W_hid', tr_W_hid.shape, tr_W_hid.size/1e6)\n",
    "print('tr_W_out', tr_W_out.shape, tr_W_out.size/1e6)\n",
    "\n",
    "print(expstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final train loss:', losses['train'][-1])\n",
    "print('Final valid loss:', losses['valid'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_hat = fwd(train_x, W_hid, W_out, act_fun)\n",
    "train_loss = MSE(train_y, train_y_hat)\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y_hat = fwd(valid_x, W_hid, W_out, act_fun)\n",
    "valid_loss = MSE(valid_y, valid_y_hat)\n",
    "valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Plot Loss, Accuracy\n",
    "#\n",
    "print(expstr)\n",
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "ax.plot(losses['batch'], label='Mini-Batch loss', alpha=.5)\n",
    "#ax.plot(losses['train'], label='Training loss')\n",
    "xx = np.arange(0, len(losses['valid'])) * 10\n",
    "ax.plot(xx, losses['valid'], label='Validation loss')\n",
    "ax.plot(accurs['batch'], label='Mini-Batch accuracy', color='red', alpha=.5)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Per-class accuracy\n",
    "#\n",
    "print(expstr)\n",
    "if np_or_tf == 'np':\n",
    "    y_hat = fwd(train_x, W_hid, W_out, act_fun)\n",
    "elif np_or_tf == 'tf':\n",
    "    y_hat = sess.run(nn_y_hat, feed_dict={nn_x: train_x})\n",
    "else:\n",
    "    raise ValueError()\n",
    "    \n",
    "y_correct = np.argmax(train_y, axis=-1) == np.argmax(y_hat, axis=-1)\n",
    "classes_correct = []\n",
    "classes_all = []\n",
    "for i in range(10):\n",
    "    is_y_class_i = y_correct * (np.argmax(train_y, axis=-1)==i)\n",
    "    nb_correct_class_i = np.sum(is_y_class_i)\n",
    "    classes_correct.append(nb_correct_class_i)\n",
    "    classes_all.append(np.count_nonzero(np.argmax(train_y,axis=-1)==i))\n",
    "classes_correct = np.array(classes_correct)\n",
    "classes_all = np.array(classes_all)\n",
    "\n",
    "plt.bar(range(10), classes_all, label='All Member')\n",
    "plt.bar(range(10), classes_correct, label='Correctly Predicted')\n",
    "plt.legend(loc=3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, n):\n",
    "    return np.array([ np.mean(x[max(i-n+1, 0): i+1]) for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_std(x, n):\n",
    "    return np.array([ np.std(x[max(i-n+1, 0): i+1]) for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(data_iw, layer_name, axis=None):\n",
    "    ni, nw = data_iw.shape   # iter, weights\n",
    "    \n",
    "    if axis is None:\n",
    "        fig, axis = plt.subplots(1,1, figsize=[16,6])\n",
    "    \n",
    "    axis.plot(data_iw, alpha=max(100/nw, 0.002))\n",
    "    axis.set_title(layer_name + ' Weights Neuron #' + str(neuron_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_update_ratios(data_iw, layer_name, mode='median', color='red', axis=None):\n",
    "    ni, nw = data_iw.shape  # iter, weights\n",
    "    \n",
    "    data_delta_iw = data_iw[1:,:] - data_iw[:-1,:]       # delta\n",
    "    ratio = data_delta_iw / data_iw[:-1,:]               # delta / weight\n",
    "    ratio_abs = np.abs(ratio)\n",
    "    \n",
    "    if axis is None:\n",
    "        fig, axis = plt.subplots(1,1, figsize=[16,6])\n",
    "    \n",
    "    if mode == 'raw':\n",
    "        axis.plot(data_delta_iw, alpha=max(1/nw, 0.002), color='red');\n",
    "        axis.set_title(layer_name + ' Delta Weights Neuron #' + str(neuron_nb))\n",
    "        return\n",
    "\n",
    "    if mode == 'fullmedian':\n",
    "        axis.plot(ratio_abs, alpha=max(1/nw, 0.002), color='pink');\n",
    "        \n",
    "    if mode in ['median', 'fullmedian']:\n",
    "        #min_vals = np.min(ratio_abs, axis=-1)\n",
    "        #min_vals[min_vals==0] = float('nan')  # replace -inf with nan (can't plot -inf on log scale)\n",
    "        #axis.plot(min_vals, alpha=.1, color=color);\n",
    "        axis.plot(np.min(ratio_abs, axis=-1), alpha=.1, color=color);\n",
    "        axis.plot(np.max(ratio_abs, axis=-1), alpha=.1, color=color);\n",
    "        axis.plot(np.percentile(ratio_abs, 10, axis=-1), alpha=1, color=color, ls=':');     # Q1\n",
    "        axis.plot(np.percentile(ratio_abs, 90, axis=-1), alpha=1, color=color, ls=':');     # Q3\n",
    "        axis.plot(np.median(ratio_abs, axis=-1), alpha=1, color=color);\n",
    "        \n",
    "    if mode == 'norms':\n",
    "        data_norm_i = np.linalg.norm(data_iw, axis=-1)\n",
    "        data_delta_norm_i = np.linalg.norm(data_delta_iw, axis=-1)\n",
    "        axis.plot(data_delta_norm_i/data_norm_i[:-1], alpha=1, color='green')\n",
    "    \n",
    "    if mode == 'DL4J':\n",
    "        tmp_ = np.mean(np.abs(data_delta_iw), axis=-1) / np.mean(np.abs(data_iw[:-1,:]), axis=1)\n",
    "        axis.plot(tmp_, color='blue')\n",
    "    \n",
    "    if mode == 'mean+std':\n",
    "        mean_ = np.mean(ratio_abs, axis=-1)\n",
    "        axis.plot(mean_, alpha=1, color='orange');\n",
    "        std_ = np.std(ratio_abs, axis=-1)\n",
    "        axis.plot(mean_ + std_, alpha=1, color='orange', ls=':');\n",
    "        \n",
    "    axis.plot([0,len(ratio_abs)],[.1, .1], alpha=.3, ls='--', c='black')\n",
    "    axis.plot([0,len(ratio_abs)],[.01, .01], ls='-', c='black')\n",
    "    axis.plot([0,len(ratio_abs)],[.001, .001], alpha=.3, ls='--', c='black')\n",
    "    axis.set_yscale('log')\n",
    "    axis.set_title(layer_name + ' abs( Delta / Weights )')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expstr)\n",
    "\n",
    "neuron_nb = 0\n",
    "tr_W_data = tr_W_out\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=[16,6])\n",
    "ax1, ax2 = axes\n",
    "\n",
    "ne, ni, nw, nn = tr_W_data.shape  # epochs, iter, weights, neurons\n",
    "tr_W_data_iw = tr_W_data[:,:,:,neuron_nb].reshape([ne*ni,nw])  # iter, weight\n",
    "\n",
    "plot_weights(tr_W_data_iw, 'Output '+str(neuron_nb), axis=ax1)\n",
    "plot_update_ratios(tr_W_data_iw, 'Output', color='blue', axis=ax2)\n",
    "\n",
    "del tr_W_data\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expstr)\n",
    "fig, ax = plt.subplots(1, 1, figsize=[16,6])\n",
    "\n",
    "ne, ni, nw, nn = tr_W_hid.shape  # epochs, iter, weights, neurons\n",
    "tr_W_data_iw = tr_W_hid.reshape([ne*ni,-1])  # iter, all weights in layer\n",
    "\n",
    "plot_update_ratios(tr_W_data_iw, 'Hidden', color='red', axis=ax)\n",
    "#ax.set_ylim([10e-24,10e4])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expstr)\n",
    "fig, ax = plt.subplots(1, 1, figsize=[16,6])\n",
    "\n",
    "ne, ni, nw, nn = tr_W_out.shape  # epochs, iter, weights, neurons\n",
    "tr_W_data_iw = tr_W_out.reshape([ne*ni,-1])  # iter, all weights in layer\n",
    "\n",
    "plot_update_ratios(tr_W_data_iw, 'Output', color='blue', axis=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_histogram(data, funct=lambda x: x, skip_first=False, lines_01=True, color=(1,0,0,1), alpha=1, ax=None, figsize=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Params:\n",
    "        data - 2d array, dims: [epochs, samples].\n",
    "               E.g. for single neuron activations: [[ 0.1, 0.2, 0.3, ... ]  activations in epoch 0, sample 1, 2, 3...\n",
    "                                                    [ 0.2, 0.1, 0.3, ... ]  activations in epoch 1, ...\n",
    "                                                    ...\n",
    "                                                    [ 0.3, 0.1, 0.2, ... ]] activations is last training epoch\n",
    "        funct - function to apply to data before plotting, e.g. sigmoid, tanh, usually none\n",
    "    \"\"\"\n",
    "    assert data.ndim==2\n",
    "    \n",
    "    def interpolate_colors(cstart, cend, n):\n",
    "        cstart, cend = np.array(cstart), np.array(cend)\n",
    "        assert cstart.shape == (4,)\n",
    "        assert cend.shape == (4,)\n",
    "        if n == 1:  return cend    # if one step, then return end color\n",
    "\n",
    "        cols = []\n",
    "        for i in range(n):\n",
    "            step = i/(n-1)\n",
    "            cols.append( (1-step)*cstart + step*cend)\n",
    "        return np.array(cols)\n",
    "    \n",
    "    color = np.array(color)\n",
    "    color_start = np.array(color/4, dtype=float)  # transparent black\n",
    "    color_end = np.array(color)\n",
    "    colors = interpolate_colors(color_start, color_end, len(data))\n",
    "    \n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    \n",
    "    ax.view_init(30, -85)\n",
    "    # ax.set_xlim(0, 1)\n",
    "    \n",
    "    for epoch in range(len(data)):                                     # One line per epoch\n",
    "        hist, bins = np.histogram(funct(data[epoch,:]), bins=100)      # apply funct and create histogram\n",
    "        bins = (bins[:-1] + bins[1:])/2                                # center bins\n",
    "        hist = hist / np.sum(hist)\n",
    "        \n",
    "        hist_0 = hist[0]\n",
    "        if skip_first:\n",
    "            bins = bins[1:]\n",
    "            hist = hist[1:]\n",
    "        \n",
    "        ax.plot(xs=bins, ys=hist,\n",
    "                zs=-epoch,\n",
    "                zdir='y', \n",
    "                color=colors[epoch])\n",
    "        nb_epochs = len(data)\n",
    "        if epoch == 0 and lines_01:\n",
    "            ax.plot(xs=[0,0], ys=[0,0], zs=[-nb_epochs,0], zdir='y', color='k')\n",
    "            ax.plot(xs=[1,1], ys=[0,0], zs=[-nb_epochs,0], zdir='y', color='k', ls='--')\n",
    "        if epoch == len(data)-1:\n",
    "            ax.plot(xs=[bins[0],bins[-1]], ys=[0,0], zs=-nb_epochs, zdir='y', color='k')\n",
    "    \n",
    "    if skip_first:\n",
    "        ax.set_xlabel('value ('+str(round(hist_0*100, 2)) + '%)'); ax.set_ylabel('epoch'); ax.set_zlabel('n')\n",
    "    else:\n",
    "        ax.set_xlabel('value'); ax.set_ylabel('epoch'); ax.set_zlabel('n')\n",
    "    #print('hist[0]:', round(hist_0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Hidden ACTIATION HISTOGRAM\n",
    "#\n",
    "print(expstr)\n",
    "fig = plt.figure(figsize=[16,6])\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ne, ni, na, nn = tr_z_hid.shape\n",
    "dd = tr_z_hid.reshape([ne,-1])\n",
    "plot_3d_histogram(dd, funct=lambda x: x, color=(0,0,0,1), ax=ax)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "plot_3d_histogram(dd, funct=lambda x: act_fun(x), skip_first=False, color=(1,0,0,1), ax=ax)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Hidden NEURONS ACTIVATIONS\n",
    "#\n",
    "print(expstr)\n",
    "ne, ni, na, nn = tr_z_hid.shape\n",
    "plot_n = 10  # tr_z_hid.shape[-1]\n",
    "fig = plt.figure(figsize=[16,2*plot_n/2])\n",
    "\n",
    "tmp_act_hid = tr_z_hid.reshape([ne,ni*na,nn])\n",
    "for n in range(plot_n):\n",
    "    \n",
    "    ax = fig.add_subplot(plot_n/2, 4, (n*2)+1, projection='3d')\n",
    "    plot_3d_histogram(tmp_act_hid[:,:,n], funct=lambda x: x, color=(0,0,0,1), ax=ax)\n",
    "    ax.set_title('Neuron #' + str(n))\n",
    "    \n",
    "    ax = fig.add_subplot(plot_n/2, 4, (n*2)+2, projection='3d')\n",
    "    plot_3d_histogram(tmp_act_hid[::,:,n], funct=lambda x: act_fun(x), skip_first=True, color=(1,0,0,1), ax=ax)\n",
    "    ax.set_title('Neuron #' + str(n))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Output ACTIATION HISTOGRAM\n",
    "#\n",
    "print(expstr)\n",
    "fig = plt.figure(figsize=[16,6])\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ne, ni, na, nn = tr_z_out.shape\n",
    "dd = tr_z_out.reshape([ne,-1])\n",
    "plot_3d_histogram(dd, funct=lambda x: x, color=(0,0,0,1), ax=ax)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "plot_3d_histogram(dd, funct=lambda x: sigmoid(x), color=(0,0,1,1), ax=ax)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "#   Output NEURONS ACTIVATIONS\n",
    "#\n",
    "print(expstr)\n",
    "ne, ni, na, nn = tr_z_out.shape\n",
    "fig = plt.figure(figsize=[16,2*nn/2])\n",
    "\n",
    "tmp_act_out = tr_z_out.reshape([ne,ni*na,nn])\n",
    "for n in range(tr_z_out.shape[-1]):\n",
    "    \n",
    "    ax = fig.add_subplot(nn/2, 4, (n*2)+1, projection='3d')\n",
    "    plot_3d_histogram(tmp_act_out[:,:,n], funct=lambda x: x, color=(0,0,0,1), ax=ax)\n",
    "    ax.set_title('Neuron #' + str(n))\n",
    "    \n",
    "    ax = fig.add_subplot(nn/2, 4, (n*2)+2, projection='3d')\n",
    "    plot_3d_histogram(tmp_act_out[:,:,n], funct=lambda x: sigmoid(x), color=(0,0,1,1), ax=ax)\n",
    "    ax.set_title('Neuron #' + str(n))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
