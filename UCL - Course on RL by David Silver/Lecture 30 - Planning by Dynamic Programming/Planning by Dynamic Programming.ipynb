{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<p style=\"text-align:center;font-weight:bold;font-size:400%\">Dynamic Programming</p>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook demonstrates implementation of algorithms presented in Lecture 3 of UCL RL course by David Silver.\n",
    "\n",
    "Algorithms:\n",
    "* Iterative Policy Evaluation\n",
    "* Policy Iteration\n",
    "* Value Iteration\n",
    "\n",
    "Notes:\n",
    "* As OpenAI gym doesn't have environment corresponding to gridworld used in lectures. We use FrozenLake-v0 instead\n",
    "\n",
    "Sources:\n",
    "* UCL Course on RL: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "  * Lecture 3 pdf: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf\n",
    "  * Lecture 3 vid: https://www.youtube.com/watch?v=Nd1-UUMVfz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import pdb\n",
    "\n",
    "np.set_printoptions(linewidth=115)  # nice printing of large arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentaiton for FrozenLake-v0: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "Frozen Lake is 4x4 grid:\n",
    "<img src='frozenlake.png'>\n",
    "Note on actions:\n",
    "* environment is 'slippery' - choosing to go 'North' will result with 1/3 probability of moving West/North/East each.\n",
    "* external walls are 'bouncy' - if agent to get off the grid, it remains in current state instead\n",
    "* above result in optimal policy which prioritizes avoiding holes over getting to target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_states = env.env.nS        # number of possible states\n",
    "nb_actions = env.env.nA       # number of actions from each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment transition probabilities and rewards are stored in array __env.env.P__, e.g. from state 6 go west (action 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 2, 0.0, False),\n",
       " (0.3333333333333333, 5, 0.0, True),\n",
       " (0.3333333333333333, 10, 0.0, False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P[6][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* given starting state __6__ and action __0__ (West), there is __33%__ chance ending in state __2__, with reward __0.0__, non-terminal\n",
    "* given starting state __6__ and action __0__ (West), there is __33%__ chance ending in state __5__, with reward __0.0__, terminal (hole)\n",
    "* given starting state __6__ and action __0__ (West), there is __33%__ chance ending in state __10__, with reward __0.0__, non-terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement algorithm as follows:\n",
    "* Initialize all state values to zero: $\\mathbf{v \\leftarrow 0}$\n",
    "* Flatten Markov Decision Process to Markov Reward Process, i.e. assume policy is part of environment\n",
    "* Iterate n times:\n",
    "  * update all state values using Bellman Expectation Equation:\n",
    "  \n",
    "<img src=\"eq_policy_eval.png\">\n",
    "\n",
    "Where:\n",
    "* $\\mathbf{v}^{k+1}$ is vector of state-values $v(s)$ at iteration k+1\n",
    "* $\\mathbf{R}^\\pi$ is vector of expected rewards for each state\n",
    "* $\\gamma$ is a discount factor, here equals 1.0\n",
    "* $\\mathbf{P}^\\pi$ is transition probability matrix under our policy $\\pi$ with shape [nb_states, nb_states]\n",
    "* $\\mathbf{v}^k$ is vector of state-values $v(s)$ at iteration k\n",
    "\n",
    "What we know:\n",
    "* $\\pi(a|s)$ - policy - probability of choosing action a, given state s - defined by us\n",
    "* $P_{s,s'}^a$ - transition probability from state-action s,a to state s' (this is __33%__ above)\n",
    "* $R_{s,s'}^a$ - expected reward on transition from state-action s,a to state s' (this is __0.0__ above)\n",
    "\n",
    "\n",
    "First we will calculate transition probability from state s to s' as follows\n",
    "<img src=\"eq_P_pi.png\">\n",
    "Where:\n",
    "* $P_{s,s'}^\\pi$ - transition probability from state s to s'\n",
    "\n",
    "Then we use $P_{s,s'}^\\pi$ to form transition probability matrix $\\mathbf{P}^\\pi$\n",
    "<img src=\"eq_P_matrix.png\">\n",
    "\n",
    "Second we calculate expected reward on transition from state-action pair\n",
    "\n",
    "<img src=\"eq_R_ssa.png\">\n",
    "Where:\n",
    "* $R_s^a$ - expected reward on transition from state-action s,a to any next state (weighted sum over all next-states)\n",
    "\n",
    "Now we can calculate expected reward from state s (not state-action!) to any next state\n",
    "<img src=\"eq_R_pi.png\">\n",
    "Where:\n",
    "* $R_s^\\pi$ - expected reward on transition from state s to any state s' (weighted sum over all actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to calculate both $\\mathbf{P}^\\pi$ and $\\mathbf{R}^\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_mdp(policy, model):\n",
    "    P_pi = np.zeros([nb_states, nb_states])  # transition probability matrix (s) to (s'), shape: [nb_states, nb_states]\n",
    "    R_pi = np.zeros([nb_states])             # expected reward from state (s) to any next state, shape: [nb_states]\n",
    "    for s in range(nb_states):\n",
    "        for a in range(nb_actions):\n",
    "            for p_, s_, r_, _ in model[s][a]:\n",
    "                # p_ - transition probability from (s,a) to (s')\n",
    "                # s_ - next state (s')\n",
    "                # r_ - reward on transition from (s,a) to (s')\n",
    "                P_pi[s, s_] += policy[s,a] * p_   # transition probability (s) -> (s')\n",
    "                Rsa = p_ * r_                     # expected reward for transition from (s,a) to any next state\n",
    "                R_pi[s] += policy[s,a] * Rsa      # expected reward for transition from (s) to any next state\n",
    "    assert np.alltrue(np.sum(P_pi, axis=-1)==np.ones([nb_states]))  # rows should sum to 1\n",
    "    return P_pi, R_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do policy evaluation for random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.ones([nb_states, nb_actions]) / nb_actions  # random policy, 25% each action\n",
    "P_pi, R_pi = flatten_mdp(policy, model=env.env.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5  0.25 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.25 0.25 0.25 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.25 0.5  0.   0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.25 0.   0.   0.   0.25 0.25 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.25 0.   0.   0.25 0.   0.25 0.   0.   0.25 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.25 0.   0.   0.   0.25 0.25 0.   0.   0.25 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.   0.25 0.   0.   0.25 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.   0.25 0.   0.   0.25 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.25 0.25 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.25 0.25 0.25]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(P_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(R_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 100 steps of iterative policy evaluation according to equation\n",
    "<img src=\"eq_policy_eval.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.014 0.012 0.021 0.01 ]\n",
      " [0.016 0.    0.041 0.   ]\n",
      " [0.035 0.088 0.142 0.   ]\n",
      " [0.    0.176 0.439 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "lmbda = 1.0  # discount\n",
    "\n",
    "V_pi = np.zeros([nb_states])\n",
    "for i in range(100):\n",
    "    V_pi = R_pi + lmbda * P_pi @ V_pi\n",
    "print(V_pi.reshape([4, -1]).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct output:\n",
    "```\n",
    "[[0.014 0.012 0.021 0.01 ]\n",
    " [0.016 0.    0.041 0.   ]\n",
    " [0.035 0.088 0.142 0.   ]\n",
    " [0.    0.176 0.439 0.   ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate greedy policy based on current policy state-values and environment model\n",
    "<img src=\"diag_Q_pi.png\"/>\n",
    "<img src=\"eq_Q_pi.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Q_pi(V_pi, model, lmbda):\n",
    "    Q_pi=np.zeros([nb_states, nb_actions])\n",
    "    for s in range(nb_states):\n",
    "        for a in range(nb_actions):\n",
    "            for p_, s_, r_, _ in model[s][a]:\n",
    "                Pssa = p_       # probability going from s,a -> s_\n",
    "                Rsa = p_ * r_   # expected reward for transition s,a -> s_\n",
    "                Vs_ = V_pi[s_]  # state-value of s_\n",
    "                Q_pi[s, a] += Rsa + lmbda * Pssa * Vs_\n",
    "    return Q_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.015 0.014 0.014 0.013]\n",
      " [0.009 0.012 0.011 0.016]\n",
      " [0.024 0.021 0.024 0.014]\n",
      " [0.01  0.01  0.007 0.014]\n",
      " [0.022 0.017 0.016 0.01 ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.054 0.047 0.054 0.007]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.017 0.041 0.035 0.046]\n",
      " [0.07  0.118 0.106 0.059]\n",
      " [0.189 0.176 0.16  0.043]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.088 0.205 0.234 0.176]\n",
      " [0.252 0.538 0.527 0.439]\n",
      " [0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "Q_pi = calc_Q_pi(V_pi, env.env.P, lmbda)   # calc Q_pi for values from policy evalutaion section above\n",
    "print(Q_pi.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct output:\n",
    "```\n",
    "[[0.015 0.014 0.014 0.013]\n",
    " [0.009 0.012 0.011 0.016]\n",
    " [0.024 0.021 0.024 0.014]\n",
    " [0.01  0.01  0.007 0.014]\n",
    " [0.022 0.017 0.016 0.01 ]\n",
    " [0.    0.    0.    0.   ]\n",
    " [0.054 0.047 0.054 0.007]\n",
    " [0.    0.    0.    0.   ]\n",
    " [0.017 0.041 0.035 0.046]\n",
    " [0.07  0.118 0.106 0.059]\n",
    " [0.189 0.176 0.16  0.043]\n",
    " [0.    0.    0.    0.   ]\n",
    " [0.    0.    0.    0.   ]\n",
    " [0.088 0.205 0.234 0.176]\n",
    " [0.252 0.538 0.527 0.439]\n",
    " [0.    0.    0.    0.   ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_pi = np.zeros([nb_states])\n",
    "policy = np.ones([nb_states, nb_actions]) / nb_actions  # random policy, 25% each action\n",
    "lmbda = 1.0  # discount\n",
    "\n",
    "for _ in range(10):\n",
    "    # flatten MDP\n",
    "    P_pi, R_pi = flatten_mdp(policy, env.env.P)\n",
    "    \n",
    "    # evaluate policy\n",
    "    for _ in range(100):\n",
    "        V_pi = R_pi + lmbda * P_pi @ V_pi\n",
    "        \n",
    "    # iterate policy\n",
    "    policy *= 0  # clear\n",
    "    Q_pi = calc_Q_pi(V_pi, env.env.P, lmbda)\n",
    "    a_max = np.argmax(Q_pi, axis=-1)     # could distribute action probability between equal max(q) values\n",
    "    policy[range(nb_states), a_max] = 1  # pick greedy action    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<' '^' '^' '^']\n",
      " ['<' '<' '<' '<']\n",
      " ['^' 'v' '<' '<']\n",
      " ['<' '>' 'v' '<']]\n"
     ]
    }
   ],
   "source": [
    "a2w = {0:'<', 1:'v', 2:'>', 3:'^'}\n",
    "policy_arrows = [a2w[x] for x in np.argmax(policy, axis=-1)]\n",
    "print(np.array(policy_arrows).reshape([-1, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct optimal policy:\n",
    "```\n",
    "[['<' '^' '^' '^']\n",
    " ['<' '<' '<' '<']\n",
    " ['^' 'v' '<' '<']\n",
    " ['<' '>' 'v' '<']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "V_pi = np.zeros([nb_states])\n",
    "policy = np.ones([nb_states, nb_actions]) / nb_actions  # random policy, 25% each action\n",
    "lmbda = 1.0  # discount\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    Q_pi = calc_Q_pi(V_pi, env.env.P, lmbda)\n",
    "    a_max = np.argmax(Q_pi, axis=-1)\n",
    "    \n",
    "    V_pi[range(nb_states)] = Q_pi[range(nb_states), a_max]\n",
    "\n",
    "# construct policy\n",
    "policy = np.zeros([nb_states, nb_actions])\n",
    "policy[range(nb_states), a_max] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<' '^' '^' '^']\n",
      " ['<' '<' '<' '<']\n",
      " ['^' 'v' '<' '<']\n",
      " ['<' '>' 'v' '<']]\n"
     ]
    }
   ],
   "source": [
    "a2w = {0:'<', 1:'v', 2:'>', 3:'^'}\n",
    "policy_arrows = [a2w[x] for x in np.argmax(policy, axis=-1)]\n",
    "print(np.array(policy_arrows).reshape([-1, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct optimal policy:\n",
    "```\n",
    "[['<' '^' '^' '^']\n",
    " ['<' '<' '<' '<']\n",
    " ['^' 'v' '<' '<']\n",
    " ['<' '>' 'v' '<']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
